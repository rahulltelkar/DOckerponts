ORCHESTRATION:
---------------------------------------------------------------------------------------------------------------------------------------------------------
1. Swarm services use a declarative model
2. To confirm that the service was created and started successfully, use the docker service ls
3. To provide a name for your service, use the --name flag: docker service create --name my_web nginx
4. gMSA credential spec - a requirement for Active Directory-authenticated applications. 
   This reduces the burden of distributing credential specs to the nodes they’re used on.
5. To use a Config as a credential spec, first create the Docker Config containing the credential spec: docker config create credspec credspec.json
6. you can create a service using this credential spec. To do so, use the --credential-spec flag with the config name, like this: docker service create --credential-spec="config://credspec" <your image>
7. If your image is available on a private registry which requires login, use the --with-registry-auth flag with docker service create
8. If your image is stored on registry.example.com, which is a private registry, use a command like the docker login registry.example.com
9. Credential spec files are applied at runtime.
10. no gMSA credentials are written to disk on worker nodes.
11. When deploying a service using a gMSA-based config, the credential spec is passed directly to the runtime of containers in that service.
12. To use a Config as a credential spec, create a Docker Config in a credential spec file named credpspec.json.
13. You can change almost everything about an existing service using the docker service update command. 
14. Since Nginx is a web service, it works much better if you publish port 80 to clients outside the swarm. You can specify this when you create the service, using the -p or --publish flag.
15. When updating an existing service, the flag is --publish-add. Assuming that the my_web service from the previous section still exists, use the following command to update it to publish port 80.
       docker service update --publish-add 80 my_web
16. here is also a --publish-rm flag to remove a port that was previously published.
17. To remove a service, use the docker service remove command.
18. If you specify a tag, the manager resolves that tag to a digest.
19. When the request to create a container task is received on a worker node, the worker node only sees the digest, not the tag.
20. When you create a service, the image’s tag is resolved to the specific digest the tag points to at the time of service creation.
21. If the manager can’t resolve the tag to a digest, each worker node is responsible for resolving the tag to a digest, and different nodes may use different versions of the image.
22. To see an image’s current digest, issue the command docker inspect <IMAGE>:<TAG> and look for the RepoDigests line.
23. If the swarm manager can resolve the image tag to a digest, it instructs the worker nodes to redeploy the tasks and use the image at that digest.
24. When you create a swarm service, you can publish that service’s ports to hosts outside the swarm in two ways: 
       i. You can rely on the routing mesh.  ii. You can publish a service task’s port directly on the swarm node .
25. Routing mesh: When you publish a service port, the swarm makes the service accessible at the target port on every node, regardless of whether there is a task for the service running on that node or not. This is less complex and is the right choice for many types of services.
26. PUBLISH A SERVICE’S PORTS USING THE ROUTING MESH: To publish a service’s ports externally to the swarm, use the --publish <PUBLISHED-PORT>:<SERVICE-PORT> flag
27. If an external host connects to that port on any swarm node, the routing mesh routes it to a task.
28. The external host does not need to know the IP addresses or internally-used ports of the service tasks to interact with the service. When a user or process connects to a service, any worker node running a service task may respond.
29. PUBLISH A SERVICE’S PORTS DIRECTLY ON THE SWARM NODE: You can publish a service task’s port directly on the swarm node where that service is running.
30. PUBLISH A SERVICE’S PORTS DIRECTLY ON THE SWARM NODE: provides the maximum flexibility, including the ability for you to develop your own routing framework.
31. To publish a service’s port directly on the node where it is running, use the mode=host option to the --publish flag.
32. create overlay network on a manager node using the docker network create command with the --driver overlay flag.
33. You can create a new service and pass the --network flag to attach the service to the overlay network: 
    docker service create \
  --replicas 3 \
  --network my-network \
  --name my-web \
    nginx
34. You can also connect an existing service to an overlay network using the --network-add flag. docker service update --network-add my-network my-web.
35. To disconnect a running service from a network, use the --network-rm flag.: docker service update --network-rm my-network my-web.
36. Placement constraints let you configure the service to run only on nodes with specific (arbitrary) metadata set, and cause the deployment to fail if appropriate nodes do not exist. 
37. you can specify that your service should only run on nodes where an arbitrary label pci_compliant is set to true.
38. Placement preferences let you apply an arbitrary label with a range of values to each node, and spread your service’s tasks across those nodes using an algorithm.
39. Swarm mode has two types of services: replicated and global.
40. For replicated services, you specify the number of replica tasks for the swarm manager to schedule onto available nodes.
41. For global services, the scheduler places one task on each available node that meets the service’s placement constraints and resource requirements.
42. I created a deployment that runs several identical tasks on nodes. which type of service deployment is this?: Replicated.
43. I created a deployment that runs exactly one task on every node. which type of service deployment is this? Global.
44. You control the type of service using the --mode flag. If you don’t specify a mode, the service defaults to replicated.
44. For replicated services, you specify the number of replica tasks you want to start using the --replicas flag. For example, to start a replicated nginx service with 3 replica tasks:
     docker service create \
  --name my_web \
  --replicas 3 \
  nginx
45. To start a global service on each available node, pass --mode global to docker service create.
46. The global service on the new node.
      docker service create \
  --name myservice \
  --mode global \
    alpine top
47. To reserve a given amount of memory or number of CPUs for a service, use the --reserve-memory or --reserve-cpu flags.
48. PLACEMENT CONSTRAINTS: service only runs on nodes with the label region set to east. If no appropriately-labelled nodes are available, tasks will wait in Pending  until they become available.
49. PLACEMENT CONSTRAINTS: The --constraint flag uses an equality operator (== or !=)
     docker service create \
  --name my-nginx \
  --replicas 5 \
  --constraint node.labels.region==east \
  nginx
50. PLACEMENT PREFERENCES: While placement constraints limit the nodes a service can run on, placement preferences try to place tasks on appropriate nodes in an algorithmic way (currently, only spread evenly). 
      docker service create \
  --replicas 9 \
  --name redis_2 \
  --placement-pref 'spread=node.labels.datacenter' \
  redis:3.0.6
  
51. When updating a service with docker service update, --placement-pref-add appends a new placement preference after all existing placement preferences.
52. --placement-pref-rm removes an existing placement preference that matches the argument.
53. The --update-delay flag configures the time delay between updates to a service task or sets of tasks.
54. By default the scheduler updates 1 task at a time. You can pass the --update-parallelism flag to configure the maximum number of service tasks that the scheduler updates simultaneously.
55. When an update to an individual task returns a state of RUNNING, the scheduler continues the update by continuing to another task until all tasks are updated. If, at any time during an update a task returns FAILED, the scheduler pauses the update.
    You can control the behavior using the --update-failure-action flag for docker service create or docker service update.
56. When an updated task returns either RUNNING or FAILED, the scheduler waits 10 seconds before stopping the next task to update:
      docker service create \
  --replicas 10 \
  --name my_web \
  --update-delay 10s \
  --update-parallelism 2 \
  --update-failure-action continue \
  alpine
57. The --update-max-failure-ratio flag controls what fraction of tasks can fail during an update before the update as a whole is considered to have failed. 
58. An individual task update is considered to have failed if the task doesn’t start up, or if it stops running within the monitoring period specified with the --update-monitor flag.
59. The default value for --update-monitor is 30 seconds.
60. In case the updated version of a service doesn’t function as expected, it’s possible to manually roll back to the previous version of the service using docker service update’s --rollback flag.
61. You can create two types of mounts for services in a swarm, volume mounts or bind mounts.
62. using the --mount flag when you create a service. the --mount-add or --mount-rm flag when updating an existing service. 
63. Data volumes are storage that exist independently of a container.
64. Volumes can be created before deploying a service, or if they don’t exist on a particular host when a task is scheduled there, they are created automatically according to the volume specification on the service. 
65. The default volume driver is local.
66.  To use a different volume driver with this create-on-demand pattern, specify the driver and its options with the --mount flag
67. Bind mounts are file system paths from the host where the scheduler deploys the container for the task. 
     docker service create \
  --mount type=bind,src=<HOST-PATH>,dst=<CONTAINER-PATH> \     /// READ-WRITE BIND.
  --name myservice \
  <IMAGE>
68. docker service create \
  --mount type=bind,src=<HOST-PATH>,dst=<CONTAINER-PATH>,readonly \   ///READ-ONLY
  --name myservice \
  <IMAGE>
69. Bind mounts can be useful but they can also cause problems. The main risks include the following:
      i. If you bind mount a host path into your service’s containers, the path must exist on every swarm node.The Docker swarm mode scheduler can schedule containers on any machine that meets resource availability requirements and satisfies all constraints and placement preferences you specify
      ii. The Docker swarm mode scheduler may reschedule your running service containers at any time if they become unhealthy or unreachable.
	  iii. Host bind mounts are non-portable. When you use bind mounts, there is no guarantee that your application runs the same way in development as it does in production.
70. The following flags are supported for template:
      --hostname
      --mount
      --env	  
71. docker node update [OPTIONS] NODE: 
     --availability ->Availability of the node (“active”|”pause”|”drain”)
     --label-add->Add or update a node label (key=value) 
     --label-rm->Remove a node label if exists	 
     --role->Role of the node (“worker”|”manager”)
72. Add metadata to a swarm node using node labels.  $ docker node update --label-add foo worker1
73. To add multiple labels to a node, pass the --label-add flag for each label: $ docker node update --label-add foo --label-add bar worker1
74. When you create a service, you can use node labels as a constraint. A constraint limits the nodes where the scheduler deploys tasks for a service.
         $ docker node update --label-add type=queue worker1    ///where type is a label and queue is service.
75. Open a terminal and ssh into the machine where you want to run your manager node. you can connect to it via SSH using the  command:
        $ docker-machine ssh manager1
76. Run the command to create a new swarm: $ docker swarm init --advertise-addr <MANAGER-IP>
77. The --advertise-addr flag configures the manager node to publish its address as 192.168.99.100. The other nodes in the swarm must be able to access the manager at the IP address.
78. Run docker info to view the current state of the swarm:
79. Run the docker node ls command to view information about nodes:
80. you can run the following command on a manager node to retrieve the join command for a worker: docker swarm join-token worker.
81. STACK must be executed on a swarm manager node.
82. The Compose file is a YAML file defining services, networks and volumes. The default path for a Compose file is ./docker-compose.yml.
83. restart: no is the default restart policy, and it does not restart a container under any circumstance. When always is specified, the container always restarts.
     The on-failure policy restarts a container if the exit code indicates an on-failure error and restart: unless-stopped
            restart: "no"
            restart: always
            restart: on-failure
            restart: unless-stopped
84. The restart option is ignored when deploying a stack in swarm mode.
85. The Raft logs used by swarm managers are encrypted on disk by default.
86. When Docker restarts, both the TLS key used to encrypt communication among swarm nodes, and the key used to encrypt and decrypt Raft logs on disk, are loaded into each manager node’s memory.
87. Ability to protect the mutual TLS encryption key and the key used to encrypt and decrypt Raft logs at rest, by allowing you to take ownership of these keys and to require manual unlocking of your managers. This feature is called autolock.
88. When Docker restarts, you must unlock the swarm first, using a key encryption key generated by Docker when the swarm was locked. You can rotate this key encryption key at any time.
89. You don’t need to unlock the swarm when a new node joins the swarm, because the key is propagated to it over mutual TLS.
90. When you initialize a new swarm, you can use the --autolock flag to enable autolocking of swarm manager nodes when Docker restarts.
                $ docker swarm init --autolock.
91. To enable autolock on an existing swarm, set the autolock flag to true: $ docker swarm update --autolock=true
92. To disable autolock, set --autolock to false. $ docker swarm update --autolock=false.
93. To unlock a locked swarm, use docker swarm unlock. : $ docker swarm unlock
94. If the key has not been rotated since the node left the swarm, and you have a quorum of functional manager nodes in the swarm, you can view the current unlock key using
         $ docker swarm unlock-key
95. docker swarm unlock-key --rotate
96. $ docker service ls

	ID            NAME        SCALE  IMAGE   COMMAND
	9uk4639qpg7n  helloworld  1/1    alpine  ping docker.com
97. A task is the atomic unit of scheduling within a swarm.
98. The container is the instantiation of the task.
99. API: accepts command and  create service object.
100. orchestrator: reconsillation loop that creates tasks for service object.
111. allocater:  allocates IP address to task.
112. dispatcher: assign task to nodes.
113. scheduler: instructs a worker to run a task. 
114. If your only intention is to prevent a service from being deployed, scale the service to 0 instead of trying to configure it in such a way that it remains in pending.
115. If all nodes are paused or drained, and you create a service, it is pending until a node becomes available.
116. The first node to become available gets all of the tasks, so this is not a good thing to do in a production environment.   
117. You can reserve a specific amount of memory for a service. If no node in the swarm has the required amount of memory, the service remains in a pending state until a node is available which can run its tasks.
118. For a replicated service, you specify the number of identical tasks you want to run. For example, you decide to deploy an HTTP service with three replicas, each serving the same content.
119. A global service is a service that runs one task on every node. There is no pre-specified number of tasks.
120: Running Services on a Docker Swarm can be scaled in different ways: docker service scale SERVICE=NUMBER. This works only for replicated services. 
       Another way is using docker service update --replicas=NUMBER SERVICE to achieve the same goal.
121. multiple services can be scaled at once, using the docker service scale SERVICE1=NUMBER1 SERVICE2=NUMBER2
122. docker inspect: Return low-level information on Docker objects. Docker inspect provides detailed information on constructs controlled by Docker.
123. By default, docker inspect will render results in a JSON array.
124. Run docker service inspect --pretty <SERVICE-ID> to display the details about a service in an easily readable format.
125. Run docker service ps <SERVICE-ID> to see which nodes are running the service.
126. By default, manager nodes in a swarm can execute tasks just like worker nodes.
127. docker stack ls shows an overview of all existing stacks.
128. docker stack ps STACK lists all tasks in a stack.
129. docker stack services STACK gives an overview over the running services.
130. docker stack rm STACK removes one or more stack.
131. Swarm manager nodes use the Raft Consensus Algorithm to manage the swarm state.
132. manager nodes are the key components for managing the swarm and storing the swarm state.
133. More Manager nodes reduce write performance because more nodes must acknowledge proposals to update the swarm state. This means more network round-trip traffic.
134. Raft requires a majority of managers, also called the quorum.
135. If the swarm loses the quorum of managers, the swarm cannot perform management tasks.
136. Even if a swarm loses the quorum of managers, swarm tasks on existing worker nodes continue to run. However, swarm nodes cannot be added, updated, or removed, and new or existing tasks cannot be started, stopped, moved, or updated.
137. Scaling down to a single manager is an unsafe operation and is not recommended.
138. If the last node leaves the swarm unexpectedly during the demote operation, the swarm becomes unavailable until you reboot the node or restart with --force-new-cluster.
139. By default manager nodes also act as a worker nodes. This means the scheduler can assign tasks to a manager node.
140. To avoid interference with manager node operation, you can drain manager nodes to make them unavailable as worker nodes:
           docker node update --availability drain <NODE>
141. When you drain a node, the scheduler reassigns any tasks running on the node to other available worker nodes in the swarm. It also prevents the scheduler from assigning tasks to the node.
142. You can monitor the health of manager nodes by querying the docker nodes API in JSON format through the /nodes HTTP endpoint
      docker node inspect manager1 --format "{{ .ManagerStatus.Reachability }}"
      reachable
       
      docker node inspect manager1 --format "{{ .Status.State }}"
      ready

142. An unreachable health status means that this particular manager node is unreachable from other manager nodes. In this case you need to take action to restore the unreachable manager:

     Restart the daemon and see if the manager comes back as reachable.     
     Reboot the machine.
     If neither restarting or rebooting work, you should add another manager node or promote a worker to be a manager node. You also need to cleanly remove the failed node entry from the manager set with docker node demote <NODE> and docker node rm <id-node>.
143. You should never restart a manager node by copying the raft directory from another node. The data directory is unique to a node ID. 
     A node can only use a node ID once to join the swarm. The node ID space should be globally unique.
145. To cleanly re-join a manager node to a cluster:

	To demote the node to a worker, run docker node demote <NODE>.
	To remove the node from the swarm, run docker node rm <NODE>.
	Re-join the node to the swarm with a fresh state using docker swarm join.
146. remove a node: docker node rm
147. you can forcefully remove the node without shutting it down by passing the --force flag. docker node rm --force node9
148. Before you forcefully remove a manager node, you must first demote it to the worker role. Make sure that you always have an odd number of manager nodes if you demote or remove a manager.
149. Docker manager nodes store the swarm state and manager logs in the /var/lib/docker/swarm/ directory.
150. You can back up the swarm using any manager. Use the following procedure. 
	1. If the swarm has auto-lock enabled, you need the unlock key to restore the swarm from backup. 
        Retrieve the unlock key if necessary and store it in a safe location. If you are unsure, read Lock your swarm to protect its encryption key.
        2. Stop Docker on the manager before backing up the data, so that no data is being changed during the backup. 
        It is possible to take a backup while the manager is running (a “hot” backup), but this is not recommended and your results are less predictable when restoring. While the manager is down, other nodes continue generating swarm data that is not part of this backup.
	3. Back up the entire /var/lib/docker/swarm directory.
        4. Restart the manager.
151. Recover from disaster: Restore from a backup.
        1. Shut down Docker on the target host machine for the restored swarm.
        2. Remove the contents of the /var/lib/docker/swarm directory on the new swarm.
        3. Restore the /var/lib/docker/swarm directory with the contents of the backup.
	4. Start Docker on the new node. Unlock the swarm if necessary. Re-initialize the swarm using the following command, 	
           so that this node does not attempt to connect to nodes that were part of the old swarm, and presumably no longer exist. $ docker swarm init --force-new-cluster
	5. Verify that the state of the swarm is as expected. This may include application-specific tests or simply checking the output of docker service ls to be sure that all expected services are present.
	6. If you use auto-lock, rotate the unlock key.
	7. Add manager and worker nodes to bring your new swarm up to operating capacity.
	8. Reinstate your previous backup regimen on the new swarm.
152. swarm cannot automatically recover if it loses a quorum.
153. The swarm can tolerate up to (N-1)/2 permanent failures beyond which requests involving swarm management cannot be processed.
154. The best way to recover from losing the quorum is to bring the failed nodes back online. If you can’t do that, the only way to recover from this state is to use the --force-new-cluster action from a manager node.
155.  you can use the --force or -f flag with the docker service update command to force the service to redistribute its tasks across the available worker nodes. 
156. When the container starts, it can only be connected to a single network, using --network
157. you can connect a running container to multiple networks using docker network connect.
158. container’s hostname defaults to be the container’s ID in Docker.
159. can override the hostname using --hostname.
-------------------------------------------------------------------------------------------------------------------------------------------------------------
IMAGE MANAGEMENT AND REGISTRY.

1. Build an image from a Dockerfile
2. The results of the health checks are available at the /debug/health endpoint on the debug HTTP server if the debug HTTP server is enabled.
3. You can delete an image from the Docker Hub by deleting individual tags in your repository in the tags area.
4. An image may be deleted from the registry via its name and reference. A delete may be issued with the following request format: DELETE /v2/<name>/manifests/<reference>
5. Tag Image: docker image tag SOURCE_IMAGE[:TAG] TARGET_IMAGE[:TAG]
6. docker image build: Build an image from a Dockerfile, docker image import: Import the contents from a tarball to create a filesystem image. docker image load: Load an image from a tar archive or STDIN. docker image save: Save one or more images to a tar archive (streamed to STDOUT by default).
7. The Registry is a stateless, highly scalable server side application that stores and lets you distribute Docker images. The Registry is open-source, under the permissive Apache license.
8. Start your registry: docker run -d -p 5000:5000 --name registry registry:2
9. Tag the image so that it points to your registry: docker image tag ubuntu localhost:5000/myfirstimage
10. Push it: Docker push localhost:5000/myfirstimage
11. Pull it back: docker pull localhost:5000/myfirstimage
12. Now stop your registry and remove all data: docker container stop registry && docker container rm -v registry
13. Docker can build images automatically by reading the instructions from a Dockerfile.
14. The docker build command builds an image from a Dockerfile and a context. 
15. The build is run by the Docker daemon, not by the CLI.
16. use the -f flag with docker build to point to a Dockerfile anywhere in your file system. $ docker build -f /path/to/a/Dockerfile .
17. You can specify a repository and tag at which to save the new image if the build succeeds: $ docker build -t shykes/myapp .
18. To tag the image into multiple repositories after the build, add multiple -t parameters when you run the build command:
      $ docker build -t shykes/myapp:1.0.2 -t shykes/myapp:latest .
19. Each instruction Dockerfile is run independently, and causes a new image to be created - so RUN cd /tmp will not have any effect on the next instructions.
20. Whenever possible, Docker will re-use the intermediate images (cache), to accelerate the docker build process significantly.
21. Build cache is only used from images that have a local parent chain. This means that these images were created by previous builds or the whole chain of images was loaded with docker load.
22. If you wish to use build cache of a specific image you can specify it with --cache-from option.Images specified with --cache-from do not need to have a parent chain and may be pulled from other registries.
23. To use the BuildKit backend, you need to set an environment variable DOCKER_BUILDKIT=1 on the CLI before invoking docker build.
24. A Dockerfile must begin with a `FROM` instruction. 
25. The FROM instruction specifies the Parent Image from which you are building.
26. FROM may only be preceded by one or more ARG instructions, which declare arguments that are used in FROM lines in the Dockerfile.
27. Parser directives do not add layers to the build, and will not be shown as a build step. Parser directives are written as a special type of comment in the form # directive=value.
     A single directive may only be used once.
28. Parser directives are not case-sensitive. However, convention is for them to be lowercase.
29.  The parser directives are supported are: syntax and esape.
30. Custom Dockerfile implementation allows you to:
        Automatically get bugfixes without updating the daemon
        Make sure all users are using the same implementation to build your Dockerfile
	Use the latest features without updating the daemon
	Try out new experimental or third-party features
31. The escape directive sets the character used to escape characters in a Dockerfile. If not specified, the default escape character is \.
32. The escape character is used both to escape characters in a line, and to escape a newline.
33. escaping is not performed in a RUN command, except at the end of a line.
34. Environment variables are notated in the Dockerfile either with $variable_name or ${variable_name}.
35. ${variable:-word} indicates that if variable is set then the result will be that value. If variable is not set then word will be the result.
36. ${variable:+word} indicates that if variable is set then word will be the result, otherwise the result is the empty string.
37. ENV abc=hello
    ENV abc=bye def=$abc
    ENV ghi=$abc
     will result in def having a value of hello, not bye. However, ghi will have a value of bye because it is not part of the same instruction that set abc to bye.   
38. docker CLI sends the context to the docker daemon.
39. The FROM instruction initializes a new build stage and sets the Base Image for subsequent instructions. As such, a valid Dockerfile must start with a FROM instruction.
40. ARG is the only instruction that may precede FROM in the Dockerfile
41. FROM can appear multiple times within a single Dockerfile to create multiple images or use one build stage as a dependency for another. 
42. Each FROM instruction clears any state created by previous instructions.
43. Optionally a name can be given to a new build stage by adding AS name to the FROM instruction. COPY --from=<name|index>
44. FROM instructions support variables that are declared by any ARG instructions that occur before the first FROM.
45. An ARG declared before a FROM is outside of a build stage, so it can’t be used in any instruction after a FROM.
46. RUN has 2 forms: 
     RUN <command> (shell form, the command is run in a shell, which by default is /bin/sh -c on Linux or cmd /S /C on Windows)
     RUN ["executable", "param1", "param2"] (exec form)
47. The RUN instruction will execute any commands in a new layer on top of the current image and commit the results. The resulting committed image will be used for the next step in the Dockerfile.
48. The exec form makes it possible to avoid shell string munging, and to RUN commands using a base image that does not contain the specified shell executable.
49. The default shell for the shell form can be changed using the SHELL command.
50. To use a different shell, other than ‘/bin/sh’, use the exec form passing in the desired shell. For example: RUN ["/bin/bash", "-c", "echo hello"]
51. The exec form is parsed as a JSON array, which means that you must use double-quotes (“) around words not single-quotes (‘).
52. The cache for RUN instructions can be invalidated by using the --no-cache flag.
53. The cache for RUN instructions can be invalidated by ADD and COPY instructions.
54. The CMD instruction has three forms:
	CMD ["executable","param1","param2"] (exec form, this is the preferred form)
	CMD ["param1","param2"] (as default parameters to ENTRYPOINT)
	CMD command param1 param2 (shell form)
55. There can only be one CMD instruction in a Dockerfile. If you list more than one CMD then only the last CMD will take effect.
56. The main purpose of a CMD is to provide defaults for an executing container. 
57. If CMD is used to provide default arguments for the ENTRYPOINT instruction, both the CMD and ENTRYPOINT instructions should be specified with the JSON array format.
58. CMD [ "echo", "$HOME" ] will not do variable substitution on $HOME. If you want shell processing then either use the shell form or execute a shell directly, for example: CMD [ "sh", "-c", "echo $HOME" ].
59. If you would like your container to run the same executable every time, then you should consider using ENTRYPOINT in combination with CMD. 
60. RUN actually runs a command and commits the result; CMD does not execute anything at build time, but specifies the intended command for the image.
61. The LABEL instruction adds metadata to an image.A LABEL is a key-value pair.
62. An image can have more than one label. You can specify multiple labels on a single line.
63. To view an image’s labels, use the docker image inspect command. You can use the --format option to show just the labels; docker image inspect --format='' myimage
64. The MAINTAINER instruction sets the Author field of the generated images.
65. The EXPOSE instruction informs Docker that the container listens on the specified network ports at runtime. 
66. The EXPOSE instruction does not actually publish the port.
67. To actually publish the port when running the container, use the -p flag on docker run to publish 
    and map one or more ports, or the -P flag to publish all exposed ports and map them to high-order ports.
68. By default, EXPOSE assumes TCP. You can also specify UDP: EXPOSE 80/udp.
69. The ENV instruction sets the environment variable <key> to the value <value>. 
70. The ENV instruction has two forms.
     The first form, ENV <key> <value>, will set a single variable to a value.
     The second form, ENV <key>=<value> ..., allows for multiple variables to be set at one time
71. You can view the valuesof ENV using docker inspect, and change them using docker run --env <key>=<value>.
72. ADD has two forms:
        ADD [--chown=<user>:<group>] <src>... <dest>
        ADD [--chown=<user>:<group>] ["<src>",... "<dest>"]
73. The ADD instruction copies new files, directories or remote file URLs from <src> and adds them to the filesystem of the image at the path <dest>.
74. COPY has two forms:
           COPY [--chown=<user>:<group>] <src>... <dest>
	   COPY [--chown=<user>:<group>] ["<src>",... "<dest>"]
75. The COPY instruction copies new files or directories from <src> and adds them to the filesystem of the container at the path <dest>.
76. If you build using STDIN (docker build - < somefile), there is no build context, so COPY can’t be used.
77. If you build by passing a Dockerfile through STDIN (docker build - < somefile), there is no build context, so the Dockerfile can only contain a URL based ADD instruction.
78. Optionally COPY accepts a flag --from=<name|index> that can be used to set the source location to a previous build stage (created with FROM .. AS <name>) that will be used instead of a build context sent by the user.
79. ENTRYPOINT has two forms:
          ENTRYPOINT ["executable", "param1", "param2"] The exec form, which is the preferred form
          ENTRYPOINT command param1 param2 The shell form:


80. An ENTRYPOINT allows you to configure a container that will run as an executable.
81. Only the last ENTRYPOINT instruction in the Dockerfile will have an effect.
82. Dockerfile should specify at least one of CMD or ENTRYPOINT commands.
83. ENTRYPOINT should be defined when using the container as an executable.
84. CMD should be used as a way of defining default arguments for an ENTRYPOINT command or for executing an ad-hoc command in a container.
85. CMD will be overridden when running the container with alternative arguments.
86. If CMD is defined from the base image, setting ENTRYPOINT will reset CMD to an empty value. In this scenario, CMD must be defined in the current image to have a value.
87. The VOLUME instruction creates a mount point with the specified name and marks it as holding externally mounted volumes from native host or other containers.
88. The value can be a JSON array, VOLUME ["/var/log/"], or a plain string with multiple arguments, such as VOLUME /var/log or VOLUME /var/log /var/db.
89. The USER instruction sets the user name (or UID) and optionally the user group (or GID) to use when running the image and for any RUN, CMD and ENTRYPOINT instructions that follow it in the Dockerfile.
90. The WORKDIR instruction sets the working directory for any RUN, CMD, ENTRYPOINT, COPY and ADD instructions that follow it in the Dockerfile.
91. The WORKDIR instruction can be used multiple times in a Dockerfile.
92. WORKDIR /a
    WORKDIR b
    WORKDIR c
    RUN pwd
           The output of the final pwd command in this Dockerfile would be /a/b/c.
93. The ARG instruction defines a variable that users can pass at build-time to the builder with the docker build command using the --build-arg <varname>=<value> flag.
94.  If a user specifies a build argument that was not defined in the Dockerfile, the build outputs a warning.
        [Warning] One or more build-args [foo] were not consumed.
95. A Dockerfile may include one or more ARG instructions. 
96. The ONBUILD instruction adds to the image a trigger instruction to be executed at a later time, when the image is used as the base for another build. 
97. The trigger will be executed in the context of the downstream build, as if it had been inserted immediately after the FROM instruction in the downstream Dockerfile.
98. Any build instruction can be registered as a trigger.
99. A list of all triggers is stored in the image manifest, under the key OnBuild. They can be inspected with the docker inspect command.
100. The STOPSIGNAL instruction sets the system call signal that will be sent to the container to exit : STOPSIGNAL signal
101. The HEALTHCHECK instruction has two forms:
         HEALTHCHECK [OPTIONS] CMD command (check container health by running a command inside the container)
         HEALTHCHECK NONE (disable any healthcheck inherited from the base image)
102. The HEALTHCHECK instruction tells Docker how to test a container to check that it is still working. 
103. When a container has a healthcheck specified, it has a health status in addition to its normal status. This status is initially starting. Whenever a health check passes, it becomes healthy (whatever state it was previously in). 
      After a certain number of consecutive failures, it becomes unhealthy.
104. The health check will first run interval seconds after the container is started, and then again interval seconds after each previous check completes.
105. If a single run of the check takes longer than timeout seconds then the check is considered to have failed.
106. It takes retries consecutive failures of the health check for the container to be considered unhealthy.
107. start period provides initialization time for containers that need time to bootstrap. 
108. 0: success - the container is healthy and ready for use. 1: unhealthy - the container is not working correctly. 2: reserved - do not use this exit code
109. Each layer is only a set of differences from the layer before it.
110. When you create a new container, you add a new writable layer on top of the underlying layers. This layer is often called the “container layer”. 
111. The major difference between a container and an image is the top writable layer.
112. When the container is deleted, the writable layer is also deleted. The underlying image remains unchanged.Because each container has its own writable container layer, and all changes are stored in this container layer.
113.  If you need multiple images to have shared access to the exact same data, store this data in a Docker volume and mount it into your containers.
114. To view the approximate size of a running container, you can use the docker ps -s command. 
115. size: the amount of data (on disk) that is used for the writable layer of each container.
116. virtual size: the amount of data used for the read-only image data used by the container plus the container’s writable layer size.
117. Following additional ways a container can take up disk space:

	Disk space used for log files if you use the json-file logging driver. This can be non-trivial if your container generates a large amount of logging data and log rotation is not configured.
	Volumes and bind mounts used by the container.
	Disk space used for the container’s configuration files, which are typically small.
	Memory written to disk (if swapping is enabled).
	Checkpoints, if you’re using the experimental checkpoint/restore feature. 
118. Copy-on-write is a strategy of sharing and copying files for maximum efficiency.
119. layers is stored in its own directory inside the Docker host’s local storage area. To examine the layers on the filesystem, list the contents of /var/lib/docker/<storage-driver>
120. The copy-on-write operation follows this rough sequence:
      Search through the image layers for the file to update. The process starts at the newest layer and works down to the base layer one layer at a time. When results are found, they are added to a cache to speed future operations.
    
      Perform a copy_up operation on the first copy of the file that is found, to copy the file to the container’s writable layer.
      Any modifications are made to this copy of the file, and the container cannot see the read-only copy of the file that exists in the lower layer.

121. Display layers of a Docker image: ocker image history [OPTIONS] IMAGE
122. docker image import: Import the contents from a tarball to create a filesystem image
123. Docker builds images automatically by reading the instructions from a Dockerfile.
124. When you issue a docker build command, the current working directory is called the build context.By default, the Dockerfile is assumed to be located here, but you can specify a different location with the file flag (-f).
125. Docker has the ability to build images by piping Dockerfile through stdin with a local or remote build context. 
126. Piping a Dockerfile through stdin can be useful to perform one-off builds without writing a Dockerfile to disk, or in situations where the Dockerfile is generated, and should not persist afterwards.
127. If you want to improve the build-speed by excluding some files from the build- context, refer to exclude with .dockerignore.
128. Multi-stage builds allow you to drastically reduce the size of your final image, without struggling to reduce the number of intermediate layers and files.
129. Only the instructions RUN, COPY, ADD create layers.   multi-stage builds,  only copy the artifacts you need into the final image.
130. If you do not want to use the cache at all, you can use the --no-cache=true option on the docker build command. 
131. Display detailed information on one or more images: docker image inspect [OPTIONS] IMAGE [IMAGE...]
132. Images that use the v2 or later format have a content-addressable identifier called a digest.
133. Can pull using a digest value. You can also reference by digest in create, run, and rmi commands, as well as the FROM image reference in a Dockerfile.
134. docker images --filter "dangling=true": shows untagged images.
135. docker rmi $(docker images -f "dangling=true" -q): remove untagged images. 
136. Log in to a Docker registry: docker login [OPTIONS] [SERVER]
137. docker login localhost:8080
138. You need to specify the credentials store in $HOME/.docker/config.json  to tell the docker engine to use it.
139. Export a container’s filesystem as a tar archive: docker export [OPTIONS] CONTAINER
140. Import the contents from a tarball to create a filesystem image: docker image import [OPTIONS] file|URL|- [REPOSITORY[:TAG]]
141. Pull from a different registry: $ docker pull myregistry.local:5000/testing/test-image
142. Push an image or a repository to a registry: docker image push [OPTIONS] NAME[:TAG]
143. you can delete an image with docker rmi or docker image rm.
144. List images: docker image ls [OPTIONS] [REPOSITORY[:TAG]]
145. Remove unused images: docker image prune [OPTIONS]
146. Search the Docker Hub for images: docker search [OPTIONS] TERM
147. Images are stored in collections, known as a repository, which is keyed by a name, as seen throughout the API specification.
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
INSTALLATION AND CONFIGURATION.
Complete configuration of backups for UCP and DTR
Consistently repeat steps to deploy Docker engine UCP and DTR on AWS and on premises in an HA config
Outline the sizing requirements prior to installation


1. How to configure Docker to start on boot?: sudo systemctl enable docker
2. Docker includes multiple logging mechanisms to help you get information from running containers and services. These mechanisms are called logging drivers.
3. The default logging driver is json-file.
4. set the value of log-driver to the name of the logging driver in the daemon.json file, which is located in /etc/docker/ on Linux hosts or C:\ProgramData\docker\config\ on Windows server hosts. 
5. If the logging driver has configurable options, you can set them in the daemon.json file as a JSON object with the key log-opts. 
        {
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "10m",
    "max-file": "3",
    "labels": "production_status",
    "env": "os,customer"
  }
}
6. To find the current default logging driver for the Docker daemon, run docker info and search for Logging Driver.
7. To find the current logging driver for a running container, if the daemon is using the json-file logging driver, run the following docker inspect command.
8. Docker provides two modes for delivering messages from the container to the log driver: 
    (default) direct,  blocking delivery from container to driver.
    non-blocking delivery that stores log messages in an intermediate per-container ring buffer for consumption by driver.
9. The non-blocking message delivery mode prevents applications from blocking due to logging back pressure.
10. The mode log option controls whether to use the blocking (default) or non-blocking message delivery.
11. The max-buffer-size log option controls the size of the ring buffer used for intermediate message storage when mode is set to non-blocking. max-buffer-size defaults to 1 megabyte.
        $ docker run -it --log-opt mode=non-blocking --log-opt max-buffer-size=4m alpine ping 127.0.0.1
12. Supported logging drivers: none, local, json-file, syslog, journald, gelf, fluentd, awslogs, splunk, etwlogs, gcplogs, logentries.
13. When using Docker Community Engine, the docker logs command is only available on the following drivers:local, json-file, journald.
14. Reading log information requires decompressing rotated log files, which causes a temporary increase in disk usage.
15. On a typical installation the Docker daemon is started by a system utility, not manually by a user. This makes it easier to automatically start Docker when the machine reboots.
16. There are two ways to configure the Docker daemon:
      Use a JSON configuration file. This is the preferred option, since it keeps all configurations in a single place.
      Use flags when starting dockerd.
17. To configure the Docker daemon using a JSON file, create a file at /etc/docker/daemon.json on Linux systems, or C:\ProgramData\docker\config\daemon.json
18. The Docker daemon can listen for Docker Engine API requests via three different types of Socket: unix, tcp, and fd. 
19. By default, a unix domain socket (or IPC socket) is created at /var/run/docker.sock
20. On Linux, the Docker daemon has support for several different image layer storage drivers: aufs, devicemapper, btrfs, zfs, overlay and overlay2.
21. The aufs driver is the oldest, but is based on a Linux kernel patch-set that is unlikely to be merged into the main kernel.aufs allows containers to share executable and shared library memory, 
      so is a useful choice when running thousands of containers with the same program or libraries.
22. The devicemapper driver uses thin provisioning and Copy on Write (CoW) snapshots. For each devicemapper graph location – typically /var/lib/docker/devicemapper.
23. If using a block device for device mapper storage, it is best to use lvm to create and manage the thin-pool volume.
24. The highlights of the lvm-based thin-pool management feature include:
      automatic or interactive thin-pool resize support, dynamically changing thin-pool features, automatic thinp metadata checking when lvm activates the thin-pool, etc.
25. You can configure the Docker daemon to use a different directory, using the data-root configuration option.
26. If the daemon is completely non-responsive, you can also force a full stack trace of all threads to be added to the daemon log by sending the SIGUSR signal to the Docker daemon.
27. unable to configure the Docker daemon with file /etc/docker/daemon.json:
           the following directives are specified both as a flag and in the configuration
           file: hosts: (from flag: [unix:///var/run/docker.sock], from file: [tcp://127.0.0.1:2376])
    If you see an error similar to this one and you are starting the daemon manually with flags, you may need to adjust your flags or the daemon.json to remove the conflict.
28. On Debian and Ubuntu systems using systemd, this means that a host flag -H is always used when starting dockerd. If you specify a hosts entry in the daemon.json, this causes a configuration conflict (as in the above message) and Docker fails to start.
    To work around this problem, create a new file /etc/systemd/system/docker.service.d/docker.conf with the following contents, to remove the -H argument that is used when starting the daemon by default.
              [Service]
	      ExecStart=
              ExecStart=/usr/bin/dockerd
29. There are two ways to enable debugging. The recommended approach is to set the debug key to true in the daemon.json file. This method works for every Docker platform.
30. Send a HUP signal to the daemon to cause it to reload its configuration. On Linux hosts, use the following command.
         $ sudo kill -SIGHUP $(pidof dockerd)
31. If the daemon is unresponsive, you can force a full stack trace to be logged by sending a SIGUSR1 signal to the daemon.
             $ sudo kill -SIGUSR1 $(pidof dockerd)
32. The operating-system independent way to check whether Docker is running is to ask Docker, using the docker info command.
33. ou can also use operating system utilities, such as sudo systemctl is-active docker or sudo status docker or sudo service docker status.
34. Finally, you can check in the process list for the dockerd process, using commands like ps or top.
35. Docker Enterprise include the following components:
     Docker Engine - Enterprise. the commercially supported Docker container runtime
     Universal Control Plane (UCP), the web-based, unified cluster and application management solution
     Docker Kubernetes Service (DKS), a certified Kubernetes distribution with 'sensible secure defaults' out-of-the-box.
     Docker Trusted Registry (DTR), a resilient and secure image management repository
     Docker Desktop Enterprise (DDE), an enterprise friendly, supported version of the popular Docker Desktop application with an extended feature set.
     
36. Docker Engine - Enterprise is responsible for container-level operations, interaction with the OS, providing the Docker API, and running the Swarm cluster. 
37. Universal Control Plane -Enterprise by providing an integrated application management platform. It is both the main interaction point for users and the integration point for applications.
38. Docker Trusted Registry - the registry to store and distribute images, an image signing service, a Web UI, an API, and data stores for image metadata and DTR state.
39. Docker provides the ability to package and run an application in a loosely isolated environment called a container.
40. Docker Engine is a client-server application with these major components:
	A server which is a type of long-running program called a daemon process (the dockerd command).
        A REST API which specifies interfaces that programs can use to talk to the daemon and instruct it what to do.
	A command line interface (CLI) client (the docker command).
	The daemon creates and manages Docker objects, such as images, containers, networks, and volumes.
50. Docker uses a client-server architecture. The Docker client talks to the Docker daemon, which does the heavy lifting of building, running, and distributing your Docker containers.        
51. The Docker client and daemon can run on the same system, or you can connect a Docker client to a remote Docker daemon.
52. The Docker client and daemon communicate using a REST API, over UNIX sockets or a network interface. The Docker daemon (dockerd) listens for Docker API requests and manages Docker objects such as images, containers, networks, and volumes.
53. An image is a read-only template with instructions for creating a Docker container.
54. A container is a runnable instance of an image. ou can create, start, stop, move, or delete a container using the Docker API or CLI.
55. By default, containers can connect to external networks using the host machine’s network connection.
56. Each member of a swarm is a Docker daemon, and all the daemons communicate using the Docker API. 
57. Docker uses a technology called namespaces to provide the isolated workspace called the container. 
58. Docker Engine uses namespaces such as the following on Linux:
     The pid namespace: Process isolation (PID: Process ID).
     The net namespace: Managing network interfaces (NET: Networking).
     The ipc namespace: Managing access to IPC resources (IPC: InterProcess Communication).
     The mnt namespace: Managing filesystem mount points (MNT: Mount).
     The uts namespace: Isolating kernel and version identifiers. (UTS: Unix Timesharing System).
59. Docker Engine on Linux also relies on another technology called control groups (cgroups). A cgroup limits an application to a specific set of resources. 
     Control groups allow Docker Engine to share available hardware resources to containers and optionally enforce limits and constraints.
60. Union file systems, or UnionFS, are file systems that operate by creating layers, making them very lightweight and fast.
61. Docker Engine combines the namespaces, control groups, and UnionFS into a wrapper called a container format. The default container format is libcontainer.
62. By default, Docker runs through a non-networked UNIX socket. It can also optionally communicate using an HTTP socket.
63. If you need Docker to be reachable through the network in a safe manner, you can enable TLS by specifying the tlsverify flag and pointing Docker’s tlscacert flag to a trusted CA certificate.
64. If you want to secure your Docker client connections by default, you can move the files to the .docker directory in your home directory --- and set the DOCKER_HOST and DOCKER_TLS_VERIFY variables as well
65. A custom certificate is configured by creating a directory under /etc/docker/certs.d
66. Create the client certificates: Use OpenSSL’s genrsa and req commands to first generate an RSA key and then use the key to create the certificate.
       $ openssl genrsa -out client.key 4096
       $ openssl req -new -x509 -text -key client.key -out client.cert
67. The Docker daemon interprets .crt files as CA certificates and .cert files as client certificates. 
         If a CA certificate is accidentally given the extension .cert instead of the correct .crt extension, the Docker daemon logs the following error message:
        
           Missing key KEY_NAME for client certificate CERT_NAME. CA certificates should use the extension .crt.
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
NETWORKING:
Use a load balancer Docker EE


1. To set the DNS server for all Docker containers, use: $ sudo dockerd --dns 8.8.8.8
2. To set the DNS search domain for all Docker containers, use: sudo dockerd --dns-search example.com
3. Some options can be reconfigured when the daemon is running without requiring to restart the process. We use the SIGHUP signal in Linux to reload.
4. live-restore: Enables keeping containers alive during daemon downtime.
5. By default, Docker Engine connects container via a default bridge to the network of the node.
6. In terms of Docker, a bridge network uses a software bridge which allows containers connected to the same bridge network to communicate, while providing isolation from containers which are not connected to that bridge network. 
7. Bridge networks apply to containers running on the same Docker daemon host. For communication among containers running on different Docker daemon hosts, you can either manage routing at the OS level, or you can use an overlay network.
8. User-defined bridges provide automatic DNS resolution between containers.
9. Containers on the default bridge network can only access each other by IP addresses, unless you use the --link option, which is considered legacy. 
    On a user-defined bridge network, containers can resolve each other by name or alias.
10. User-defined bridges provide better isolation.
11. Using a user-defined network provides a scoped network in which only containers attached to that network are able to communicate.
12. Containers can be attached and detached from user-defined networks on the fly.
13. To remove a container from the default bridge network, you need to stop the container and recreate it with different network options.
14. Each user-defined network creates a configurable bridge.
15. If your containers use the default bridge network, you can configure it, but all the containers use the same settings, such as MTU and iptables rules. In addition, configuring the default bridge network happens outside of Docker itself, and requires a restart of Docker.
16. User-defined bridge networks are created and configured using docker network create. If different groups of applications have different network requirements, you can configure each user-defined bridge separately, as you create it.
17. Linked containers on the default bridge network share environment variables.
18. Originally, the only way to share environment variables between two containers was to link them using the --link flag. This type of variable sharing is not possible with user-defined networks.
19. There are superior ways to share environment variables. A few ideas:
    Multiple containers can mount a file or directory containing the shared information, using a Docker volume.
    Multiple containers can be started together using docker-compose and the compose file can define the shared variables.
    You can use swarm services instead of standalone containers, and take advantage of shared secrets and configs.
20. Containers connected to the same user-defined bridge network effectively expose all ports to each other. For a port to be accessible to containers or non-Docker hosts on different networks, that port must be published using the -p or --publish flag.
21. Use the docker network create command to create a user-defined bridge network: $ docker network create my-net
22. Use the docker network rm command to remove a user-defined bridge network: $ docker network rm my-net.
23. When you create a new container, you can specify one or more --network flags. 
24. To connect a running container to an existing user-defined bridge, use the docker network connect command: $ docker network connect my-net my-nginx
25. To disconnect a running container from a user-defined bridge, use the docker network disconnect command. docker network disconnect my-net my-nginx
26. You can specify the subnet, the IP address range, the gateway, and other options. See the docker network create reference or the output of docker network create --help for details.
27. By default, traffic from containers connected to the default bridge network is not forwarded to the outside world. To enable forwarding, you need to change two settings. 
         These are not Docker commands and they affect the Docker host’s kernel. 
                1. Configure the Linux kernel to allow IP forwarding. $ sysctl net.ipv4.conf.all.forwarding=1
                2. Change the policy for the iptables FORWARD policy from DROP to ACCEPT. $ sudo iptables -P FORWARD ACCEPT
28. The default bridge network is considered a legacy detail of Docker and is not recommended for production use. Configuring it is a manual operation, and it has technical shortcomings.
29. Containers connected to the default bridge network can communicate, but only by IP address, unless they are linked using the legacy --link flag.
30. To configure the default bridge network, you specify options in daemon.json. Here is an example daemon.json with several options specified. 
       {
  "bip": "192.168.1.5/24",
  "fixed-cidr": "192.168.1.5/25",
  "fixed-cidr-v6": "2001:db8::/64",
  "mtu": 1500,
  "default-gateway": "10.20.1.1",
  "default-gateway-v6": "2001:db8:abcd::89",
  "dns": ["10.20.1.2","10.20.1.3"]
}


31. The DRIVER accepts bridge or overlay which are the built-in network drivers. 
     If you don’t specify the --driver option, the command automatically creates a bridge network for you: $ docker network create -d bridge my-bridge-network

32. Bridge networks are isolated networks on a single Engine installation. If you want to create a network that spans multiple Docker hosts each running an Engine, you must create an overlay network.
33. overlay networks require some pre-existing conditions before you can create one. These conditions are:
	Access to a key-value store. Engine supports Consul, Etcd, and ZooKeeper (Distributed store) key-value stores.
        A cluster of hosts with connectivity to the key-value store.
	A properly configured Engine daemon on each host in the cluster.
34. The dockerd options that support the overlay network are:
	--cluster-store
	--cluster-store-opt
	--cluster-advertise
35. It is a good idea to install Docker Swarm to manage the cluster that makes up your network. Swarm provides sophisticated discovery and server management tools that can assist your implementation.
36. Command to create overlay network: $ docker network create -d overlay my-multihost-network
37. You should create overlay networks with /24 blocks (the default), which limits you to 256 IP addresses, when you create networks using the default VIP-based endpoint-mode. 
38. If you need more than 256 IP addresses, do not increase the IP block size. You can either use dnsrr endpoint mode with an external load balancer, or use multiple smaller overlay networks. 
39. When you start a container, use the --network flag to connect it to a network. This example adds the busybox container to the mynet network:
        $ docker run -itd --network=mynet busybox
40. When you create a network, Engine creates a non-overlapping subnetwork for the network by default. This subnetwork is not a subdivision of an existing network. It is purely for ip-addressing purposes. 
     You can override this default and specify subnetwork values directly using the --subnet option. On a bridge network you can only create a single subnet:
     $ docker network create --driver=bridge --subnet=192.168.0.0/16 br0
41. By default, when you connect a container to an overlay network, Docker also connects a bridge network to it to provide external connectivity. If you want to create an externally isolated overlay network, you can specify the --internal option.
42. You can create the network which will be used to provide the routing-mesh in the swarm cluster. You do so by specifying --ingress when creating the network. Only one ingress network can be created at the time.
43. docker network prune: Remove all unused networks
44. The Docker networking architecture is built on a set of interfaces called the Container Networking Model (CNM).
45. Components of Container Networking Model (CNM): Sandbox, Endpoint, Network.
46. Sandbox:  contains the configuration of a container's network stack. This includes the management of the container's interfaces, routing table, and DNS settings.
47. Endpoint — An Endpoint joins a Sandbox to a Network. 
48. Network - A Network is a collection of endpoints that have connectivity between them. Endpoints that are not connected to a network do not have connectivity on a network.
49. There are two broad types of CNM network drivers:
	1. Native Network Drivers: 
	2. Remote Network Drivers 
50. Native network drivers: Host, Bridge, Overlay, MACVLAN, None.
51. Host- With the host driver, a container uses the networking stack of the host. There is no namespace separation, and all interfaces on the host can be used directly by the container.
52. Bridge- The bridge driver creates a Linux bridge on the host that is managed by Docker. By default containers on a bridge can communicate with each other. External access to containers can also be configured through the bridge driver.
53. Overlay- The overlay driver creates an overlay network that supports multi-host networks out of the box. It uses a combination of local Linux bridges and VXLAN to overlay container-to-container communications over physical network infrastructure.
54. MACVLAN- The macvlan driver uses the Linux MACVLAN bridge mode to establish a connection between container interfaces and a parent host interface (or sub-interfaces). It can be used to provide IP addresses to containers that are routable on the physical network. Additionally VLANs can be trunked to the macvlan driver to enforce Layer 2 container segmentation.
55. None- The none driver gives a container its own networking stack and network namespace but does not configure interfaces inside the container. Without additional configuration, the container is completely isolated from the host networking stack.
56. Docker Remote Network Drivers: contiv, weave, kuryr
57. Docker Remote IPAM Drivers: infoblox
58. There are several Linux networking building blocks which Docker uses to implement its native CNM network drivers. This list includes Linux bridges, network namespaces, veth pairs, and iptables. 
59. A Linux bridge is a Layer 2 device that is the virtual implementation of a physical switch inside the Linux kernel. It forwards traffic based on MAC addresses which it learns dynamically by inspecting traffic.
60. A Linux network namespace is an isolated network stack in the kernel with its own interfaces, routes, and firewall rules. It is a security aspect of containers and Linux, used to isolate containers.
61. Network namespaces ensure that two containers on the same host aren't able to communicate with each other or even the host itself unless configured to do so via Docker networks.
     The host network namespace contains the host interfaces and host routing table.
62. A virtual ethernet device or veth is a Linux networking interface that acts as a connecting wire between two network namespaces.A veth is a full duplex link that has a single interface in each namespace.
63. Bridge: The default network driver. If you don’t specify a driver, this is the type of network you are creating. Bridge networks are usually used when your applications run in standalone containers that need to communicate. 
64. Host: For standalone containers, remove network isolation between the container and the Docker host, and use the host’s networking directly.
     If you use the host network mode for a container, that container’s network stack is not isolated from the Docker host (the container shares the host’s networking namespace), and the container does not get its own IP-address allocated. 
65. Host mode networking can be useful to optimize performance, and in situations where a container needs to handle a large range of ports, as it does not require network address translation (NAT), and no “userland-proxy” is created for each port.
66. You can also use a host network for a swarm service, by passing --network host to the docker service create command.
67. overlay: Overlay networks connect multiple Docker daemons together and enable swarm services to communicate with each other.
68. You need the following ports open to traffic to and from each Docker host participating on an overlay network:
        TCP port 2377 for cluster management communications
        TCP and UDP port 7946 for communication among nodes
	UDP port 4789 for overlay network traffic
69. To create an overlay network for use with swarm services, use a command like the following:$ docker network create -d overlay my-overlay
70. To create an overlay network which can be used by swarm services or standalone containers to communicate with other standalone containers running on other Docker daemons, add the --attachable flag:
       $ docker network create -d overlay --attachable my-attachable-overlay
71. All swarm service management traffic is encrypted by default using the AES algorithm in GCM mode.
72. To encrypt application data as well, add --opt encrypted when creating the overlay network. 
73. Overlay network encryption is not supported on Windows. If a Windows node attempts to connect to an encrypted overlay network, no error is detected but the node cannot communicate.
74. You can use the overlay network feature with both --opt encrypted --attachable and attach unmanaged containers to that network:
     $ docker network create --opt encrypted --driver overlay --attachable my-attachable-multi-host-network
75. Customizing the ingress network involves removing and recreating it. This is usually done before you create any services in the swarm. If you have existing services which publish ports, those services need to be removed before you can remove the ingress network.
76. The docker_gwbridge is a virtual bridge that connects the overlay networks (including the ingress network) to an individual Docker daemon’s physical network. 
     Docker creates it automatically when you initialize a swarm or join a Docker host to a swarm, but it is not a Docker device. 
77. Swarm services connected to the same overlay network effectively expose all ports to each other.
78. For a port to be accessible outside of the service, that port must be published using the -p or --publish flag on docker service create or docker service update
79. -p 8080:80 or -p published=8080,target=80: Map TCP port 80 on the service to port 8080 on the routing mesh.
80. -p 8080:80/udp or -p published=8080,target=80,protocol=udp: Map UDP port 80 on the service to port 8080 on the routing mesh.
81. -p 8080:80/tcp -p 8080:80/udp or -p published=8080,target=80,protocol=tcp -p published=8080,target=80,protocol=udp:
          Map TCP port 80 on the service to TCP port 8080 on the routing mesh, and map UDP port 80 on the service to UDP port 8080 on the routing mesh.
82. By default, swarm services which publish ports do so using the routing mesh. Services using the routing mesh are running in virtual IP (VIP) mode. 
83. When using the routing mesh, there is no guarantee about which Docker node services client requests.
84. To bypass the routing mesh, you can start a service using DNS Round Robin (DNSRR) mode, by setting the --endpoint-mode flag to dnsrr.
85. By default, control traffic relating to swarm management and traffic to and from your applications runs over the same network, though the swarm control traffic is encrypted.
86. You can configure Docker to use separate network interfaces for handling the two different types of traffic. When you initialize or join the swarm, specify --advertise-addr and --datapath-addr separately. You must do this for each node joining the swarm.
87. macvlan: Macvlan networks allow you to assign a MAC address to a container, making it appear as a physical device on your network. The Docker daemon routes traffic to containers by their MAC addresses.
88. none: For this container, disable all networking. Usually used in conjunction with a custom network driver. none is not available for swarm services. 
89. User-defined bridge networks are best when you need multiple containers to communicate on the same Docker host.
90. Host networks are best when the network stack should not be isolated from the Docker host, but you want other aspects of the container to be isolated.
91. Overlay networks are best when you need containers running on different Docker hosts to communicate, or when multiple applications work together using swarm services.
92. Macvlan networks are best when you are migrating from a VM setup or need your containers to look like physical hosts on your network, each with a unique MAC address.
93. Third-party network plugins allow you to integrate Docker with specialized network stacks.
94. List port mappings or a specific mapping for the container: docker port CONTAINER [PRIVATE_PORT[/PROTO]]
95. Basically, you can look up every port which is exposed by using docker inspect CONTAINER on every container. However, to just get the port binding information, you could use docker port CONTAINER.
96. To troubleshoot problems with container communication, you can use docker network inspect of the different containers to see, whether those containers are connected through the same network.
97. The traffic between DTR and UCP is always encrypted to ensure security. 
98. Traffic between containers is not encrypted by default
99. Docker focuses on three key areas of container security: secure access, secure content, and secure platform. 
100. By default the user inside the container is root. Using a defense in depth model, it is recommended that not all containers run as root. An easy way to mitigate this is to use the --user declaration at run time. 
101. Seccomp (short for Secure Computing Mode) is a security feature of the Linux kernel, used to restrict the syscalls available to a given process.
102. Seccomp profiles are applied at container creation time and cannot be altered for running containers.
103. AppArmor and SELinux are security modules similar to Seccomp in their use of profiles, however they differ in how those profiles are executed. 
      They need to be enabled on the host, while SELinux can be enabled at the daemon level. To enable SELinux in the Docker daemon, modify /etc/docker/daemon.json and add the following:
         {
           "selinux-enabled": true
		}
104. To check if SELinux is enabled:   docker info --format '{{.SecurityOptions}}'
105. Content Trust is the cryptographic guarantee that the image pulled is the correct image. export DOCKER_CONTENT_TRUST=1
106. UCP out of the box delivers with two Certificate Authorities (CA) with Mutual TLS. The two CAs set up by UCP include:
       The first CA is used for ALL internal communication between managers and workers
       The second CA is for the end user communication.
107. Worker nodes are unprivileged, meaning they do not have access to the cluster state or secrets. When adding nodes to the UCP cluster, a join token must be used
108. The client bundle allows end users to securely connect from a Docker Client to UCP via certificates when deploying workloads and administering the environment.
109. Docker Enterprise Access Control is a policy-based model that uses access control lists (ACLs) called grants to dictate access between users and cluster resources. A grant ties together who, has permission for which actions, against what resources.
110. UCP also provides default roles that are pre-created. These are common role types that can be used to ease the burden of creating custom roles.
      None: The user has no access to swarm resources. This maps to the No Access role in UCP 2.1.x.
      View Only	: The user can view resources like services, volumes, and networks but can't create them.
      Restricted Control: 
      Scheduler
      Full Control:
111. Docker Enterprise enables controlling access to swarm resources by using collections. 	                       
112. The routing mesh enables each node in the swarm to accept connections on published ports for any service running in the swarm, even if there’s no task running on the node.
113. To use the ingress network in the swarm, you need to have the following ports open between the swarm nodes before you enable swarm mode:
         Port 7946 TCP/UDP for container network discovery.
         Port 4789 UDP for the container ingress network.
114. Use the --publish flag to publish a port when you create a service. target is used to specify the port inside the container, and published is used to specify the port to bind on the routing mesh.
115. The <PUBLISHED-PORT> is the port where the swarm makes the service available. If you omit it, a random high-numbered port is bound. The <CONTAINER-PORT> is the port where the container listens.
      $ docker service create \
  --name my-web \
  --publish published=8080,target=80 \
  --replicas 2 \
  nginx
116. You can publish a port for an existing service using the following command:
          $ docker service update \
  --publish-add published=<PUBLISHED-PORT>,target=<CONTAINER-PORT> \
  <SERVICE>

117. You can use docker service inspect to view the service’s published port. For instance:
      docker service inspect --format="{{json .Endpoint.Spec.Ports}}" my-web

       [{"Protocol":"tcp","TargetPort":80,"PublishedPort":8080}]

118. By default, when you publish a port, it is a TCP port. dns-cache 
119. You can configure an external load balancer for swarm services, either in combination with the routing mesh or without using the routing mesh at all.
120. To use an external load balancer without the routing mesh, set --endpoint-mode to dnsrr.
121. Docker uses a concept called services to deploy applications. Services consist of containers created from the same image. Each service consists of tasks that execute on worker nodes and define the state of the application.
122. The ucp-interlock-proxy can be scaled up to have more replicas, and those replicas can be constrained to only those nodes that have high performance network interfaces
123. For services to be published using Interlock Proxy, they must contain, among other labels, at least two labels where the keys are com.docker.lb.hosts and com.docker.lb.port.
124. Update the high performance nodes by using node labels to identify them: docker node update --label-add nodetype=loadbalancer <node>
125. Constrain the Interlock Proxy service tasks to only run on the high performance nodes using the node labels. 
     This is done by updating the ucp-interlock service configuration to deploy the interlock proxy service with the updated constraints in the ProxyConstraints array as explained below:
126. The Interlock Proxy polls the Docker API for changes every 3 seconds (default), so once an application is deployed, Interlock Proxy polls for the new service and finds it at http://<domain-to-route>.
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

SECURITY:

Access control model

1. Universal Control Plane (UCP) is the enterprise-grade cluster management solution from Docker. You install it on-premises or in your virtual private cloud, and it helps you manage your Docker cluster and applications through a single interface.
2. With UCP, you can manage from a centralized place all of the computing resources you have available, like nodes, volumes, and networks. also deploy and monitor your applications and services.
3. UCP has its own built-in authentication mechanism and integrates with LDAP services. It also has role-based access control (RBAC), so that you can control who can access and make changes to your cluster and applications.
4. UCP integrates with Docker Trusted Registry (DTR) so that you can keep the Docker images you use for your applications behind your firewall, where they are safe and can’t be tampered with.
5. Docker Enterprise Edition (EE) is the only Containers as a Service (CaaS) Platform for IT that manages and secures diverse applications across disparate infrastructure, both on-premises and in the cloud.
6. A client bundle is a group of certificates downloadable directly from the Docker Universal Control Plane (UCP) user interface within the admin section for “My Profile”.
    A client bundle contains a private and public key pair that authorizes your requests in UCP. 
       It also contains utility scripts you can use to configure your Docker and kubectl client tools to talk to your UCP deployment.
7. Docker Trusted Registry (DTR) is a containerized application that runs on a Docker Universal Control Plane cluster.Once you have DTR deployed, you use your Docker CLI client to login, push, and pull images.
8. DTR internal components:When you install DTR on a node, the following containers are started:
	dtr-api-<replica_id>: Executes the DTR business logic. It serves the DTR web application and API
	dtr-garant-<replica_id>: Manages DTR authentication
	dtr-jobrunner-<replica_id>: Runs cleanup jobs in the background
	dtr-nginx-<replica_id>: Receives http and https requests and proxies them to other DTR components. By default it listens to ports 80 and 443 of the host
	dtr-notary-server-<replica_id>: Receives, validates, and serves content trust metadata, and is consulted when pushing or pulling to DTR with content trust enabled
	dtr-notary-signer-<replica_id>: Performs server-side timestamp and snapshot signing for content trust metadata
	dtr-registry-<replica_id>: implements the functionality for pulling and pushing Docker images. It also handles how images are stored
	dtr-rethinkdb-<replica_id>: A database for persisting repository metadata
	dtr-scanningstore-<replica_id>: Stores security scanning data
9. To allow containers to communicate, when installing DTR the following networks are created:
	Name: dtr-ol
	Type: overlay
	Description: Allows DTR components running on different nodes to communicate, to replicate DTR data
10. DTR uses these named volumes for persisting data:
	dtr-ca-<replica_id>
	dtr-notary-<replica_id>
	dtr-postgres-<replica_id>
	dtr-registry-<replica_id>
	dtr-rethink-<replica_id>
	dtr-nfs-registry-<replica_id>
11. we can customize the volume driver used for these volumes, by creating the volumes before installing DTR. During the installation, DTR checks which volumes don’t exist in the node, and creates them using the default volume driver.
	By default, the data for these volumes can be found at /var/lib/docker/volumes/<volume-name>/_data.
12. By default, Docker Trusted Registry stores images on the filesystem of the node where it is running, but you should configure it to use a centralized storage backend.		
13. DTR supports these storage back ends:
	NFS, Amazon S3, cleversafe, Google cloud storage, openstack swift, Microsoft azure.
14. Docker Enterprise is made up of three related products:
      Docker Engine - Enterprise
      Universal Control Plane (UCP)
      Docker Trusted Registry (DTR)
15. Docker Engine - Enterprise is a client-server application with these major components:
	A server which is a type of long-running program called a daemon process (the dockerd command).
	A REST API which specifies interfaces that programs can use to talk to the daemon and instruct it what to do.
	A command line interface (CLI) client (the docker command).
16. Universal Control Plane (UCP) is the enterprise-grade cluster management solution from Docker. You install it on-premises or in your virtual private cloud, and it helps you manage your Docker cluster and applications through a single interface.
17. If nodes are not already running in a swarm when installing UCP, nodes will be configured to run in swarm mode.
18. When you deploy UCP, it starts running a globally scheduled service called ucp-agent. This service monitors the node where it’s running and starts and stops UCP services, based on whether the node is a manager or a worker node.
19. If the node is a: Manager: the ucp-agent service automatically starts serving
    all UCP components, including the UCP web UI and data stores used by UCP. The ucp-agent accomplishes this by deploying several containers on the node. By promoting a node to manager, UCP automatically becomes highly available and fault tolerant.
20. If the node is a: Worker: on worker nodes, the ucp-agent service starts serving
    a proxy service that ensures only authorized users and other UCP services can run Docker commands in that node. The ucp-agent deploys a subset of containers on worker nodes.

21.The core component of UCP is a globally scheduled service called ucp-agent. 
22. By default, pause containers are hidden, but you can see them by running docker ps -a.
23. There are two ways to interact with UCP: the web UI or the CLI.
24. You can monitor the status of UCP using the web UI or the CLI. You can also use the _ping endpoint to build monitoring automation.
25. The nodes in a swarm use mutual Transport Layer Security (TLS) to authenticate, authorize, and encrypt the communications with other nodes in the swarm.
26. By default, the manager node generates a new root Certificate Authority (CA) along with a key pair, which are used to secure communications with other nodes that join the swarm. 
27. we can specify your own externally-generated root CA, using the --external-ca flag of the docker swarm init command.
28. The manager node also generates two tokens to use when you join additional nodes to the swarm: one worker token and one manager token. Each token includes the digest of the root CA’s certificate and a randomly generated secret.
29. Each time a new node joins the swarm, the manager issues a certificate to the node. 
30. The certificate contains a randomly generated node ID to identify the node under the certificate common name (CN) and the role under the organizational unit (OU).
31. By default, each node in the swarm renews its certificate every three months. You can configure this interval by running the docker swarm update --cert-expiry <TIME PERIOD> command.
32. Run docker swarm ca --rotate to generate a new CA certificate and key.
33. we can pass the --ca-cert and --external-ca flags to specify the root certificate and to use a root CA external to the swarm. 
34. -ca-cert and --ca-key flags to specify the exact certificate and key you would like the swarm to use.
35. When you issue the docker swarm ca --rotate command, the following things happen in sequence:
	1.Docker generates a cross-signed certificate. This means that a version of the new root CA certificate is signed with the old root CA certificate. This cross-signed certificate is used as an intermediate certificate for all new node certificates. 
          This ensures that nodes that still trust the old root CA can still validate a certificate signed by the new CA.
	2.In Docker 17.06 and higher, Docker also tells all nodes to immediately renew their TLS certificates. This process may take several minutes, depending on the number of nodes in the swarm.
	3.After every node in the swarm has a new TLS certificate signed by the new CA, Docker forgets about the old CA certificate and key material, and tells all the nodes to trust the new CA certificate only.
            This also causes a change in the swarm’s join tokens. The previous join tokens are no longer valid.
36. security features you can configure and use within your Docker Engine installation.
	1.You can configure Docker’s trust features so that your users can push and pull trusted images.
	2.You can protect the Docker daemon socket and ensure only trusted Docker client connections.
	3.You can use certificate-based client-server authentication to verify a Docker daemon has the rights to access images on a registry. 
	4.You can configure secure computing mode (Seccomp) policies to secure system calls in a container. 
	5.An AppArmor profile for Docker is installed with the official .deb packages. For information about this profile and overriding it.
	6.You can map the root user in the containers to a non-root user.
	7.You can also run the Docker daemon as a non-root user.
37. There are four major areas to consider when reviewing Docker security:
	the intrinsic security of the kernel and its support for namespaces and cgroups;
	the attack surface of the Docker daemon itself;
	loopholes in the container configuration profile, either by default, or when customized by users.
	the “hardening” security features of the kernel and how they interact with containers.
38. Use this command to create a backup of a UCP manager node.
       docker container run \
    --rm \
    --interactive \
    --name ucp \
    --log-driver none \
    --volume /var/run/docker.sock:/var/run/docker.sock \
    docker/ucp \
    backup [command options] > backup.tar
     This command creates a tar file with the contents of the volumes used by this UCP manager node, and prints it. You can then use the restore command to restore the data from an existing backup.

39. The backup contains private keys and other sensitive information. Use the --passphrase flag to encrypt the backup with PGP-compatible encryption or --no-passphrase to opt out (not recommended).
40. If you are installing UCP on a manager node with SELinunx enabled at the daemon and operating system level, you will need to pass ` –security-opt label=disable` in to your install command. 
    This flag will disable SELinux policies on the installation container. 
41. Use this command to verify the UCP images on this node. This command checks the UCP images that are available in this node, and pulls the ones that are missing.
          docker container run --rm -it \
        --name ucp \
        -v /var/run/docker.sock:/var/run/docker.sock \
        docker/ucp \
        images [command options]
42. Use this command to install UCP on a node. Running this command will initialize a new swarm, turn a node into a manager, and install UCP.
	docker/ucp  install [command options]
43. Use this command to restore a UCP cluster from a backup: docker/ucp restore
44. If the current node does not belong in a cluster, one will be initialized using the value of the --host-address flag
45.  When restoring on an existing swarm-mode cluster, no previous UCP components must be running on any node of the cluster. This cleanup can be performed with the uninstall-ucp command.
46. By default, the backup tar file is read from stdin. You can also bind-mount the backup file under /config/backup.tar, and run the restore command with the --interactive flag.
47. Docker Content Trust Keys set consists of the following classes of keys:
     an offline key that is the root of DCT for an image tag
     repository or tagging keys that sign tags
     server-managed keys such as the timestamp key, which provides freshness security guarantees for your repository
48. Offline key is used create tagging key, offline key belongs to person or orgnization. resides in client-side
49. tagging key is associated with the image repository. can used for push n pull tag images. resides in client-side
50. Timestamp key is associated with an image repos. Tssssshis is created by docker and resides inside server,
51. Within the Docker CLI we can sign and push a container image with the $ docker trust command syntax. This is built on top of the Notary feature set,
52. To sign a Docker Image you will need a delegation key pair. These keys can be generated locally using $ docker trust key generate, generated by a certificate authority.
53. If you are generating delegation keys with $ docker trust key generate, the private key is automatically added to the local trust store. (By default this is stored in ~/.docker/trust/)
54. If you are importing a separate key, such as one from a UCP Client Bundle you will need to use the $ docker trust key load command.
55. we will need to add the delegation public key to the Notary server; this is specific to a particular image repository in Notary known as a Global Unique Name (GUN).
         $ docker trust signer add --key cert.pem jeff dtr.example.com/admin/demo
56. Remote trust data for a tag or a repository can be viewed by the $ docker trust inspect command
57. DCT is controlled by the Docker Engine’s configuration file. By default this is found at /etc/docker/daemon.json
58. The content-trust flag is based around a mode variable instructing the engine whether to enforce signed images, and a trust-pinning variable instructing the engine which sources to trust.
59. Currently, content trust is disabled by default in the Docker Client. To enable it, set the DOCKER_CONTENT_TRUST environment variable to 1
60. The commands that operate with DCT are:
         push
	build
	create
	pull
	run

----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
SECURITY:

1. Docker supports the following storage drivers:
    overlay2, devicemapper, btrfs, zfs, vfs, aufs
2. devicemapper is supported, but requires direct-lvm for production environments, because loopback-lvm, while zero-configuration, has very poor performance.
3. The vfs storage driver is intended for testing purposes, and for situations where no copy-on-write filesystem can be used. Performance of this storage driver is poor, and is not generally recommended for production use.
4. overlay2, aufs, and overlay all operate at the file level rather than the block level. This uses memory more efficiently, but the container’s writable layer may grow quite large in write-heavy workloads
5. Block-level storage drivers such as devicemapper, btrfs, and zfs perform better for write-heavy workloads.
6. btrfs and zfs require a lot of memory.
7. zfs is a good choice for high-density workloads such as PaaS.
8. To see what storage driver Docker is currently using, use docker info and look for the Storage Driver line:
9. Configure loop-lvm mode for testing: The loop-lvm mode makes use of a ‘loopback’ mechanism that allows files on the local disk to be read from and written to as if they were an actual physical disk or block device.
10. setting up loop-lvm mode can help identify basic issues (such as missing user space packages, kernel drivers, etc.) 
11. Stop Docker: $ sudo systemctl stop docker
12. Edit /etc/docker/daemon.json. If it does not yet exist, create it. Assuming that the file was empty, add the following contents for adding devicemapper:
       {
	  "storage-driver": "devicemapper"
	}
13. Start Docker: $ sudo systemctl start docker
14. Production hosts using the devicemapper storage driver must use direct-lvm mode. This mode uses block devices to create the thin pool. 
15. Direct-lvm is faster than using loopback devices, uses system resources more efficiently, and block devices can grow as needed. However, more setup is required than in loop-lvm mode.
16. we can monitor free space on the volume using lvs or lvs -a. Consider using a monitoring tool at the OS level, such as Nagios.
17. To view the LVM logs, you can use journalctl:
	$ sudo journalctl -fu dm-event.service
18. RESIZE A LOOP-LVM THIN POOL: The easiest way to resize a loop-lvm thin pool is to use the device_tool utility but you can use operating system utilities instead.
19. Use the tool. The following example resizes the thin pool to 200GB by manually.
      $ ./device_tool resize 200GB
20. In loop-lvm mode, a loopback device is used to store the data, and another to store the metadata. loop-lvm mode is only supported for testing, because it has significant performance and stability drawbacks.
21. If you are using loop-lvm mode, the output of docker info shows file paths for Data loop file and Metadata loop file:
         $ docker info |grep 'loop file'
22. Increase the size of the data file to 200 G using the truncate command, which is used to increase or decrease the size of a file: manually:loop-lvm:Use operating system utilities
         $ sudo truncate -s 200G /var/lib/docker/devicemapper/data
23. Use the pvdisplay command to find the physical block devices currently in use by your thin pool, and the volume group’s name.
          $ sudo pvdisplay |grep 'VG Name'
24. Activate the devicemapper after reboot: If you reboot the host and find that the docker service failed to start, look for the error, “Non existing device”. 
      You need to re-activate the logical volumes with this command:
        $ sudo lvchange -ay docker/thinpool
25. The /var/lib/docker/devicemapper/metadata/ directory contains metadata about the Devicemapper configuration itself and about each image and container layer that exist.
26. The devicemapper storage driver uses snapshots, and this metadata include information about those snapshots. These files are in JSON format.
27. The /var/lib/docker/devicemapper/mnt/ directory contains a mount point for each image and container layer that exists.
28. Image layer mount points are empty, but a container’s mount point shows the container’s filesystem as it appears from within the container.
29. Another feature of devicemapper is its use of snapshots (also sometimes called thin devices or virtual devices), which store the differences introduced in each layer as very small, lightweight thin pools.
30. Snapshots provide many benefits:
	Layers which are shared in common between containers are only stored on disk once, unless they are writable. For instance, if you have 10 different images which are all based on alpine, the alpine image and all its parent images are only stored once each on disk.
	Snapshots are an implementation of a copy-on-write (CoW) strategy. This means that a given file or directory is only copied to the container’s writable layer when it is modified or deleted by that container.
	Because devicemapper operates at the block level, multiple blocks in a writable layer can be modified simultaneously.
	Snapshots can be backed up using standard OS-level backup utilities. Just make a copy of /var/lib/docker/devicemapper/.
31. Writing a new file: With the devicemapper driver, writing new data to a container is accomplished by an allocate-on-demand operation. Each block of the new file is allocated in the container’s writable layer and the block is written there.
32. Writing and then deleting a file:If a container writes to a file and later deletes the file, all of those operations happen in the container’s writable layer. In that case, if you are using direct-lvm, the blocks are freed. 
     If you use loop-lvm, the blocks may not be freed. This is another reason not to use loop-lvm in production.
33. Volumes are the preferred mechanism for persisting data generated by and used by Docker containers.
34. While bind mounts are dependent on the directory structure of the host machine.
35. Volumes have several advantages over bind mounts:
	Volumes are easier to back up or migrate than bind mounts.
	You can manage volumes using Docker CLI commands or the Docker API.
	Volumes work on both Linux and Windows containers.
	Volumes can be more safely shared among multiple containers.
	Volume drivers let you store volumes on remote hosts or cloud providers, to encrypt the contents of volumes, or to add other functionality.
	New volumes can have their content pre-populated by a container.
36. The -v or --volume flag was used for standalone containers and the --mount flag was used for swarm services. we can also use --mount with standalone containers.
37. The biggest difference is that the -v syntax combines all the options together in one field, while the --mount syntax separates them.
38. -v or --volume: Consists of three fields, separated by colon characters (:).
	the first field is the name of the volume, and is unique on a given host machine.
	The second field is the path where the file or directory are mounted in the container.
	The third field is optional, and is a comma-separated list of options. ro   //read-only.
39. --mount: Consists of multiple key-value pairs, separated by commas and each consisting of a <key>=<value> tuple. 
40. The type of the mount, which can be bind, volume, or tmpfs
41. The source of the mount. For named volumes, this is the name of the volume.
42. The destination takes as its value the path where the file or directory is mounted in the container. When using volumes with services, only --mount is supported.
43. you can create and manage volumes outside the scope of any container.
	$ docker volume create my-vol
44. If you start a container with a volume that does not yet exist, Docker creates the volume for you. 
45. $ docker run -d \
  --name devtest \						/// using mount
  --mount source=myvol2,target=/app \
  nginx:latest

46. $ docker run -d \
  --name devtest \
  -v myvol2:/app \						//using -v
  nginx:latest 

47. Stop the container and remove the volume. Note volume removal is a separate step.
	$ docker container stop devtest
	$ docker container rm devtest
	$ docker volume rm myvol2
48. The following example starts a nginx service with four replicas, each of which uses a local volume called myvol2.
	$ docker service create -d \
	  --replicas=4 \
	  --name devtest-service \
 	 --mount source=myvol2,target=/app \
	  nginx:latest
Use docker service ps devtest-service to verify that the service is running:
  $ docker service ps devtest-service

ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS
4d7oz1j85wwn        devtest-service.1   nginx:latest        moby   		Running              Running 14 seconds ago

49. Remove the service, which stops all its tasks:
	$ docker service rm devtest-service
	Removing the service does not remove any volumes created by the service. Volume removal is a separate step.

50. The docker service create command does not support the -v or --volume flag.
51. When mounting a volume into a service’s containers, you must use the --mount flag.
52. Create a volume using a volume driver:
	$ docker volume create --driver vieux/sshfs \
 	 -o sshcmd=test@node2:/home/test \
 	 -o password=testpassword \
	  sshvolume
53. If the volume driver requires you to pass options, you must use the --mount flag to mount the volume, rather than -v.
54. Volumes are useful for backups, restores, and migrations. Use the --volumes-from flag to create a new container that mounts that volume.
55. A Docker data volume persists after a container is deleted. There are two types of volumes
	Named volumes have a specific source from outside the container, for example awesome:/bar.
	Anonymous volumes have no specific source so when the container is deleted, instruct the Docker Engine daemon to remove them.
56. To automatically remove anonymous volumes, use the --rm option
57. To remove all unused volumes and free up space:
	$ docker volume prune
58. use docker system prune to clean up multiple types of objects at once.
59. The docker image prune command allows you to clean up unused images. By default, docker image prune only cleans up dangling images. A dangling image is one that is not tagged and is not referenced by any container. 
	$ docker image prune
60. To remove all images which are not used by existing containers, use the -a flag: $ docker image prune -a
61. You can limit which images are pruned using filtering expressions with the --filter flag. For example, to only consider images created more than 24 hours ago:
	$ docker image prune -a --filter "until=24h"
62. When you stop a container, it is not automatically removed unless you started it with the --rm flag.
63. A stopped container’s writable layers still take up disk space. To clean this up, you can use the docker container prune command.
	$ docker container prune
64. By default, all stopped containers are removed. You can limit the scope using the --filter flag. $ docker container prune --filter "until=24h"
65. Volumes can be used by one or more containers, and take up space on the Docker host. Volumes are never removed automatically, because to do so could destroy data.
	$ docker volume prune
66. By default, all unused volumes are removed. You can limit the scope using the --filter flag.
	$ docker volume prune --filter "label!=keep"
67. Docker networks don’t take up much disk space, but they do create iptables rules, bridge network devices, and routing table entries. To clean these things up, you can use docker network prune to clean up networks which aren’t used by any containers.
68. By default, all unused networks are removed. You can limit the scope using the --filter flag.
	$ docker network prune --filter "until=24h"
69. DTR garbage collection can be ran in dry mode using the PARAM_DRY_RUN=true environment variable.


















































      






 












































 

















     



































































































   



	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 


























