ORCHESTRATION:
---------------------------------------------------------------------------------------------------------------------------------------------------------
1. Swarm services use a declarative model
2. To confirm that the service was created and started successfully, use the docker service ls
3. To provide a name for your service, use the --name flag: docker service create --name my_web nginx
4. gMSA credential spec - a requirement for Active Directory-authenticated applications. 
   This reduces the burden of distributing credential specs to the nodes they’re used on.
5. To use a Config as a credential spec, first create the Docker Config containing the credential spec: docker config create credspec credspec.json
6. you can create a service using this credential spec. To do so, use the --credential-spec flag with the config name, like this: docker service create --credential-spec="config://credspec" <your image>
7. If your image is available on a private registry which requires login, use the --with-registry-auth flag with docker service create
8. If your image is stored on registry.example.com, which is a private registry, use a command like the docker login registry.example.com
9. Credential spec files are applied at runtime.
10. no gMSA credentials are written to disk on worker nodes.
11. When deploying a service using a gMSA-based config, the credential spec is passed directly to the runtime of containers in that service.
12. To use a Config as a credential spec, create a Docker Config in a credential spec file named credpspec.json.
13. You can change almost everything about an existing service using the docker service update command. 
14. Since Nginx is a web service, it works much better if you publish port 80 to clients outside the swarm. You can specify this when you create the service, using the -p or --publish flag.
15. When updating an existing service, the flag is --publish-add. Assuming that the my_web service from the previous section still exists, use the following command to update it to publish port 80.
       docker service update --publish-add 80 my_web
16. here is also a --publish-rm flag to remove a port that was previously published.
17. To remove a service, use the docker service remove command.
18. If you specify a tag, the manager resolves that tag to a digest.
19. When the request to create a container task is received on a worker node, the worker node only sees the digest, not the tag.
20. When you create a service, the image’s tag is resolved to the specific digest the tag points to at the time of service creation.
21. If the manager can’t resolve the tag to a digest, each worker node is responsible for resolving the tag to a digest, and different nodes may use different versions of the image.
22. To see an image’s current digest, issue the command docker inspect <IMAGE>:<TAG> and look for the RepoDigests line.
23. If the swarm manager can resolve the image tag to a digest, it instructs the worker nodes to redeploy the tasks and use the image at that digest.
24. When you create a swarm service, you can publish that service’s ports to hosts outside the swarm in two ways: 
       i. You can rely on the routing mesh.  ii. You can publish a service task’s port directly on the swarm node .
25. Routing mesh: When you publish a service port, the swarm makes the service accessible at the target port on every node, regardless of whether there is a task for the service running on that node or not. This is less complex and is the right choice for many types of services.
26. PUBLISH A SERVICE’S PORTS USING THE ROUTING MESH: To publish a service’s ports externally to the swarm, use the --publish <PUBLISHED-PORT>:<SERVICE-PORT> flag
27. If an external host connects to that port on any swarm node, the routing mesh routes it to a task.
28. The external host does not need to know the IP addresses or internally-used ports of the service tasks to interact with the service. When a user or process connects to a service, any worker node running a service task may respond.
29. PUBLISH A SERVICE’S PORTS DIRECTLY ON THE SWARM NODE: You can publish a service task’s port directly on the swarm node where that service is running.
30. PUBLISH A SERVICE’S PORTS DIRECTLY ON THE SWARM NODE: provides the maximum flexibility, including the ability for you to develop your own routing framework.
31. To publish a service’s port directly on the node where it is running, use the mode=host option to the --publish flag.
32. create overlay network on a manager node using the docker network create command with the --driver overlay flag.
33. You can create a new service and pass the --network flag to attach the service to the overlay network: 
    docker service create \
  --replicas 3 \
  --network my-network \
  --name my-web \
    nginx
34. You can also connect an existing service to an overlay network using the --network-add flag. docker service update --network-add my-network my-web.
35. To disconnect a running service from a network, use the --network-rm flag.: docker service update --network-rm my-network my-web.
36. Placement constraints let you configure the service to run only on nodes with specific (arbitrary) metadata set, and cause the deployment to fail if appropriate nodes do not exist. 
37. you can specify that your service should only run on nodes where an arbitrary label pci_compliant is set to true.
38. Placement preferences let you apply an arbitrary label with a range of values to each node, and spread your service’s tasks across those nodes using an algorithm.
39. Swarm mode has two types of services: replicated and global.
40. For replicated services, you specify the number of replica tasks for the swarm manager to schedule onto available nodes.
41. For global services, the scheduler places one task on each available node that meets the service’s placement constraints and resource requirements.
42. I created a deployment that runs several identical tasks on nodes. which type of service deployment is this?: Replicated.
43. I created a deployment that runs exactly one task on every node. which type of service deployment is this? Global.
44. You control the type of service using the --mode flag. If you don’t specify a mode, the service defaults to replicated.
44. For replicated services, you specify the number of replica tasks you want to start using the --replicas flag. For example, to start a replicated nginx service with 3 replica tasks:
     docker service create \
  --name my_web \
  --replicas 3 \
  nginx
45. To start a global service on each available node, pass --mode global to docker service create.
46. The global service on the new node.
      docker service create \
  --name myservice \
  --mode global \
    alpine top
47. To reserve a given amount of memory or number of CPUs for a service, use the --reserve-memory or --reserve-cpu flags.
48. PLACEMENT CONSTRAINTS: service only runs on nodes with the label region set to east. If no appropriately-labelled nodes are available, tasks will wait in Pending  until they become available.
49. PLACEMENT CONSTRAINTS: The --constraint flag uses an equality operator (== or !=)
     docker service create \
  --name my-nginx \
  --replicas 5 \
  --constraint node.labels.region==east \
  nginx
50. PLACEMENT PREFERENCES: While placement constraints limit the nodes a service can run on, placement preferences try to place tasks on appropriate nodes in an algorithmic way (currently, only spread evenly). 
      docker service create \
  --replicas 9 \
  --name redis_2 \
  --placement-pref 'spread=node.labels.datacenter' \
  redis:3.0.6
  
51. When updating a service with docker service update, --placement-pref-add appends a new placement preference after all existing placement preferences.
52. --placement-pref-rm removes an existing placement preference that matches the argument.
53. The --update-delay flag configures the time delay between updates to a service task or sets of tasks.
54. By default the scheduler updates 1 task at a time. You can pass the --update-parallelism flag to configure the maximum number of service tasks that the scheduler updates simultaneously.
55. When an update to an individual task returns a state of RUNNING, the scheduler continues the update by continuing to another task until all tasks are updated. If, at any time during an update a task returns FAILED, the scheduler pauses the update.
    You can control the behavior using the --update-failure-action flag for docker service create or docker service update.
56. When an updated task returns either RUNNING or FAILED, the scheduler waits 10 seconds before stopping the next task to update:
      docker service create \
  --replicas 10 \
  --name my_web \
  --update-delay 10s \
  --update-parallelism 2 \
  --update-failure-action continue \
  alpine
57. The --update-max-failure-ratio flag controls what fraction of tasks can fail during an update before the update as a whole is considered to have failed. 
58. An individual task update is considered to have failed if the task doesn’t start up, or if it stops running within the monitoring period specified with the --update-monitor flag.
59. The default value for --update-monitor is 30 seconds.
60. In case the updated version of a service doesn’t function as expected, it’s possible to manually roll back to the previous version of the service using docker service update’s --rollback flag.
61. You can create two types of mounts for services in a swarm, volume mounts or bind mounts.
62. using the --mount flag when you create a service. the --mount-add or --mount-rm flag when updating an existing service. 
63. Data volumes are storage that exist independently of a container.
64. Volumes can be created before deploying a service, or if they don’t exist on a particular host when a task is scheduled there, they are created automatically according to the volume specification on the service. 
65. The default volume driver is local.
66.  To use a different volume driver with this create-on-demand pattern, specify the driver and its options with the --mount flag
67. Bind mounts are file system paths from the host where the scheduler deploys the container for the task. 
     docker service create \
  --mount type=bind,src=<HOST-PATH>,dst=<CONTAINER-PATH> \     /// READ-WRITE BIND.
  --name myservice \
  <IMAGE>
68. docker service create \
  --mount type=bind,src=<HOST-PATH>,dst=<CONTAINER-PATH>,readonly \   ///READ-ONLY
  --name myservice \
  <IMAGE>
69. Bind mounts can be useful but they can also cause problems. The main risks include the following:
      i. If you bind mount a host path into your service’s containers, the path must exist on every swarm node.The Docker swarm mode scheduler can schedule containers on any machine that meets resource availability requirements and satisfies all constraints and placement preferences you specify
      ii. The Docker swarm mode scheduler may reschedule your running service containers at any time if they become unhealthy or unreachable.
	  iii. Host bind mounts are non-portable. When you use bind mounts, there is no guarantee that your application runs the same way in development as it does in production.
70. The following flags are supported for template:
      --hostname
      --mount
      --env	  
71. docker node update [OPTIONS] NODE: 
     --availability ->Availability of the node (“active”|”pause”|”drain”)
     --label-add->Add or update a node label (key=value) 
     --label-rm->Remove a node label if exists	 
     --role->Role of the node (“worker”|”manager”)
72. Add metadata to a swarm node using node labels.  $ docker node update --label-add foo worker1
73. To add multiple labels to a node, pass the --label-add flag for each label: $ docker node update --label-add foo --label-add bar worker1
74. When you create a service, you can use node labels as a constraint. A constraint limits the nodes where the scheduler deploys tasks for a service.
         $ docker node update --label-add type=queue worker1    ///where type is a label and queue is service.
75. Open a terminal and ssh into the machine where you want to run your manager node. you can connect to it via SSH using the  command:
        $ docker-machine ssh manager1
76. Run the command to create a new swarm: $ docker swarm init --advertise-addr <MANAGER-IP>
77. The --advertise-addr flag configures the manager node to publish its address as 192.168.99.100. The other nodes in the swarm must be able to access the manager at the IP address.
78. Run docker info to view the current state of the swarm:
79. Run the docker node ls command to view information about nodes:
80. you can run the following command on a manager node to retrieve the join command for a worker: docker swarm join-token worker.
81. STACK must be executed on a swarm manager node.
82. The Compose file is a YAML file defining services, networks and volumes. The default path for a Compose file is ./docker-compose.yml.
83. restart: no is the default restart policy, and it does not restart a container under any circumstance. When always is specified, the container always restarts.
     The on-failure policy restarts a container if the exit code indicates an on-failure error and restart: unless-stopped
            restart: "no"
            restart: always
            restart: on-failure
            restart: unless-stopped
84. The restart option is ignored when deploying a stack in swarm mode.
85. The Raft logs used by swarm managers are encrypted on disk by default.
86. When Docker restarts, both the TLS key used to encrypt communication among swarm nodes, and the key used to encrypt and decrypt Raft logs on disk, are loaded into each manager node’s memory.
87. Ability to protect the mutual TLS encryption key and the key used to encrypt and decrypt Raft logs at rest, by allowing you to take ownership of these keys and to require manual unlocking of your managers. This feature is called autolock.
88. When Docker restarts, you must unlock the swarm first, using a key encryption key generated by Docker when the swarm was locked. You can rotate this key encryption key at any time.
89. You don’t need to unlock the swarm when a new node joins the swarm, because the key is propagated to it over mutual TLS.
90. When you initialize a new swarm, you can use the --autolock flag to enable autolocking of swarm manager nodes when Docker restarts.
                $ docker swarm init --autolock.
91. To enable autolock on an existing swarm, set the autolock flag to true: $ docker swarm update --autolock=true
92. To disable autolock, set --autolock to false. $ docker swarm update --autolock=false.
93. To unlock a locked swarm, use docker swarm unlock. : $ docker swarm unlock
94. If the key has not been rotated since the node left the swarm, and you have a quorum of functional manager nodes in the swarm, you can view the current unlock key using
         $ docker swarm unlock-key
95. docker swarm unlock-key --rotate
96. $ docker service ls

	ID            NAME        SCALE  IMAGE   COMMAND
	9uk4639qpg7n  helloworld  1/1    alpine  ping docker.com
97. A task is the atomic unit of scheduling within a swarm.
98. The container is the instantiation of the task.
99. API: accepts command and  create service object.
100. orchestrator: reconsillation loop that creates tasks for service object.
111. allocater:  allocates IP address to task.
112. dispatcher: assign task to nodes.
113. scheduler: instructs a worker to run a task. 
114. If your only intention is to prevent a service from being deployed, scale the service to 0 instead of trying to configure it in such a way that it remains in pending.
115. If all nodes are paused or drained, and you create a service, it is pending until a node becomes available.
116. The first node to become available gets all of the tasks, so this is not a good thing to do in a production environment.   
117. You can reserve a specific amount of memory for a service. If no node in the swarm has the required amount of memory, the service remains in a pending state until a node is available which can run its tasks.
118. For a replicated service, you specify the number of identical tasks you want to run. For example, you decide to deploy an HTTP service with three replicas, each serving the same content.
119. A global service is a service that runs one task on every node. There is no pre-specified number of tasks.
120: Running Services on a Docker Swarm can be scaled in different ways: docker service scale SERVICE=NUMBER. This works only for replicated services. 
       Another way is using docker service update --replicas=NUMBER SERVICE to achieve the same goal.
121. multiple services can be scaled at once, using the docker service scale SERVICE1=NUMBER1 SERVICE2=NUMBER2
122. docker inspect: Return low-level information on Docker objects. Docker inspect provides detailed information on constructs controlled by Docker.
123. By default, docker inspect will render results in a JSON array.
124. Run docker service inspect --pretty <SERVICE-ID> to display the details about a service in an easily readable format.
125. Run docker service ps <SERVICE-ID> to see which nodes are running the service.
126. By default, manager nodes in a swarm can execute tasks just like worker nodes.
127. docker stack ls shows an overview of all existing stacks.
128. docker stack ps STACK lists all tasks in a stack.
129. docker stack services STACK gives an overview over the running services.
130. docker stack rm STACK removes one or more stack.
131. Swarm manager nodes use the Raft Consensus Algorithm to manage the swarm state.
132. manager nodes are the key components for managing the swarm and storing the swarm state.
133. More Manager nodes reduce write performance because more nodes must acknowledge proposals to update the swarm state. This means more network round-trip traffic.
134. Raft requires a majority of managers, also called the quorum.
135. If the swarm loses the quorum of managers, the swarm cannot perform management tasks.
136. Even if a swarm loses the quorum of managers, swarm tasks on existing worker nodes continue to run. However, swarm nodes cannot be added, updated, or removed, and new or existing tasks cannot be started, stopped, moved, or updated.
137. Scaling down to a single manager is an unsafe operation and is not recommended.
138. If the last node leaves the swarm unexpectedly during the demote operation, the swarm becomes unavailable until you reboot the node or restart with --force-new-cluster.
139. By default manager nodes also act as a worker nodes. This means the scheduler can assign tasks to a manager node.
140. To avoid interference with manager node operation, you can drain manager nodes to make them unavailable as worker nodes:
           docker node update --availability drain <NODE>
141. When you drain a node, the scheduler reassigns any tasks running on the node to other available worker nodes in the swarm. It also prevents the scheduler from assigning tasks to the node.
142. You can monitor the health of manager nodes by querying the docker nodes API in JSON format through the /nodes HTTP endpoint
      docker node inspect manager1 --format "{{ .ManagerStatus.Reachability }}"
      reachable
       
      docker node inspect manager1 --format "{{ .Status.State }}"
      ready

142. An unreachable health status means that this particular manager node is unreachable from other manager nodes. In this case you need to take action to restore the unreachable manager:

     Restart the daemon and see if the manager comes back as reachable.     
     Reboot the machine.
     If neither restarting or rebooting work, you should add another manager node or promote a worker to be a manager node. You also need to cleanly remove the failed node entry from the manager set with docker node demote <NODE> and docker node rm <id-node>.
143. You should never restart a manager node by copying the raft directory from another node. The data directory is unique to a node ID. 
     A node can only use a node ID once to join the swarm. The node ID space should be globally unique.
145. To cleanly re-join a manager node to a cluster:

	To demote the node to a worker, run docker node demote <NODE>.
	To remove the node from the swarm, run docker node rm <NODE>.
	Re-join the node to the swarm with a fresh state using docker swarm join.
146. remove a node: docker node rm
147. you can forcefully remove the node without shutting it down by passing the --force flag. docker node rm --force node9
148. Before you forcefully remove a manager node, you must first demote it to the worker role. Make sure that you always have an odd number of manager nodes if you demote or remove a manager.
149. Docker manager nodes store the swarm state and manager logs in the /var/lib/docker/swarm/ directory.
150. You can back up the swarm using any manager. Use the following procedure. 
	1. If the swarm has auto-lock enabled, you need the unlock key to restore the swarm from backup. 
        Retrieve the unlock key if necessary and store it in a safe location. If you are unsure, read Lock your swarm to protect its encryption key.
        2. Stop Docker on the manager before backing up the data, so that no data is being changed during the backup. 
        It is possible to take a backup while the manager is running (a “hot” backup), but this is not recommended and your results are less predictable when restoring. While the manager is down, other nodes continue generating swarm data that is not part of this backup.
	3. Back up the entire /var/lib/docker/swarm directory.
        4. Restart the manager.
151. Recover from disaster: Restore from a backup.
        1. Shut down Docker on the target host machine for the restored swarm.
        2. Remove the contents of the /var/lib/docker/swarm directory on the new swarm.
        3. Restore the /var/lib/docker/swarm directory with the contents of the backup.
	4. Start Docker on the new node. Unlock the swarm if necessary. Re-initialize the swarm using the following command, 	
           so that this node does not attempt to connect to nodes that were part of the old swarm, and presumably no longer exist. $ docker swarm init --force-new-cluster
	5. Verify that the state of the swarm is as expected. This may include application-specific tests or simply checking the output of docker service ls to be sure that all expected services are present.
	6. If you use auto-lock, rotate the unlock key.
	7. Add manager and worker nodes to bring your new swarm up to operating capacity.
	8. Reinstate your previous backup regimen on the new swarm.
152. swarm cannot automatically recover if it loses a quorum.
153. The swarm can tolerate up to (N-1)/2 permanent failures beyond which requests involving swarm management cannot be processed.
154. The best way to recover from losing the quorum is to bring the failed nodes back online. If you can’t do that, the only way to recover from this state is to use the --force-new-cluster action from a manager node.
155.  you can use the --force or -f flag with the docker service update command to force the service to redistribute its tasks across the available worker nodes. 
156. When the container starts, it can only be connected to a single network, using --network
157. you can connect a running container to multiple networks using docker network connect.
158. container’s hostname defaults to be the container’s ID in Docker.
159. can override the hostname using --hostname.
-------------------------------------------------------------------------------------------------------------------------------------------------------------
IMAGE MANAGEMENT AND REGISTRY.

1. Build an image from a Dockerfile
2. The results of the health checks are available at the /debug/health endpoint on the debug HTTP server if the debug HTTP server is enabled.
3. You can delete an image from the Docker Hub by deleting individual tags in your repository in the tags area.
4. An image may be deleted from the registry via its name and reference. A delete may be issued with the following request format: DELETE /v2/<name>/manifests/<reference>
5. Tag Image: docker image tag SOURCE_IMAGE[:TAG] TARGET_IMAGE[:TAG]
6. docker image build: Build an image from a Dockerfile, docker image import: Import the contents from a tarball to create a filesystem image. docker image load: Load an image from a tar archive or STDIN. docker image save: Save one or more images to a tar archive (streamed to STDOUT by default).
7. The Registry is a stateless, highly scalable server side application that stores and lets you distribute Docker images. The Registry is open-source, under the permissive Apache license.
8. Start your registry: docker run -d -p 5000:5000 --name registry registry:2
9. Tag the image so that it points to your registry: docker image tag ubuntu localhost:5000/myfirstimage
10. Push it: Docker push localhost:5000/myfirstimage
11. Pull it back: docker pull localhost:5000/myfirstimage
12. Now stop your registry and remove all data: docker container stop registry && docker container rm -v registry
13. Docker can build images automatically by reading the instructions from a Dockerfile.
14. The docker build command builds an image from a Dockerfile and a context. 
15. The build is run by the Docker daemon, not by the CLI.
16. use the -f flag with docker build to point to a Dockerfile anywhere in your file system. $ docker build -f /path/to/a/Dockerfile .
17. You can specify a repository and tag at which to save the new image if the build succeeds: $ docker build -t shykes/myapp .
18. To tag the image into multiple repositories after the build, add multiple -t parameters when you run the build command:
      $ docker build -t shykes/myapp:1.0.2 -t shykes/myapp:latest .
19. Each instruction Dockerfile is run independently, and causes a new image to be created - so RUN cd /tmp will not have any effect on the next instructions.
20. Whenever possible, Docker will re-use the intermediate images (cache), to accelerate the docker build process significantly.
21. Build cache is only used from images that have a local parent chain. This means that these images were created by previous builds or the whole chain of images was loaded with docker load.
22. If you wish to use build cache of a specific image you can specify it with --cache-from option.Images specified with --cache-from do not need to have a parent chain and may be pulled from other registries.
23. To use the BuildKit backend, you need to set an environment variable DOCKER_BUILDKIT=1 on the CLI before invoking docker build.
24. A Dockerfile must begin with a `FROM` instruction. 
25. The FROM instruction specifies the Parent Image from which you are building.
26. FROM may only be preceded by one or more ARG instructions, which declare arguments that are used in FROM lines in the Dockerfile.
27. Parser directives do not add layers to the build, and will not be shown as a build step. Parser directives are written as a special type of comment in the form # directive=value.
     A single directive may only be used once.
28. Parser directives are not case-sensitive. However, convention is for them to be lowercase.
29.  The parser directives are supported are: syntax and esape.
30. Custom Dockerfile implementation allows you to:
        Automatically get bugfixes without updating the daemon
        Make sure all users are using the same implementation to build your Dockerfile
	Use the latest features without updating the daemon
	Try out new experimental or third-party features
31. The escape directive sets the character used to escape characters in a Dockerfile. If not specified, the default escape character is \.
32. The escape character is used both to escape characters in a line, and to escape a newline.
33. escaping is not performed in a RUN command, except at the end of a line.
34. Environment variables are notated in the Dockerfile either with $variable_name or ${variable_name}.
35. ${variable:-word} indicates that if variable is set then the result will be that value. If variable is not set then word will be the result.
36. ${variable:+word} indicates that if variable is set then word will be the result, otherwise the result is the empty string.
37. ENV abc=hello
    ENV abc=bye def=$abc
    ENV ghi=$abc
     will result in def having a value of hello, not bye. However, ghi will have a value of bye because it is not part of the same instruction that set abc to bye.   
38. docker CLI sends the context to the docker daemon.
39. The FROM instruction initializes a new build stage and sets the Base Image for subsequent instructions. As such, a valid Dockerfile must start with a FROM instruction.
40. ARG is the only instruction that may precede FROM in the Dockerfile
41. FROM can appear multiple times within a single Dockerfile to create multiple images or use one build stage as a dependency for another. 
42. Each FROM instruction clears any state created by previous instructions.
43. Optionally a name can be given to a new build stage by adding AS name to the FROM instruction. COPY --from=<name|index>
44. FROM instructions support variables that are declared by any ARG instructions that occur before the first FROM.
45. An ARG declared before a FROM is outside of a build stage, so it can’t be used in any instruction after a FROM.
46. RUN has 2 forms: 
     RUN <command> (shell form, the command is run in a shell, which by default is /bin/sh -c on Linux or cmd /S /C on Windows)
     RUN ["executable", "param1", "param2"] (exec form)
47. The RUN instruction will execute any commands in a new layer on top of the current image and commit the results. The resulting committed image will be used for the next step in the Dockerfile.
48. The exec form makes it possible to avoid shell string munging, and to RUN commands using a base image that does not contain the specified shell executable.
49. The default shell for the shell form can be changed using the SHELL command.
50. To use a different shell, other than ‘/bin/sh’, use the exec form passing in the desired shell. For example: RUN ["/bin/bash", "-c", "echo hello"]
51. The exec form is parsed as a JSON array, which means that you must use double-quotes (“) around words not single-quotes (‘).
52. The cache for RUN instructions can be invalidated by using the --no-cache flag.
53. The cache for RUN instructions can be invalidated by ADD and COPY instructions.
54. The CMD instruction has three forms:
	CMD ["executable","param1","param2"] (exec form, this is the preferred form)
	CMD ["param1","param2"] (as default parameters to ENTRYPOINT)
	CMD command param1 param2 (shell form)
55. There can only be one CMD instruction in a Dockerfile. If you list more than one CMD then only the last CMD will take effect.
56. The main purpose of a CMD is to provide defaults for an executing container. 
57. If CMD is used to provide default arguments for the ENTRYPOINT instruction, both the CMD and ENTRYPOINT instructions should be specified with the JSON array format.
58. CMD [ "echo", "$HOME" ] will not do variable substitution on $HOME. If you want shell processing then either use the shell form or execute a shell directly, for example: CMD [ "sh", "-c", "echo $HOME" ].
59. If you would like your container to run the same executable every time, then you should consider using ENTRYPOINT in combination with CMD. 
60. RUN actually runs a command and commits the result; CMD does not execute anything at build time, but specifies the intended command for the image.
61. The LABEL instruction adds metadata to an image.A LABEL is a key-value pair.
62. An image can have more than one label. You can specify multiple labels on a single line.
63. To view an image’s labels, use the docker image inspect command. You can use the --format option to show just the labels; docker image inspect --format='' myimage
64. The MAINTAINER instruction sets the Author field of the generated images.
65. The EXPOSE instruction informs Docker that the container listens on the specified network ports at runtime. 
66. The EXPOSE instruction does not actually publish the port.
67. To actually publish the port when running the container, use the -p flag on docker run to publish 
    and map one or more ports, or the -P flag to publish all exposed ports and map them to high-order ports.
68. By default, EXPOSE assumes TCP. You can also specify UDP: EXPOSE 80/udp.
69. The ENV instruction sets the environment variable <key> to the value <value>. 
70. The ENV instruction has two forms.
     The first form, ENV <key> <value>, will set a single variable to a value.
     The second form, ENV <key>=<value> ..., allows for multiple variables to be set at one time
71. You can view the valuesof ENV using docker inspect, and change them using docker run --env <key>=<value>.
72. ADD has two forms:
        ADD [--chown=<user>:<group>] <src>... <dest>
        ADD [--chown=<user>:<group>] ["<src>",... "<dest>"]
73. The ADD instruction copies new files, directories or remote file URLs from <src> and adds them to the filesystem of the image at the path <dest>.
74. COPY has two forms:
           COPY [--chown=<user>:<group>] <src>... <dest>
	   COPY [--chown=<user>:<group>] ["<src>",... "<dest>"]
75. The COPY instruction copies new files or directories from <src> and adds them to the filesystem of the container at the path <dest>.
76. If you build using STDIN (docker build - < somefile), there is no build context, so COPY can’t be used.
77. If you build by passing a Dockerfile through STDIN (docker build - < somefile), there is no build context, so the Dockerfile can only contain a URL based ADD instruction.
78. Optionally COPY accepts a flag --from=<name|index> that can be used to set the source location to a previous build stage (created with FROM .. AS <name>) that will be used instead of a build context sent by the user.
79. ENTRYPOINT has two forms:
          ENTRYPOINT ["executable", "param1", "param2"] The exec form, which is the preferred form
          ENTRYPOINT command param1 param2 The shell form:


80. An ENTRYPOINT allows you to configure a container that will run as an executable.
81. Only the last ENTRYPOINT instruction in the Dockerfile will have an effect.
82. Dockerfile should specify at least one of CMD or ENTRYPOINT commands.
83. ENTRYPOINT should be defined when using the container as an executable.
84. CMD should be used as a way of defining default arguments for an ENTRYPOINT command or for executing an ad-hoc command in a container.
85. CMD will be overridden when running the container with alternative arguments.
86. If CMD is defined from the base image, setting ENTRYPOINT will reset CMD to an empty value. In this scenario, CMD must be defined in the current image to have a value.
87. The VOLUME instruction creates a mount point with the specified name and marks it as holding externally mounted volumes from native host or other containers.
88. The value can be a JSON array, VOLUME ["/var/log/"], or a plain string with multiple arguments, such as VOLUME /var/log or VOLUME /var/log /var/db.
89. The USER instruction sets the user name (or UID) and optionally the user group (or GID) to use when running the image and for any RUN, CMD and ENTRYPOINT instructions that follow it in the Dockerfile.
90. The WORKDIR instruction sets the working directory for any RUN, CMD, ENTRYPOINT, COPY and ADD instructions that follow it in the Dockerfile.
91. The WORKDIR instruction can be used multiple times in a Dockerfile.
92. WORKDIR /a
    WORKDIR b
    WORKDIR c
    RUN pwd
           The output of the final pwd command in this Dockerfile would be /a/b/c.
93. The ARG instruction defines a variable that users can pass at build-time to the builder with the docker build command using the --build-arg <varname>=<value> flag.
94.  If a user specifies a build argument that was not defined in the Dockerfile, the build outputs a warning.
        [Warning] One or more build-args [foo] were not consumed.
95. A Dockerfile may include one or more ARG instructions. 
96. The ONBUILD instruction adds to the image a trigger instruction to be executed at a later time, when the image is used as the base for another build. 
97. The trigger will be executed in the context of the downstream build, as if it had been inserted immediately after the FROM instruction in the downstream Dockerfile.
98. Any build instruction can be registered as a trigger.
99. A list of all triggers is stored in the image manifest, under the key OnBuild. They can be inspected with the docker inspect command.
100. The STOPSIGNAL instruction sets the system call signal that will be sent to the container to exit : STOPSIGNAL signal
101. The HEALTHCHECK instruction has two forms:
         HEALTHCHECK [OPTIONS] CMD command (check container health by running a command inside the container)
         HEALTHCHECK NONE (disable any healthcheck inherited from the base image)
102. The HEALTHCHECK instruction tells Docker how to test a container to check that it is still working. 
103. When a container has a healthcheck specified, it has a health status in addition to its normal status. This status is initially starting. Whenever a health check passes, it becomes healthy (whatever state it was previously in). 
      After a certain number of consecutive failures, it becomes unhealthy.
104. The health check will first run interval seconds after the container is started, and then again interval seconds after each previous check completes.
105. If a single run of the check takes longer than timeout seconds then the check is considered to have failed.
106. It takes retries consecutive failures of the health check for the container to be considered unhealthy.
107. start period provides initialization time for containers that need time to bootstrap. 
108. 0: success - the container is healthy and ready for use. 1: unhealthy - the container is not working correctly. 2: reserved - do not use this exit code
109. Each layer is only a set of differences from the layer before it.
110. When you create a new container, you add a new writable layer on top of the underlying layers. This layer is often called the “container layer”. 
111. The major difference between a container and an image is the top writable layer.
112. When the container is deleted, the writable layer is also deleted. The underlying image remains unchanged.Because each container has its own writable container layer, and all changes are stored in this container layer.
113.  If you need multiple images to have shared access to the exact same data, store this data in a Docker volume and mount it into your containers.
114. To view the approximate size of a running container, you can use the docker ps -s command. 
115. size: the amount of data (on disk) that is used for the writable layer of each container.
116. virtual size: the amount of data used for the read-only image data used by the container plus the container’s writable layer size.
117. Following additional ways a container can take up disk space:

	Disk space used for log files if you use the json-file logging driver. This can be non-trivial if your container generates a large amount of logging data and log rotation is not configured.
	Volumes and bind mounts used by the container.
	Disk space used for the container’s configuration files, which are typically small.
	Memory written to disk (if swapping is enabled).
	Checkpoints, if you’re using the experimental checkpoint/restore feature. 
118. Copy-on-write is a strategy of sharing and copying files for maximum efficiency.
119. layers is stored in its own directory inside the Docker host’s local storage area. To examine the layers on the filesystem, list the contents of /var/lib/docker/<storage-driver>
120. The copy-on-write operation follows this rough sequence:
      Search through the image layers for the file to update. The process starts at the newest layer and works down to the base layer one layer at a time. When results are found, they are added to a cache to speed future operations.
    
      Perform a copy_up operation on the first copy of the file that is found, to copy the file to the container’s writable layer.
      Any modifications are made to this copy of the file, and the container cannot see the read-only copy of the file that exists in the lower layer.

121. Display layers of a Docker image: ocker image history [OPTIONS] IMAGE
122. docker image import: Import the contents from a tarball to create a filesystem image
123. Docker builds images automatically by reading the instructions from a Dockerfile.
124. When you issue a docker build command, the current working directory is called the build context.By default, the Dockerfile is assumed to be located here, but you can specify a different location with the file flag (-f).
125. Docker has the ability to build images by piping Dockerfile through stdin with a local or remote build context. 
126. Piping a Dockerfile through stdin can be useful to perform one-off builds without writing a Dockerfile to disk, or in situations where the Dockerfile is generated, and should not persist afterwards.
127. If you want to improve the build-speed by excluding some files from the build- context, refer to exclude with .dockerignore.
128. Multi-stage builds allow you to drastically reduce the size of your final image, without struggling to reduce the number of intermediate layers and files.
129. Only the instructions RUN, COPY, ADD create layers.   multi-stage builds,  only copy the artifacts you need into the final image.
130. If you do not want to use the cache at all, you can use the --no-cache=true option on the docker build command. 
131. Display detailed information on one or more images: docker image inspect [OPTIONS] IMAGE [IMAGE...]
132. Images that use the v2 or later format have a content-addressable identifier called a digest.
133. Can pull using a digest value. You can also reference by digest in create, run, and rmi commands, as well as the FROM image reference in a Dockerfile.
134. docker images --filter "dangling=true": shows untagged images.
135. docker rmi $(docker images -f "dangling=true" -q): remove untagged images. 
136. Log in to a Docker registry: docker login [OPTIONS] [SERVER]
137. docker login localhost:8080
138. You need to specify the credentials store in $HOME/.docker/config.json  to tell the docker engine to use it.
139. Export a container’s filesystem as a tar archive: docker export [OPTIONS] CONTAINER
140. Import the contents from a tarball to create a filesystem image: docker image import [OPTIONS] file|URL|- [REPOSITORY[:TAG]]
141. Pull from a different registry: $ docker pull myregistry.local:5000/testing/test-image
142. Push an image or a repository to a registry: docker image push [OPTIONS] NAME[:TAG]
143. you can delete an image with docker rmi or docker image rm.
144. List images: docker image ls [OPTIONS] [REPOSITORY[:TAG]]
145. Remove unused images: docker image prune [OPTIONS]
146. Search the Docker Hub for images: docker search [OPTIONS] TERM
147. Images are stored in collections, known as a repository, which is keyed by a name, as seen throughout the API specification.
148. The content-trust flag is based around a mode variable instructing the engine whether to enforce signed images, and a trust-pinning variable instructing the engine which sources to trust.

Mode can take three variables:

Disabled - Verification is not active and the remainder of the content-trust related metadata will be ignored. This is the default value if mode is not specified.
Permissive - Verification will be performed, but only failures will be logged and remain unenforced. This configuration is intended for testing of changes related to content-trust. The results of the signature verification is displayed in the Docker Engine’s daemon logs.
Enforced - Content trust will be enforced and an image that cannot be verified successfully will not be pulled or run.
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
INSTALLATION AND CONFIGURATION.
Complete configuration of backups for UCP and DTR
Consistently repeat steps to deploy Docker engine UCP and DTR on AWS and on premises in an HA config
Outline the sizing requirements prior to installation


1. How to configure Docker to start on boot?: sudo systemctl enable docker
2. Docker includes multiple logging mechanisms to help you get information from running containers and services. These mechanisms are called logging drivers.
3. The default logging driver is json-file.
4. set the value of log-driver to the name of the logging driver in the daemon.json file, which is located in /etc/docker/ on Linux hosts or C:\ProgramData\docker\config\ on Windows server hosts. 
5. If the logging driver has configurable options, you can set them in the daemon.json file as a JSON object with the key log-opts. 
        {
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "10m",
    "max-file": "3",
    "labels": "production_status",
    "env": "os,customer"
  }
}
6. To find the current default logging driver for the Docker daemon, run docker info and search for Logging Driver.
7. To find the current logging driver for a running container, if the daemon is using the json-file logging driver, run the following docker inspect command.
8. Docker provides two modes for delivering messages from the container to the log driver: 
    (default) direct,  blocking delivery from container to driver.
    non-blocking delivery that stores log messages in an intermediate per-container ring buffer for consumption by driver.
9. The non-blocking message delivery mode prevents applications from blocking due to logging back pressure.
10. The mode log option controls whether to use the blocking (default) or non-blocking message delivery.
11. The max-buffer-size log option controls the size of the ring buffer used for intermediate message storage when mode is set to non-blocking. max-buffer-size defaults to 1 megabyte.
        $ docker run -it --log-opt mode=non-blocking --log-opt max-buffer-size=4m alpine ping 127.0.0.1
12. Supported logging drivers: none, local, json-file, syslog, journald, gelf, fluentd, awslogs, splunk, etwlogs, gcplogs, logentries.
13. When using Docker Community Engine, the docker logs command is only available on the following drivers:local, json-file, journald.
14. Reading log information requires decompressing rotated log files, which causes a temporary increase in disk usage.
15. On a typical installation the Docker daemon is started by a system utility, not manually by a user. This makes it easier to automatically start Docker when the machine reboots.
16. There are two ways to configure the Docker daemon:
      Use a JSON configuration file. This is the preferred option, since it keeps all configurations in a single place.
      Use flags when starting dockerd.
17. To configure the Docker daemon using a JSON file, create a file at /etc/docker/daemon.json on Linux systems, or C:\ProgramData\docker\config\daemon.json
18. The Docker daemon can listen for Docker Engine API requests via three different types of Socket: unix, tcp, and fd. 
19. By default, a unix domain socket (or IPC socket) is created at /var/run/docker.sock
20. On Linux, the Docker daemon has support for several different image layer storage drivers: aufs, devicemapper, btrfs, zfs, overlay and overlay2.
21. The aufs driver is the oldest, but is based on a Linux kernel patch-set that is unlikely to be merged into the main kernel.aufs allows containers to share executable and shared library memory, 
      so is a useful choice when running thousands of containers with the same program or libraries.
22. The devicemapper driver uses thin provisioning and Copy on Write (CoW) snapshots. For each devicemapper graph location – typically /var/lib/docker/devicemapper.
23. If using a block device for device mapper storage, it is best to use lvm to create and manage the thin-pool volume.
24. The highlights of the lvm-based thin-pool management feature include:
      automatic or interactive thin-pool resize support, dynamically changing thin-pool features, automatic thinp metadata checking when lvm activates the thin-pool, etc.
25. You can configure the Docker daemon to use a different directory, using the data-root configuration option.
26. If the daemon is completely non-responsive, you can also force a full stack trace of all threads to be added to the daemon log by sending the SIGUSR signal to the Docker daemon.
27. unable to configure the Docker daemon with file /etc/docker/daemon.json:
           the following directives are specified both as a flag and in the configuration
           file: hosts: (from flag: [unix:///var/run/docker.sock], from file: [tcp://127.0.0.1:2376])
    If you see an error similar to this one and you are starting the daemon manually with flags, you may need to adjust your flags or the daemon.json to remove the conflict.
28. On Debian and Ubuntu systems using systemd, this means that a host flag -H is always used when starting dockerd. If you specify a hosts entry in the daemon.json, this causes a configuration conflict (as in the above message) and Docker fails to start.
    To work around this problem, create a new file /etc/systemd/system/docker.service.d/docker.conf with the following contents, to remove the -H argument that is used when starting the daemon by default.
              [Service]
	      ExecStart=
              ExecStart=/usr/bin/dockerd
29. There are two ways to enable debugging. The recommended approach is to set the debug key to true in the daemon.json file. This method works for every Docker platform.
30. Send a HUP signal to the daemon to cause it to reload its configuration. On Linux hosts, use the following command.
         $ sudo kill -SIGHUP $(pidof dockerd)
31. If the daemon is unresponsive, you can force a full stack trace to be logged by sending a SIGUSR1 signal to the daemon.
             $ sudo kill -SIGUSR1 $(pidof dockerd)
32. The operating-system independent way to check whether Docker is running is to ask Docker, using the docker info command.
33. ou can also use operating system utilities, such as sudo systemctl is-active docker or sudo status docker or sudo service docker status.
34. Finally, you can check in the process list for the dockerd process, using commands like ps or top.
35. Docker Enterprise include the following components:
     Docker Engine - Enterprise. the commercially supported Docker container runtime
     Universal Control Plane (UCP), the web-based, unified cluster and application management solution
     Docker Kubernetes Service (DKS), a certified Kubernetes distribution with 'sensible secure defaults' out-of-the-box.
     Docker Trusted Registry (DTR), a resilient and secure image management repository
     Docker Desktop Enterprise (DDE), an enterprise friendly, supported version of the popular Docker Desktop application with an extended feature set.
     
36. Docker Engine - Enterprise is responsible for container-level operations, interaction with the OS, providing the Docker API, and running the Swarm cluster. 
37. Universal Control Plane -Enterprise by providing an integrated application management platform. It is both the main interaction point for users and the integration point for applications.
38. Docker Trusted Registry - the registry to store and distribute images, an image signing service, a Web UI, an API, and data stores for image metadata and DTR state.
39. Docker provides the ability to package and run an application in a loosely isolated environment called a container.
40. Docker Engine is a client-server application with these major components:
	A server which is a type of long-running program called a daemon process (the dockerd command).
        A REST API which specifies interfaces that programs can use to talk to the daemon and instruct it what to do.
	A command line interface (CLI) client (the docker command).
	The daemon creates and manages Docker objects, such as images, containers, networks, and volumes.
50. Docker uses a client-server architecture. The Docker client talks to the Docker daemon, which does the heavy lifting of building, running, and distributing your Docker containers.        
51. The Docker client and daemon can run on the same system, or you can connect a Docker client to a remote Docker daemon.
52. The Docker client and daemon communicate using a REST API, over UNIX sockets or a network interface. The Docker daemon (dockerd) listens for Docker API requests and manages Docker objects such as images, containers, networks, and volumes.
53. An image is a read-only template with instructions for creating a Docker container.
54. A container is a runnable instance of an image. ou can create, start, stop, move, or delete a container using the Docker API or CLI.
55. By default, containers can connect to external networks using the host machine’s network connection.
56. Each member of a swarm is a Docker daemon, and all the daemons communicate using the Docker API. 
57. Docker uses a technology called namespaces to provide the isolated workspace called the container. 
58. Docker Engine uses namespaces such as the following on Linux:
     The pid namespace: Process isolation (PID: Process ID).
     The net namespace: Managing network interfaces (NET: Networking).
     The ipc namespace: Managing access to IPC resources (IPC: InterProcess Communication).
     The mnt namespace: Managing filesystem mount points (MNT: Mount).
     The uts namespace: Isolating kernel and version identifiers. (UTS: Unix Timesharing System).
59. Docker Engine on Linux also relies on another technology called control groups (cgroups). A cgroup limits an application to a specific set of resources. 
     Control groups allow Docker Engine to share available hardware resources to containers and optionally enforce limits and constraints.
60. Union file systems, or UnionFS, are file systems that operate by creating layers, making them very lightweight and fast.
61. Docker Engine combines the namespaces, control groups, and UnionFS into a wrapper called a container format. The default container format is libcontainer.
62. By default, Docker runs through a non-networked UNIX socket. It can also optionally communicate using an HTTP socket.
63. If you need Docker to be reachable through the network in a safe manner, you can enable TLS by specifying the tlsverify flag and pointing Docker’s tlscacert flag to a trusted CA certificate.
64. If you want to secure your Docker client connections by default, you can move the files to the .docker directory in your home directory --- and set the DOCKER_HOST and DOCKER_TLS_VERIFY variables as well
65. A custom certificate is configured by creating a directory under /etc/docker/certs.d
66. Create the client certificates: Use OpenSSL’s genrsa and req commands to first generate an RSA key and then use the key to create the certificate.
       $ openssl genrsa -out client.key 4096
       $ openssl req -new -x509 -text -key client.key -out client.cert
67. The Docker daemon interprets .crt files as CA certificates and .cert files as client certificates. 
         If a CA certificate is accidentally given the extension .cert instead of the correct .crt extension, the Docker daemon logs the following error message:
        
           Missing key KEY_NAME for client certificate CERT_NAME. CA certificates should use the extension .crt.
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
NETWORKING:
Use a load balancer Docker EE


1. To set the DNS server for all Docker containers, use: $ sudo dockerd --dns 8.8.8.8
2. To set the DNS search domain for all Docker containers, use: sudo dockerd --dns-search example.com
3. Some options can be reconfigured when the daemon is running without requiring to restart the process. We use the SIGHUP signal in Linux to reload.
4. live-restore: Enables keeping containers alive during daemon downtime.
5. By default, Docker Engine connects container via a default bridge to the network of the node.
6. In terms of Docker, a bridge network uses a software bridge which allows containers connected to the same bridge network to communicate, while providing isolation from containers which are not connected to that bridge network. 
7. Bridge networks apply to containers running on the same Docker daemon host. For communication among containers running on different Docker daemon hosts, you can either manage routing at the OS level, or you can use an overlay network.
8. User-defined bridges provide automatic DNS resolution between containers.
9. Containers on the default bridge network can only access each other by IP addresses, unless you use the --link option, which is considered legacy. 
    On a user-defined bridge network, containers can resolve each other by name or alias.
10. User-defined bridges provide better isolation.
11. Using a user-defined network provides a scoped network in which only containers attached to that network are able to communicate.
12. Containers can be attached and detached from user-defined networks on the fly.
13. To remove a container from the default bridge network, you need to stop the container and recreate it with different network options.
14. Each user-defined network creates a configurable bridge.
15. If your containers use the default bridge network, you can configure it, but all the containers use the same settings, such as MTU and iptables rules. In addition, configuring the default bridge network happens outside of Docker itself, and requires a restart of Docker.
16. User-defined bridge networks are created and configured using docker network create. If different groups of applications have different network requirements, you can configure each user-defined bridge separately, as you create it.
17. Linked containers on the default bridge network share environment variables.
18. Originally, the only way to share environment variables between two containers was to link them using the --link flag. This type of variable sharing is not possible with user-defined networks.
19. There are superior ways to share environment variables. A few ideas:
    Multiple containers can mount a file or directory containing the shared information, using a Docker volume.
    Multiple containers can be started together using docker-compose and the compose file can define the shared variables.
    You can use swarm services instead of standalone containers, and take advantage of shared secrets and configs.
20. Containers connected to the same user-defined bridge network effectively expose all ports to each other. For a port to be accessible to containers or non-Docker hosts on different networks, that port must be published using the -p or --publish flag.
21. Use the docker network create command to create a user-defined bridge network: $ docker network create my-net
22. Use the docker network rm command to remove a user-defined bridge network: $ docker network rm my-net.
23. When you create a new container, you can specify one or more --network flags. 
24. To connect a running container to an existing user-defined bridge, use the docker network connect command: $ docker network connect my-net my-nginx
25. To disconnect a running container from a user-defined bridge, use the docker network disconnect command. docker network disconnect my-net my-nginx
26. You can specify the subnet, the IP address range, the gateway, and other options. See the docker network create reference or the output of docker network create --help for details.
27. By default, traffic from containers connected to the default bridge network is not forwarded to the outside world. To enable forwarding, you need to change two settings. 
         These are not Docker commands and they affect the Docker host’s kernel. 
                1. Configure the Linux kernel to allow IP forwarding. $ sysctl net.ipv4.conf.all.forwarding=1
                2. Change the policy for the iptables FORWARD policy from DROP to ACCEPT. $ sudo iptables -P FORWARD ACCEPT
28. The default bridge network is considered a legacy detail of Docker and is not recommended for production use. Configuring it is a manual operation, and it has technical shortcomings.
29. Containers connected to the default bridge network can communicate, but only by IP address, unless they are linked using the legacy --link flag.
30. To configure the default bridge network, you specify options in daemon.json. Here is an example daemon.json with several options specified. 
       {
  "bip": "192.168.1.5/24",
  "fixed-cidr": "192.168.1.5/25",
  "fixed-cidr-v6": "2001:db8::/64",
  "mtu": 1500,
  "default-gateway": "10.20.1.1",
  "default-gateway-v6": "2001:db8:abcd::89",
  "dns": ["10.20.1.2","10.20.1.3"]
}


31. The DRIVER accepts bridge or overlay which are the built-in network drivers. 
     If you don’t specify the --driver option, the command automatically creates a bridge network for you: $ docker network create -d bridge my-bridge-network

32. Bridge networks are isolated networks on a single Engine installation. If you want to create a network that spans multiple Docker hosts each running an Engine, you must create an overlay network.
33. overlay networks require some pre-existing conditions before you can create one. These conditions are:
	Access to a key-value store. Engine supports Consul, Etcd, and ZooKeeper (Distributed store) key-value stores.
        A cluster of hosts with connectivity to the key-value store.
	A properly configured Engine daemon on each host in the cluster.
34. The dockerd options that support the overlay network are:
	--cluster-store
	--cluster-store-opt
	--cluster-advertise
35. It is a good idea to install Docker Swarm to manage the cluster that makes up your network. Swarm provides sophisticated discovery and server management tools that can assist your implementation.
36. Command to create overlay network: $ docker network create -d overlay my-multihost-network
37. You should create overlay networks with /24 blocks (the default), which limits you to 256 IP addresses, when you create networks using the default VIP-based endpoint-mode. 
38. If you need more than 256 IP addresses, do not increase the IP block size. You can either use dnsrr endpoint mode with an external load balancer, or use multiple smaller overlay networks. 
39. When you start a container, use the --network flag to connect it to a network. This example adds the busybox container to the mynet network:
        $ docker run -itd --network=mynet busybox
40. When you create a network, Engine creates a non-overlapping subnetwork for the network by default. This subnetwork is not a subdivision of an existing network. It is purely for ip-addressing purposes. 
     You can override this default and specify subnetwork values directly using the --subnet option. On a bridge network you can only create a single subnet:
     $ docker network create --driver=bridge --subnet=192.168.0.0/16 br0
41. By default, when you connect a container to an overlay network, Docker also connects a bridge network to it to provide external connectivity. If you want to create an externally isolated overlay network, you can specify the --internal option.
42. You can create the network which will be used to provide the routing-mesh in the swarm cluster. You do so by specifying --ingress when creating the network. Only one ingress network can be created at the time.
43. docker network prune: Remove all unused networks
44. The Docker networking architecture is built on a set of interfaces called the Container Networking Model (CNM).
45. Components of Container Networking Model (CNM): Sandbox, Endpoint, Network.
46. Sandbox:  contains the configuration of a container's network stack. This includes the management of the container's interfaces, routing table, and DNS settings.
47. Endpoint — An Endpoint joins a Sandbox to a Network. 
48. Network - A Network is a collection of endpoints that have connectivity between them. Endpoints that are not connected to a network do not have connectivity on a network.
49. There are two broad types of CNM network drivers:
	1. Native Network Drivers: 
	2. Remote Network Drivers 
50. Native network drivers: Host, Bridge, Overlay, MACVLAN, None.
51. Host- With the host driver, a container uses the networking stack of the host. There is no namespace separation, and all interfaces on the host can be used directly by the container.
52. Bridge- The bridge driver creates a Linux bridge on the host that is managed by Docker. By default containers on a bridge can communicate with each other. External access to containers can also be configured through the bridge driver.
53. Overlay- The overlay driver creates an overlay network that supports multi-host networks out of the box. It uses a combination of local Linux bridges and VXLAN to overlay container-to-container communications over physical network infrastructure.
54. MACVLAN- The macvlan driver uses the Linux MACVLAN bridge mode to establish a connection between container interfaces and a parent host interface (or sub-interfaces). It can be used to provide IP addresses to containers that are routable on the physical network. Additionally VLANs can be trunked to the macvlan driver to enforce Layer 2 container segmentation.
55. None- The none driver gives a container its own networking stack and network namespace but does not configure interfaces inside the container. Without additional configuration, the container is completely isolated from the host networking stack.
56. Docker Remote Network Drivers: contiv, weave, kuryr
57. Docker Remote IPAM Drivers: infoblox
58. There are several Linux networking building blocks which Docker uses to implement its native CNM network drivers. This list includes Linux bridges, network namespaces, veth pairs, and iptables. 
59. A Linux bridge is a Layer 2 device that is the virtual implementation of a physical switch inside the Linux kernel. It forwards traffic based on MAC addresses which it learns dynamically by inspecting traffic.
60. A Linux network namespace is an isolated network stack in the kernel with its own interfaces, routes, and firewall rules. It is a security aspect of containers and Linux, used to isolate containers.
61. Network namespaces ensure that two containers on the same host aren't able to communicate with each other or even the host itself unless configured to do so via Docker networks.
     The host network namespace contains the host interfaces and host routing table.
62. A virtual ethernet device or veth is a Linux networking interface that acts as a connecting wire between two network namespaces.A veth is a full duplex link that has a single interface in each namespace.
63. Bridge: The default network driver. If you don’t specify a driver, this is the type of network you are creating. Bridge networks are usually used when your applications run in standalone containers that need to communicate. 
64. Host: For standalone containers, remove network isolation between the container and the Docker host, and use the host’s networking directly.
     If you use the host network mode for a container, that container’s network stack is not isolated from the Docker host (the container shares the host’s networking namespace), and the container does not get its own IP-address allocated. 
65. Host mode networking can be useful to optimize performance, and in situations where a container needs to handle a large range of ports, as it does not require network address translation (NAT), and no “userland-proxy” is created for each port.
66. You can also use a host network for a swarm service, by passing --network host to the docker service create command.
67. overlay: Overlay networks connect multiple Docker daemons together and enable swarm services to communicate with each other.
68. You need the following ports open to traffic to and from each Docker host participating on an overlay network:
        TCP port 2377 for cluster management communications
        TCP and UDP port 7946 for communication among nodes
	UDP port 4789 for overlay network traffic
69. To create an overlay network for use with swarm services, use a command like the following:$ docker network create -d overlay my-overlay
70. To create an overlay network which can be used by swarm services or standalone containers to communicate with other standalone containers running on other Docker daemons, add the --attachable flag:
       $ docker network create -d overlay --attachable my-attachable-overlay
71. All swarm service management traffic is encrypted by default using the AES algorithm in GCM mode.
72. To encrypt application data as well, add --opt encrypted when creating the overlay network. 
73. Overlay network encryption is not supported on Windows. If a Windows node attempts to connect to an encrypted overlay network, no error is detected but the node cannot communicate.
74. You can use the overlay network feature with both --opt encrypted --attachable and attach unmanaged containers to that network:
     $ docker network create --opt encrypted --driver overlay --attachable my-attachable-multi-host-network
75. Customizing the ingress network involves removing and recreating it. This is usually done before you create any services in the swarm. If you have existing services which publish ports, those services need to be removed before you can remove the ingress network.
76. The docker_gwbridge is a virtual bridge that connects the overlay networks (including the ingress network) to an individual Docker daemon’s physical network. 
     Docker creates it automatically when you initialize a swarm or join a Docker host to a swarm, but it is not a Docker device. 
77. Swarm services connected to the same overlay network effectively expose all ports to each other.
78. For a port to be accessible outside of the service, that port must be published using the -p or --publish flag on docker service create or docker service update
79. -p 8080:80 or -p published=8080,target=80: Map TCP port 80 on the service to port 8080 on the routing mesh.
80. -p 8080:80/udp or -p published=8080,target=80,protocol=udp: Map UDP port 80 on the service to port 8080 on the routing mesh.
81. -p 8080:80/tcp -p 8080:80/udp or -p published=8080,target=80,protocol=tcp -p published=8080,target=80,protocol=udp:
          Map TCP port 80 on the service to TCP port 8080 on the routing mesh, and map UDP port 80 on the service to UDP port 8080 on the routing mesh.
82. By default, swarm services which publish ports do so using the routing mesh. Services using the routing mesh are running in virtual IP (VIP) mode. 
83. When using the routing mesh, there is no guarantee about which Docker node services client requests.
84. To bypass the routing mesh, you can start a service using DNS Round Robin (DNSRR) mode, by setting the --endpoint-mode flag to dnsrr.
85. By default, control traffic relating to swarm management and traffic to and from your applications runs over the same network, though the swarm control traffic is encrypted.
86. You can configure Docker to use separate network interfaces for handling the two different types of traffic. When you initialize or join the swarm, specify --advertise-addr and --datapath-addr separately. You must do this for each node joining the swarm.
87. macvlan: Macvlan networks allow you to assign a MAC address to a container, making it appear as a physical device on your network. The Docker daemon routes traffic to containers by their MAC addresses.
88. none: For this container, disable all networking. Usually used in conjunction with a custom network driver. none is not available for swarm services. 
89. User-defined bridge networks are best when you need multiple containers to communicate on the same Docker host.
90. Host networks are best when the network stack should not be isolated from the Docker host, but you want other aspects of the container to be isolated.
91. Overlay networks are best when you need containers running on different Docker hosts to communicate, or when multiple applications work together using swarm services.
92. Macvlan networks are best when you are migrating from a VM setup or need your containers to look like physical hosts on your network, each with a unique MAC address.
93. Third-party network plugins allow you to integrate Docker with specialized network stacks.
94. List port mappings or a specific mapping for the container: docker port CONTAINER [PRIVATE_PORT[/PROTO]]
95. Basically, you can look up every port which is exposed by using docker inspect CONTAINER on every container. However, to just get the port binding information, you could use docker port CONTAINER.
96. To troubleshoot problems with container communication, you can use docker network inspect of the different containers to see, whether those containers are connected through the same network.
97. The traffic between DTR and UCP is always encrypted to ensure security. 
98. Traffic between containers is not encrypted by default
99. Docker focuses on three key areas of container security: secure access, secure content, and secure platform. 
100. By default the user inside the container is root. Using a defense in depth model, it is recommended that not all containers run as root. An easy way to mitigate this is to use the --user declaration at run time. 
101. Seccomp (short for Secure Computing Mode) is a security feature of the Linux kernel, used to restrict the syscalls available to a given process.
102. Seccomp profiles are applied at container creation time and cannot be altered for running containers.
103. AppArmor and SELinux are security modules similar to Seccomp in their use of profiles, however they differ in how those profiles are executed. 
      They need to be enabled on the host, while SELinux can be enabled at the daemon level. To enable SELinux in the Docker daemon, modify /etc/docker/daemon.json and add the following:
         {
           "selinux-enabled": true
		}
104. To check if SELinux is enabled:   docker info --format '{{.SecurityOptions}}'
105. Content Trust is the cryptographic guarantee that the image pulled is the correct image. export DOCKER_CONTENT_TRUST=1
106. UCP out of the box delivers with two Certificate Authorities (CA) with Mutual TLS. The two CAs set up by UCP include:
       The first CA is used for ALL internal communication between managers and workers
       The second CA is for the end user communication.
107. Worker nodes are unprivileged, meaning they do not have access to the cluster state or secrets. When adding nodes to the UCP cluster, a join token must be used
108. The client bundle allows end users to securely connect from a Docker Client to UCP via certificates when deploying workloads and administering the environment.
109. Docker Enterprise Access Control is a policy-based model that uses access control lists (ACLs) called grants to dictate access between users and cluster resources. A grant ties together who, has permission for which actions, against what resources.
110. UCP also provides default roles that are pre-created. These are common role types that can be used to ease the burden of creating custom roles.
      None: The user has no access to swarm resources. This maps to the No Access role in UCP 2.1.x.
      View Only	: The user can view resources like services, volumes, and networks but can't create them.
      Restricted Control: 
      Scheduler
      Full Control:
111. Docker Enterprise enables controlling access to swarm resources by using collections. 	                       
112. The routing mesh enables each node in the swarm to accept connections on published ports for any service running in the swarm, even if there’s no task running on the node.
113. To use the ingress network in the swarm, you need to have the following ports open between the swarm nodes before you enable swarm mode:
         Port 7946 TCP/UDP for container network discovery.
         Port 4789 UDP for the container ingress network.
114. Use the --publish flag to publish a port when you create a service. target is used to specify the port inside the container, and published is used to specify the port to bind on the routing mesh.
115. The <PUBLISHED-PORT> is the port where the swarm makes the service available. If you omit it, a random high-numbered port is bound. The <CONTAINER-PORT> is the port where the container listens.
      $ docker service create \
  --name my-web \
  --publish published=8080,target=80 \
  --replicas 2 \
  nginx
116. You can publish a port for an existing service using the following command:
          $ docker service update \
  --publish-add published=<PUBLISHED-PORT>,target=<CONTAINER-PORT> \
  <SERVICE>

117. You can use docker service inspect to view the service’s published port. For instance:
      docker service inspect --format="{{json .Endpoint.Spec.Ports}}" my-web

       [{"Protocol":"tcp","TargetPort":80,"PublishedPort":8080}]

118. By default, when you publish a port, it is a TCP port. dns-cache 
119. You can configure an external load balancer for swarm services, either in combination with the routing mesh or without using the routing mesh at all.
120. To use an external load balancer without the routing mesh, set --endpoint-mode to dnsrr.
121. Docker uses a concept called services to deploy applications. Services consist of containers created from the same image. Each service consists of tasks that execute on worker nodes and define the state of the application.
122. The ucp-interlock-proxy can be scaled up to have more replicas, and those replicas can be constrained to only those nodes that have high performance network interfaces
123. For services to be published using Interlock Proxy, they must contain, among other labels, at least two labels where the keys are com.docker.lb.hosts and com.docker.lb.port.
124. Update the high performance nodes by using node labels to identify them: docker node update --label-add nodetype=loadbalancer <node>
125. Constrain the Interlock Proxy service tasks to only run on the high performance nodes using the node labels. 
     This is done by updating the ucp-interlock service configuration to deploy the interlock proxy service with the updated constraints in the ProxyConstraints array as explained below:
126. The Interlock Proxy polls the Docker API for changes every 3 seconds (default), so once an application is deployed, Interlock Proxy polls for the new service and finds it at http://<domain-to-route>.
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

SECURITY:

Access control model

1. Universal Control Plane (UCP) is the enterprise-grade cluster management solution from Docker. You install it on-premises or in your virtual private cloud, and it helps you manage your Docker cluster and applications through a single interface.
2. With UCP, you can manage from a centralized place all of the computing resources you have available, like nodes, volumes, and networks. also deploy and monitor your applications and services.
3. UCP has its own built-in authentication mechanism and integrates with LDAP services. It also has role-based access control (RBAC), so that you can control who can access and make changes to your cluster and applications.
4. UCP integrates with Docker Trusted Registry (DTR) so that you can keep the Docker images you use for your applications behind your firewall, where they are safe and can’t be tampered with.
5. Docker Enterprise Edition (EE) is the only Containers as a Service (CaaS) Platform for IT that manages and secures diverse applications across disparate infrastructure, both on-premises and in the cloud.
6. A client bundle is a group of certificates downloadable directly from the Docker Universal Control Plane (UCP) user interface within the admin section for “My Profile”.
    A client bundle contains a private and public key pair that authorizes your requests in UCP. 
       It also contains utility scripts you can use to configure your Docker and kubectl client tools to talk to your UCP deployment.
7. Docker Trusted Registry (DTR) is a containerized application that runs on a Docker Universal Control Plane cluster.Once you have DTR deployed, you use your Docker CLI client to login, push, and pull images.
8. DTR internal components:When you install DTR on a node, the following containers are started:
	dtr-api-<replica_id>: Executes the DTR business logic. It serves the DTR web application and API
	dtr-garant-<replica_id>: Manages DTR authentication
	dtr-jobrunner-<replica_id>: Runs cleanup jobs in the background
	dtr-nginx-<replica_id>: Receives http and https requests and proxies them to other DTR components. By default it listens to ports 80 and 443 of the host
	dtr-notary-server-<replica_id>: Receives, validates, and serves content trust metadata, and is consulted when pushing or pulling to DTR with content trust enabled
	dtr-notary-signer-<replica_id>: Performs server-side timestamp and snapshot signing for content trust metadata
	dtr-registry-<replica_id>: implements the functionality for pulling and pushing Docker images. It also handles how images are stored
	dtr-rethinkdb-<replica_id>: A database for persisting repository metadata
	dtr-scanningstore-<replica_id>: Stores security scanning data
9. To allow containers to communicate, when installing DTR the following networks are created:
	Name: dtr-ol
	Type: overlay
	Description: Allows DTR components running on different nodes to communicate, to replicate DTR data
10. DTR uses these named volumes for persisting data:
	dtr-ca-<replica_id>
	dtr-notary-<replica_id>
	dtr-postgres-<replica_id>
	dtr-registry-<replica_id>
	dtr-rethink-<replica_id>
	dtr-nfs-registry-<replica_id>
11. we can customize the volume driver used for these volumes, by creating the volumes before installing DTR. During the installation, DTR checks which volumes don’t exist in the node, and creates them using the default volume driver.
	By default, the data for these volumes can be found at /var/lib/docker/volumes/<volume-name>/_data.
12. By default, Docker Trusted Registry stores images on the filesystem of the node where it is running, but you should configure it to use a centralized storage backend.		
13. DTR supports these storage back ends:
	NFS, Amazon S3, cleversafe, Google cloud storage, openstack swift, Microsoft azure.
14. Docker Enterprise is made up of three related products:
      Docker Engine - Enterprise
      Universal Control Plane (UCP)
      Docker Trusted Registry (DTR)
15. Docker Engine - Enterprise is a client-server application with these major components:
	A server which is a type of long-running program called a daemon process (the dockerd command).
	A REST API which specifies interfaces that programs can use to talk to the daemon and instruct it what to do.
	A command line interface (CLI) client (the docker command).
16. Universal Control Plane (UCP) is the enterprise-grade cluster management solution from Docker. You install it on-premises or in your virtual private cloud, and it helps you manage your Docker cluster and applications through a single interface.
17. If nodes are not already running in a swarm when installing UCP, nodes will be configured to run in swarm mode.
18. When you deploy UCP, it starts running a globally scheduled service called ucp-agent. This service monitors the node where it’s running and starts and stops UCP services, based on whether the node is a manager or a worker node.
19. If the node is a: Manager: the ucp-agent service automatically starts serving
    all UCP components, including the UCP web UI and data stores used by UCP. The ucp-agent accomplishes this by deploying several containers on the node. By promoting a node to manager, UCP automatically becomes highly available and fault tolerant.
20. If the node is a: Worker: on worker nodes, the ucp-agent service starts serving
    a proxy service that ensures only authorized users and other UCP services can run Docker commands in that node. The ucp-agent deploys a subset of containers on worker nodes.

21.The core component of UCP is a globally scheduled service called ucp-agent. 
22. By default, pause containers are hidden, but you can see them by running docker ps -a.
23. There are two ways to interact with UCP: the web UI or the CLI.
24. You can monitor the status of UCP using the web UI or the CLI. You can also use the _ping endpoint to build monitoring automation.
25. The nodes in a swarm use mutual Transport Layer Security (TLS) to authenticate, authorize, and encrypt the communications with other nodes in the swarm.
26. By default, the manager node generates a new root Certificate Authority (CA) along with a key pair, which are used to secure communications with other nodes that join the swarm. 
27. we can specify your own externally-generated root CA, using the --external-ca flag of the docker swarm init command.
28. The manager node also generates two tokens to use when you join additional nodes to the swarm: one worker token and one manager token. Each token includes the digest of the root CA’s certificate and a randomly generated secret.
29. Each time a new node joins the swarm, the manager issues a certificate to the node. 
30. The certificate contains a randomly generated node ID to identify the node under the certificate common name (CN) and the role under the organizational unit (OU).
31. By default, each node in the swarm renews its certificate every three months. You can configure this interval by running the docker swarm update --cert-expiry <TIME PERIOD> command.
32. Run docker swarm ca --rotate to generate a new CA certificate and key.
33. we can pass the --ca-cert and --external-ca flags to specify the root certificate and to use a root CA external to the swarm. 
34. -ca-cert and --ca-key flags to specify the exact certificate and key you would like the swarm to use.
35. When you issue the docker swarm ca --rotate command, the following things happen in sequence:
	1.Docker generates a cross-signed certificate. This means that a version of the new root CA certificate is signed with the old root CA certificate. This cross-signed certificate is used as an intermediate certificate for all new node certificates. 
          This ensures that nodes that still trust the old root CA can still validate a certificate signed by the new CA.
	2.In Docker 17.06 and higher, Docker also tells all nodes to immediately renew their TLS certificates. This process may take several minutes, depending on the number of nodes in the swarm.
	3.After every node in the swarm has a new TLS certificate signed by the new CA, Docker forgets about the old CA certificate and key material, and tells all the nodes to trust the new CA certificate only.
            This also causes a change in the swarm’s join tokens. The previous join tokens are no longer valid.
36. security features you can configure and use within your Docker Engine installation.
	1.You can configure Docker’s trust features so that your users can push and pull trusted images.
	2.You can protect the Docker daemon socket and ensure only trusted Docker client connections.
	3.You can use certificate-based client-server authentication to verify a Docker daemon has the rights to access images on a registry. 
	4.You can configure secure computing mode (Seccomp) policies to secure system calls in a container. 
	5.An AppArmor profile for Docker is installed with the official .deb packages. For information about this profile and overriding it.
	6.You can map the root user in the containers to a non-root user.
	7.You can also run the Docker daemon as a non-root user.
37. There are four major areas to consider when reviewing Docker security:
	the intrinsic security of the kernel and its support for namespaces and cgroups;
	the attack surface of the Docker daemon itself;
	loopholes in the container configuration profile, either by default, or when customized by users.
	the “hardening” security features of the kernel and how they interact with containers.
38. Use this command to create a backup of a UCP manager node.
       docker container run \
    --rm \
    --interactive \
    --name ucp \
    --log-driver none \
    --volume /var/run/docker.sock:/var/run/docker.sock \
    docker/ucp \
    backup [command options] > backup.tar
     This command creates a tar file with the contents of the volumes used by this UCP manager node, and prints it. You can then use the restore command to restore the data from an existing backup.

39. The backup contains private keys and other sensitive information. Use the --passphrase flag to encrypt the backup with PGP-compatible encryption or --no-passphrase to opt out (not recommended).
40. If you are installing UCP on a manager node with SELinunx enabled at the daemon and operating system level, you will need to pass ` –security-opt label=disable` in to your install command. 
    This flag will disable SELinux policies on the installation container. 
41. Use this command to verify the UCP images on this node. This command checks the UCP images that are available in this node, and pulls the ones that are missing.
          docker container run --rm -it \
        --name ucp \
        -v /var/run/docker.sock:/var/run/docker.sock \
        docker/ucp \
        images [command options]
42. Use this command to install UCP on a node. Running this command will initialize a new swarm, turn a node into a manager, and install UCP.
	docker/ucp  install [command options]
43. Use this command to restore a UCP cluster from a backup: docker/ucp restore
44. If the current node does not belong in a cluster, one will be initialized using the value of the --host-address flag
45.  When restoring on an existing swarm-mode cluster, no previous UCP components must be running on any node of the cluster. This cleanup can be performed with the uninstall-ucp command.
46. By default, the backup tar file is read from stdin. You can also bind-mount the backup file under /config/backup.tar, and run the restore command with the --interactive flag.
47. Docker Content Trust Keys set consists of the following classes of keys:
     an offline key that is the root of DCT for an image tag
     repository or tagging keys that sign tags
     server-managed keys such as the timestamp key, which provides freshness security guarantees for your repository
48. Offline key is used create tagging key, offline key belongs to person or orgnization. resides in client-side
49. tagging key is associated with the image repository. can used for push n pull tag images. resides in client-side
50. Timestamp key is associated with an image repos. Tssssshis is created by docker and resides inside server,
51. Within the Docker CLI we can sign and push a container image with the $ docker trust command syntax. This is built on top of the Notary feature set,
52. To sign a Docker Image you will need a delegation key pair. These keys can be generated locally using $ docker trust key generate, generated by a certificate authority.
53. If you are generating delegation keys with $ docker trust key generate, the private key is automatically added to the local trust store. (By default this is stored in ~/.docker/trust/)
54. If you are importing a separate key, such as one from a UCP Client Bundle you will need to use the $ docker trust key load command.
55. we will need to add the delegation public key to the Notary server; this is specific to a particular image repository in Notary known as a Global Unique Name (GUN).
         $ docker trust signer add --key cert.pem jeff dtr.example.com/admin/demo
56. Remote trust data for a tag or a repository can be viewed by the $ docker trust inspect command
57. DCT is controlled by the Docker Engine’s configuration file. By default this is found at /etc/docker/daemon.json
58. The content-trust flag is based around a mode variable instructing the engine whether to enforce signed images, and a trust-pinning variable instructing the engine which sources to trust.
59. Currently, content trust is disabled by default in the Docker Client. To enable it, set the DOCKER_CONTENT_TRUST environment variable to 1
60. The commands that operate with DCT are:
         push
	build
	create
	pull
	run

----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
SECURITY:

1. Docker supports the following storage drivers:
    overlay2, devicemapper, btrfs, zfs, vfs, aufs
2. devicemapper is supported, but requires direct-lvm for production environments, because loopback-lvm, while zero-configuration, has very poor performance.
3. The vfs storage driver is intended for testing purposes, and for situations where no copy-on-write filesystem can be used. Performance of this storage driver is poor, and is not generally recommended for production use.
4. overlay2, aufs, and overlay all operate at the file level rather than the block level. This uses memory more efficiently, but the container’s writable layer may grow quite large in write-heavy workloads
5. Block-level storage drivers such as devicemapper, btrfs, and zfs perform better for write-heavy workloads.
6. btrfs and zfs require a lot of memory.
7. zfs is a good choice for high-density workloads such as PaaS.
8. To see what storage driver Docker is currently using, use docker info and look for the Storage Driver line:
9. Configure loop-lvm mode for testing: The loop-lvm mode makes use of a ‘loopback’ mechanism that allows files on the local disk to be read from and written to as if they were an actual physical disk or block device.
10. setting up loop-lvm mode can help identify basic issues (such as missing user space packages, kernel drivers, etc.) 
11. Stop Docker: $ sudo systemctl stop docker
12. Edit /etc/docker/daemon.json. If it does not yet exist, create it. Assuming that the file was empty, add the following contents for adding devicemapper:
       {
	  "storage-driver": "devicemapper"
	}
13. Start Docker: $ sudo systemctl start docker
14. Production hosts using the devicemapper storage driver must use direct-lvm mode. This mode uses block devices to create the thin pool. 
15. Direct-lvm is faster than using loopback devices, uses system resources more efficiently, and block devices can grow as needed. However, more setup is required than in loop-lvm mode.
16. we can monitor free space on the volume using lvs or lvs -a. Consider using a monitoring tool at the OS level, such as Nagios.
17. To view the LVM logs, you can use journalctl:
	$ sudo journalctl -fu dm-event.service
18. RESIZE A LOOP-LVM THIN POOL: The easiest way to resize a loop-lvm thin pool is to use the device_tool utility but you can use operating system utilities instead.
19. Use the tool. The following example resizes the thin pool to 200GB by manually.
      $ ./device_tool resize 200GB
20. In loop-lvm mode, a loopback device is used to store the data, and another to store the metadata. loop-lvm mode is only supported for testing, because it has significant performance and stability drawbacks.
21. If you are using loop-lvm mode, the output of docker info shows file paths for Data loop file and Metadata loop file:
         $ docker info |grep 'loop file'
22. Increase the size of the data file to 200 G using the truncate command, which is used to increase or decrease the size of a file: manually:loop-lvm:Use operating system utilities
         $ sudo truncate -s 200G /var/lib/docker/devicemapper/data
23. Use the pvdisplay command to find the physical block devices currently in use by your thin pool, and the volume group’s name.
          $ sudo pvdisplay |grep 'VG Name'
24. Activate the devicemapper after reboot: If you reboot the host and find that the docker service failed to start, look for the error, “Non existing device”. 
      You need to re-activate the logical volumes with this command:
        $ sudo lvchange -ay docker/thinpool
25. The /var/lib/docker/devicemapper/metadata/ directory contains metadata about the Devicemapper configuration itself and about each image and container layer that exist.
26. The devicemapper storage driver uses snapshots, and this metadata include information about those snapshots. These files are in JSON format.
27. The /var/lib/docker/devicemapper/mnt/ directory contains a mount point for each image and container layer that exists.
28. Image layer mount points are empty, but a container’s mount point shows the container’s filesystem as it appears from within the container.
29. Another feature of devicemapper is its use of snapshots (also sometimes called thin devices or virtual devices), which store the differences introduced in each layer as very small, lightweight thin pools.
30. Snapshots provide many benefits:
	Layers which are shared in common between containers are only stored on disk once, unless they are writable. For instance, if you have 10 different images which are all based on alpine, the alpine image and all its parent images are only stored once each on disk.
	Snapshots are an implementation of a copy-on-write (CoW) strategy. This means that a given file or directory is only copied to the container’s writable layer when it is modified or deleted by that container.
	Because devicemapper operates at the block level, multiple blocks in a writable layer can be modified simultaneously.
	Snapshots can be backed up using standard OS-level backup utilities. Just make a copy of /var/lib/docker/devicemapper/.
31. Writing a new file: With the devicemapper driver, writing new data to a container is accomplished by an allocate-on-demand operation. Each block of the new file is allocated in the container’s writable layer and the block is written there.
32. Writing and then deleting a file:If a container writes to a file and later deletes the file, all of those operations happen in the container’s writable layer. In that case, if you are using direct-lvm, the blocks are freed. 
     If you use loop-lvm, the blocks may not be freed. This is another reason not to use loop-lvm in production.
33. Volumes are the preferred mechanism for persisting data generated by and used by Docker containers.
34. While bind mounts are dependent on the directory structure of the host machine.
35. Volumes have several advantages over bind mounts:
	Volumes are easier to back up or migrate than bind mounts.
	You can manage volumes using Docker CLI commands or the Docker API.
	Volumes work on both Linux and Windows containers.
	Volumes can be more safely shared among multiple containers.
	Volume drivers let you store volumes on remote hosts or cloud providers, to encrypt the contents of volumes, or to add other functionality.
	New volumes can have their content pre-populated by a container.
36. The -v or --volume flag was used for standalone containers and the --mount flag was used for swarm services. we can also use --mount with standalone containers.
37. The biggest difference is that the -v syntax combines all the options together in one field, while the --mount syntax separates them.
38. -v or --volume: Consists of three fields, separated by colon characters (:).
	the first field is the name of the volume, and is unique on a given host machine.
	The second field is the path where the file or directory are mounted in the container.
	The third field is optional, and is a comma-separated list of options. ro   //read-only.
39. --mount: Consists of multiple key-value pairs, separated by commas and each consisting of a <key>=<value> tuple. 
40. The type of the mount, which can be bind, volume, or tmpfs
41. The source of the mount. For named volumes, this is the name of the volume.
42. The destination takes as its value the path where the file or directory is mounted in the container. When using volumes with services, only --mount is supported.
43. you can create and manage volumes outside the scope of any container.
	$ docker volume create my-vol
44. If you start a container with a volume that does not yet exist, Docker creates the volume for you. 
45. $ docker run -d \
  --name devtest \						/// using mount
  --mount source=myvol2,target=/app \
  nginx:latest

46. $ docker run -d \
  --name devtest \
  -v myvol2:/app \						//using -v
  nginx:latest 

47. Stop the container and remove the volume. Note volume removal is a separate step.
	$ docker container stop devtest
	$ docker container rm devtest
	$ docker volume rm myvol2
48. The following example starts a nginx service with four replicas, each of which uses a local volume called myvol2.
	$ docker service create -d \
	  --replicas=4 \
	  --name devtest-service \
 	 --mount source=myvol2,target=/app \
	  nginx:latest
Use docker service ps devtest-service to verify that the service is running:
  $ docker service ps devtest-service

ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS
4d7oz1j85wwn        devtest-service.1   nginx:latest        moby   		Running              Running 14 seconds ago

49. Remove the service, which stops all its tasks:
	$ docker service rm devtest-service
	Removing the service does not remove any volumes created by the service. Volume removal is a separate step.

50. The docker service create command does not support the -v or --volume flag.
51. When mounting a volume into a service’s containers, you must use the --mount flag.
52. Create a volume using a volume driver:
	$ docker volume create --driver vieux/sshfs \
 	 -o sshcmd=test@node2:/home/test \
 	 -o password=testpassword \
	  sshvolume
53. If the volume driver requires you to pass options, you must use the --mount flag to mount the volume, rather than -v.
54. Volumes are useful for backups, restores, and migrations. Use the --volumes-from flag to create a new container that mounts that volume.
55. A Docker data volume persists after a container is deleted. There are two types of volumes
	Named volumes have a specific source from outside the container, for example awesome:/bar.
	Anonymous volumes have no specific source so when the container is deleted, instruct the Docker Engine daemon to remove them.
56. To automatically remove anonymous volumes, use the --rm option
57. To remove all unused volumes and free up space:
	$ docker volume prune
58. use docker system prune to clean up multiple types of objects at once.
59. The docker image prune command allows you to clean up unused images. By default, docker image prune only cleans up dangling images. A dangling image is one that is not tagged and is not referenced by any container. 
	$ docker image prune
60. To remove all images which are not used by existing containers, use the -a flag: $ docker image prune -a
61. You can limit which images are pruned using filtering expressions with the --filter flag. For example, to only consider images created more than 24 hours ago:
	$ docker image prune -a --filter "until=24h"
62. When you stop a container, it is not automatically removed unless you started it with the --rm flag.
63. A stopped container’s writable layers still take up disk space. To clean this up, you can use the docker container prune command.
	$ docker container prune
64. By default, all stopped containers are removed. You can limit the scope using the --filter flag. $ docker container prune --filter "until=24h"
65. Volumes can be used by one or more containers, and take up space on the Docker host. Volumes are never removed automatically, because to do so could destroy data.
	$ docker volume prune
66. By default, all unused volumes are removed. You can limit the scope using the --filter flag.
	$ docker volume prune --filter "label!=keep"
67. Docker networks don’t take up much disk space, but they do create iptables rules, bridge network devices, and routing table entries. To clean these things up, you can use docker network prune to clean up networks which aren’t used by any containers.
68. By default, all unused networks are removed. You can limit the scope using the --filter flag.
	$ docker network prune --filter "until=24h"
69. DTR garbage collection can be ran in dry mode using the PARAM_DRY_RUN=true environment variable.
======================================================================================================================================================================================================================================================================================

1. Which command do you use to create a new swarm?
docker swarm init --advertise-addr <MANAGER-IP>

2. What is this flag --advertise-addr for?
This flag configures the IP address for the manager node and The other nodes in the swarm must be able to access the manager at the IP address.

3. How do you know the current status of the swarm?
docker info // you can find the info under the swarm section

4. Which command do you use to find the information about the nodes in the swarm?
docker node ls

5. How to add another manager to the swarm?
// it generate the instructions for the manager to be added
docker swarm join-token manager

6. How to add another worker node to the swarm?
// it generate the instructions for the worker to be added
docker swarm join-token worker

7. How to run the container?
docker run <image>

8. What is the autolock feature in the Docker swarm?
When Docker restarts, both the TLS key used to encrypt communication among swarm nodes, and the key used to encrypt and decrypt Raft logs on disk, are loaded into each manager node’s memory.
Docker 1.13 introduces the ability to protect the mutual TLS encryption key and the key used to encrypt and decrypt Raft logs at rest, 
by allowing you to take ownership of these keys and to require manual unlocking of your managers. This feature is called autolock.

9. How to lock the swarm?
// This command produces unlock key. You need to place that in safe place
docker swarm init --autolock

10. How to unlock the swarm?
docker swarm unlock

11. Are we able to enable autolock feature only when we create a swarm for the first time?
No. You can lock the existing swarm as well

12. How to enable or disable autolock on the existing swarm?
//enable autolock
docker swarm update --autolock=true
//disable autolock
docker swarm update --autolock=false

13. How to view the current unlock key for the running swarm?
docker swarm unlock-key

14. How to rotate the unlock key?
docker swarm unlock-key --rotate

15. If the key was rotated after one of the manager nodes became unavailable and if you don’t have access to the previous key you may need to force the manager to leave the swarm and join it back as a new manager. Is this statement correct?
Yes

16. How to deploy a service in the docker swarm?
// for the nginx image
docker create service --replicas 3 --name nginx-web nginx

17. How to list the services in the Docker swarm?
docker service ls

18. How to list the tasks of the service in the Docker swarm?
docker service ps <service name>

19. How to inspect the service on the swarm?
docker service inspect <service name>

20. How to inspect the service on the swarm so that it will print limited information in an easily readable format?
docker service inspect <service> --pretty

21. How to find out which nodes are running the service?
docker service ps <service>

22. How to find out more details of the container running these tasks of the service?
// you need to run this command on the particular node
docker ps

23. If you are running co-related services in the docker swarm, what do you call this?
stack

24. What is Docker stack?
A stack is a group of interrelated services that share dependencies, and can be orchestrated and scaled together.

25. Explain the several commands associated with Docker stack?
// deploy the new stack or update
docker stack deploy -c <compose file>
// list services in the stack
docker stack services
// list the tasks in the stack
docker stack ps
// remove the stack
docker stack rm
//List stack
docker stack ls

26. How to filter the services in the stack?
// with the help of --filter flag
docker stack service nginx-web --filter name=web 

27. How to format the output of the docker stack services command?
docker stack services --format "{{.ID}}: {{.Mode}} {{.Replicas}}"

28. How to increase the number of replicas?
docker service scale SERVICE=REPLICAS
// example
docker service scale frontend=50

// you can scale multiple services as well
docker service scale frontend=50 backend=30

/ you can also scale with the update command
docker service update --replicas=50 frontend

29. How to revert the changes for the service configuration?
docker service rollback my-service

30. What are the networks available for the docker services?
overlay networks: manage communications among the Docker daemons participating in the swarm.You can attach a service to one or more existing overlay networks as well, 
to enable service-to-service communication.

ingress network: is a special overlay network that facilitates load balancing among a service’s nodes. When any swarm node receives a request on a published port, it hands that request off to a module called IPVS. 
IPVS keeps track of all the IP addresses participating in that service, selects one of them, and routes the request to it, over the ingress network.

docker_gwbridge: is a bridge network that connects the overlay networks (including the ingress network) to an individual Docker daemon’s physical network.


31. Is the ingress network created automatically when you initialize or join a swarm?
Yes

32. Is docker_gwbridge network created automatically when you initialize or join a swarm?
Yes

33. How to create an overlay network?
docker network create --driver overlay my-network
// you can customize it
 docker network create \
  --driver overlay \
  --subnet 10.0.9.0/24 \
  --gateway 10.0.9.99 \
  my-network

34. How to inspect the network?
docker network inspect my-network

35. How to attach a service to an overlay network?
docker service create \
  --replicas 3 \
  --name my-web \
  --network my-network \
  nginx

36. Can service containers connected to the overlay network communicate with each other?
yes

37. How to find which networks the service is connected to?
docker network inspect my-network
               or
docker service ls // for the name
docker service ps <SERVICE> // to list the networks

38. 38. Customize the ingress network involves removing and creating a new one and you need to do that before you create any services in the swarm. Is this statement correct?
Yes

39. 39. How to remove and create an ingress network?
docker network rm ingress

docker network create \
  --driver overlay \
  --ingress \
  --subnet=10.11.0.0/16 \
  --gateway=10.11.0.2 \
  --opt com.docker.network.mtu=1200 \
  my-ingress

40. What is the difference between -v and --mount flags in terms of creating volumes?
Originally, the -v or --volume flag was used for standalone containers and the --mount flag was used for swarm services. However, starting with Docker 17.06, you can also use --mount with standalone containers. In general, --mount is more explicit and verbose.

41. How to create a service with volume?
docker service create -d \
  --replicas=4 \
  --name devtest-service \
  --mount source=myvol2,target=/app \
  nginx:latest

42. Does docker service create command supports -v or — volume flag?
No.

43. What are the volume drivers?
When building fault-tolerant applications, you might need to configure multiple replicas of the same service to have access to the same files.
Volume drivers allow you to abstract the underlying storage system from the application logic. For example, if your services use a volume with an NFS driver,
you can update the services to use a different driver, as an example to store data in the cloud, without changing the application logic.

44. How to create a volume with the volume driver?
docker volume create --driver vieux/sshfs \
  -o sshcmd=test@node2:/home/test \
  -o password=testpassword \
  sshvolume

45. How to create a service with volume driver?
docker service create -d \
  --name nfs-service \
  --mount 'type=volume,source=nfsvolume,target=/app,volume-driver=local,volume-opt=type=nfs,volume-opt=device=:/var/docker-nfs,volume-opt=o=addr=10.0.0.10' \
  nginx:latest

46. I created a deployment that runs exactly one task on every node. which type of service deployment is this?
global

47. I created a deployment that runs several identical tasks on nodes. which type of service deployment is this?
replicated

48. If you want to troubleshoot the UCP clusters what is the best method?
it's always best practice to use client bundle to troubleshoot UCP clusters

49. What is the general flow when troubleshooting services or clusters?
docker service ls
docker service ps <service>
docker service inspect <service>
docker inspect <task>
docker inspect <container>
docker logs <container>

50. How to update metadata about a node?
you can use labels to add metadata about the node

51. How to add a label to the node?
docker node update --label-add foo worker1
// add multiple labels
docker node update --label-add foo --label-add bar worker1

52. How to remove the label from the node?
docker node update --label-rm foo worker1

53.  How to set up the service to divide tasks evenly over different categories of nodes?
--placement-pref

// example: if we have three datacenters 3 replicas will be placed on each datacenter
docker service create \
  --replicas 9 \
  --name redis_2 \
  --placement-pref 'spread=node.labels.datacenter' \
  redis:3.0.6

54. How to limit your service on particular nodes?
--constraint
// example: the following limits tasks for the redis service to nodes where the node type label equals queue

docker service create \
  --name redis_2 \
  --constraint 'node.labels.type == queue' \
  redis:3.0.6

55. Which algorithm does the docker engine use when it is in swarm mode to manage the global cluster state?
Raft Consensus Algorithm

56. What is a quorum and why it is important?
Quorun ensure that the cluster state stays consistent in the presence of failures by requiring a majority of nodes to agree on values.
Raft tolerates up to (N-1)/2 failures and requires a majority or quorum of (N/2)+1 members to agree on values proposed to the cluster.
without quorun swarm wont be able to serve the requests

57. What are the supported flags for creating services with templates?
--env
--mount
--hostname
// example
service create --name hosttempl \
    --hostname="{{.Node.Hostname}}-{{.Node.ID}}-{{.Service.Name}}"\
      busybox top

58. Which instruction sets the base image for the subsequent builds in the Dokcerfile?
FROM

59. No instruction can precede FROM in the Dockerfile. Is this statement correct?
No. ARG is the only instruction can precede FROM

60. What are the two forms for the RUN instruction?
shell form: RUN <command>
exec form: RUN ["executable", "param1", "param2"]

61. What does the RUN instruction do in the Dockerfile?
The RUN instruction will execute any commands in a new layer on top of the current image and commit the results.

62. The RUN command normally utilizes cache from the previous build. Which flag should you specify for the build not to use cache?
--no-cache
docker build --no-cache .

63. Is there any other instruction that can invalidate the cache?
Yes. ADD

64. How many forms that CMD instruction has?
CMD ["executable","param1","param2"] (exec form, this is the preferred form)
CMD ["param1","param2"] (as default parameters to ENTRYPOINT)
CMD command param1 param2 (shell form)

65. If CMD instruction provides default arguments for the ENTRYPOINT instruction, both should be specified in JSON format. Is this statement correct?
Yes

66. What is the purpose of the CMD instruction in the Dockerfile?
The main purpose of a CMD is to provide defaults for an executing container. These defaults can include an executable, or they can omit the executable, in which case you must specify an ENTRYPOINT instruction as well.

67. How to make your container execute the same executable every time?
use ENTRYPOINT in combination with CMD

68. What is the purpose of the LABEL instruction in the Dockerfile?
It adds metadata to the Image

69. How to check the labels for the current image?
docker inspect // Under Labels section

70.  The EXPOSE instruction actually publish the port. Is this statement correct?
No. It serves as a type of documentation between the image publisher and image consumer

71. What should you do to actually publish the ports?
use -p flag when running a container

72. What is the purpose of the ENV instruction in the Dockerfile?
ENV <key> <value>
an ENV instruction sets the enviroment value to the key and it is available for the subsequent build steps and in the running container as well.

73. How to change the environment variables when running containers?
docker run --env <key>=<value>

74. What is the difference between ADD and COPY instructions?
ADD [--chown=<user>:<group>] <src>... <dest>
The ADD instruction copies new files, directories or remote file URLs from <src> and adds them to the filesystem of the image at the path <dest>.
COPY [--chown=<user>:<group>] <src>... <dest>
The COPY instruction copies new files or directories from <src> and adds them to the filesystem of the container at the path <dest>.

75. What is ENTRYPOINT instruction in the Dockerfile?
An ENTRYPOINT allows you to configure a container that will run as an executable.
Command line arguments to docker run <image> will be appended after all elements in an exec form ENTRYPOINT, and will override all elements specified using CMD.

76. How can you override the ENTRYPOINT instruction?
docker run --entrypoint

77. What is the VOLUME instruction in the Dockerfile?
The VOLUME instruction creates a mount point with the specified name and marks it as holding externally mounted volumes from native host or other containers.

78. What initializes the newly created Volume?
docker run -v

79. What is the USER instruction in the Dockerfile?
The USER instruction sets the user name (or UID) and optionally the user group (or GID) to use when running the image and for any RUN, CMD and ENTRYPOINT instructions that follow it in the Dockerfile.

80. What is the WORKDIR instruction in the Dockerfile?
The WORKDIR instruction sets the working directory for any RUN, CMD, ENTRYPOINT, COPY and ADD instructions that follow it in the Dockerfile.

81. You have specified multiple WORKDIR instructions in the Dockerfile what is the result WORKDIR?
WORKDIR /a
WORKDIR b
WORKDIR c
RUN pwd
result: /a/b/c

82. You have specified multiple WORKDIR instructions in the Dockerfile what is the result WORKDIR?
WORKDIR /a
WORKDIR /b
WORKDIR c
RUN pwd
result: /b/c

83. What is the ARG instruction in the Dockerfile?
ARG <name>[=<default value>]
The ARG instruction defines a variable that users can pass at build-time to the builder with the docker build command using the --build-arg <varname>=<value> flag.

84. What is the ONBUILD instruction in the Dockerfile?
The ONBUILD instruction adds to the image a trigger instruction to be executed at a later time, when the image is used as the base for another build.

85. Which instruction sets the system call signal that will be sent to the container to exit?
STOPSIGNAL signal

86. Which instruction let Docker daemon know the health of the container?
HEALTHCHECK

87. What are all the options that can be provided for the HEALTHCHECK instruction?
--interval=DURATION (default: 30s)
--timeout=DURATION (default: 30s)
--start-period=DURATION (default: 0s)
--retries=N (default: 3) 

88. What is the SHELL instruction in the Dockerfile?
The SHELL instruction allows the default shell used for the shell form of commands to be overridden. The default shell on Linux is ["/bin/sh", "-c"], and on Windows is ["cmd", "/S", "/C"]. The SHELL instruction must be written in JSON form in a Dockerfile.

89. Create ephemeral containers is considered best practice?
Yes

90. What should you do if you want to exclude some files while executing the docker build image and don’t want to send all the files to Docker daemon?
use .dockerignore file

91. What is the best way to drastically reduce the size of an image?
Multi Stage Builds


92. How do you minimize the number of layers while building the image?
Only the instructions RUN, COPY, ADD create layers.
Where possible, use multi-stage builds, and only copy the artifacts you need into the final image.
sort multi line arguments
RUN apt-get update && apt-get install -y \
  bzr \
  cvs \
  git \
  mercurial \
  subversion

93. How to leverage the build cache?
Put instructions that likely to change often at the bottom of the dockerfile.

94. How to remove unused images?
docker image prune

95. How to see the history of the image?
docker image history

96.How to format the output of the docker inspect command?
by using --format flag
//examples
docker inspect --format='{{range .NetworkSettings.Networks}}{{.MacAddress}}{{end}}' $INSTANCE_ID
docker inspect --format='{{.LogPath}}' $INSTANCE_ID

97. How to tag an image?
docker tag SOURCE_IMAGE[:TAG] TARGET_IMAGE[:TAG]
docker tag 0e5574283393 fedora/httpd:version1.0 // by id
docker tag httpd fedora/httpd:version1.0 // by name
docker tag httpd:test fedora/httpd:version1.0.test // by name and tag
docker tag 0e5574283393 myregistryhost:5000/fedora/httpd:version1.0

98. How to run a local registry?
docker run -d -p 5000:5000 --restart=always --name registry registry:2

99. How to copy an image from the docker hub to a local repository?
// pull an image from the Docker Hub
docker pull ubuntu
// tag an image
docker tag ubuntu:16.04 localhost:5000/my-ubuntu
// push the image
docker push localhost:5000/my-ubuntu

100. How to stop and remove a local registry?
docker container stop registry && docker container rm -v registry.

101. How to display the layers of the Docker image?
docker image inspect //under Layers section

102. How to create a Docker image from archive or stdin?
docker image load
// example
docker image load -i example.tar

103. How to modify an image to a single layer?
// take any multiple layer image
// run the container
docker export <container> > single-layer.tar
docker import /path/to/single-layer.tar
// check the history
docker image history

104. Each layer is only a set of differences from the layer before it. The layers are stacked on top of each other. Is this statement about the image correct?
Yes

105. When you create a container It adds one writable layer on top of all the layers of the image. Is this statement about the image correct?
yes

106. What is the copy-on-write (CoW) strategy?
Copy-on-write is a strategy of sharing and copying files for maximum efficiency. If a file or directory exists in a lower layer within the image, and another layer (including the writable layer) needs read access to it, it just uses the existing file. The first time another layer needs to modify the file (when building the image or running the container), 
the file is copied into that layer and modified. This minimizes I/O and the size of each of the subsequent layers.

107. How to customize the registry while deploying?
// customize published port
docker run -d \
  -p 5001:5000 \
  --name registry-test \
  registry:2

// If you want to change the port the registry listens on within the container
docker run -d \
  -e REGISTRY_HTTP_ADDR=0.0.0.0:5001 \
  -p 5001:5001 \
  --name registry-test \
  registry:2

// storage customization
docker run -d \
  -p 5000:5000 \
  --restart=always \
  --name registry \
  -v /mnt/registry:/var/lib/registry \
  registry:2

108. How to configure a registry?
The Registry configuration is based on a YAML file. you can specify a configuration variable from the environment by passing -e arguments to your docker run stanza or from within a Dockerfile using the ENV instruction.

// for example you have a configuration like this for root directory
storage:
  filesystem:
    rootdirectory: /var/lib/registry

/ you can create environment variable like this
REGISTRY_STORAGE_FILESYSTEM_ROOTDIRECTORY=/somewhere
it will change from /var/lib/registry to /somewhere

109. What is the location of the registry configuration file?
/etc/docker/registry/config.yml

110. How to customize an entire config file of registry?
docker run -d -p 5000:5000 --restart=always --name registry \
             -v `pwd`/config.yml:/etc/docker/registry/config.yml \
             registry:2

111. How to login to a self-hosted registry?
docker login localhost:5000

112. Where do you configure any credential helpers or credentials for the registry to prevent passing every time you log in?
/etc/docker/daemon.json

113. How to limit the number of records when docker search?
docker search nginx --limit=2

114. How to format the docker search?
docker search --format "{{.Name}}: {{.StarCount}}" nginx

115. How to disable Image signing while pushing an image to the repository?
docker push [OPTIONS] NAME[:TAG]
--disable-content-trust=true

116. How to enable docker content trust in the Docker CLI?
export DOCKER_CONTENT_TRUST=1
docker push <dtr-domain>/<repository>/<image>:<tag>

117. How to pull an image from the repository?
docker pull [OPTIONS] NAME[:TAG|@DIGEST]

118. How to pull an image with multiple images?
-a or --all-tags
docker pull --all-tags fedora

119. How to remove all images which are not used by existing containers?
docker image prune -a

120. by uisng --filter flag
docker image prune -a --filter "until=24h"

121. How to remove an image?
docker rmi <IMAGE ID>

122. How to remove image without deleting the untagged parent images?
docker rmi --no-prune <IMAGE ID>

123. How to delete an image from the repository?
login into DTR web UI
go to the TAGS section delete the specific TAG 
you can also delete all images by deleting the entire repository

124. What is the recommended way of installing Docker
set up docker repositories
install from them for the ease of installation and upgrade tasks.

125. How to upgrade docker-engine?
sudo apt-get update
install docker from the instructions from here

126. How to uninstall docker?
sudo apt-get purge docker-ce
sudo rm -rf /var/lib/docker

127. Are Images, containers, volumes, or customized configuration files on your host are not automatically removed when you uninstall docker?
No. You need to explicitly delete those

128. How to add the user to the Docker group and use docker as a non-root user?
sudo usermod -aG docker your-user

129. What are the ways to install docker?
1. using repositories
2. using DEB package
3. using convience scripts

130. How to install Docker CE on Centos?
// uninstall older versions
sudo yum remove docker \
                docker-client \
                docker-client-latest \
                docker-common \
                docker-latest \
                docker-latest-logrotate \
                docker-logrotate \
                docker-engine
// install required libs
sudo yum install -y yum-utils \
  device-mapper-persistent-data \
  lvm2
// set up the stable repo
sudo yum-config-manager \
    --add-repo \
    https://download.docker.com/linux/centos/docker-ce.repo
// install
sudo yum install docker-ce docker-ce-cli containerd.io
// if you want to install specific versions
sudo yum install docker-ce-<VERSION_STRING> docker-ce-cli-<VERSION_STRING> containerd.io
// start docker
sudo systemctl start docker

131. How to install Docker CE on Debian?
/ uninstall older versions
sudo apt-get remove docker docker-engine docker.io containerd runc
// update
sudo apt-get update
// install required 
sudo apt-get install \
    apt-transport-https \
    ca-certificates \
    curl \
    gnupg2 \
    software-properties-common
// add dockers official gpg key
curl -fsSL https://download.docker.com/linux/debian/gpg | sudo apt-key add -
// set up stable repo
sudo add-apt-repository \
   "deb [arch=amd64] https://download.docker.com/linux/debian \
   $(lsb_release -cs) \
   stable"
// update and install
sudo apt-get update
sudo apt-get install docker-ce docker-ce-cli containerd.io
// if you want to install specific versions
sudo apt-get install docker-ce=<VERSION_STRING> docker-ce-cli=<VERSION_STRING> containerd.io

132. How to install Docker CE on Fedora?
// uninstall old versions
sudo dnf remove docker \
                docker-client \
                docker-client-latest \
                docker-common \
                docker-latest \
                docker-latest-logrotate \
                docker-logrotate \
                docker-selinux \
                docker-engine-selinux \
                docker-engine
// install required packages
sudo dnf -y install dnf-plugins-core
// add the stable repo
sudo dnf config-manager \
    --add-repo \
    https://download.docker.com/linux/fedora/docker-ce.repo
// install community version
sudo dnf install docker-ce docker-ce-cli containerd.io
// if you want specific versions
sudo dnf -y install docker-ce-<VERSION_STRING> docker-ce-cli-<VERSION_STRING> containerd.io
// start docker
sudo systemctl start docker

133. How to install Docker CE on Ubuntu?
// uninstall old versions
sudo apt-get remove docker docker-engine docker.io containerd runc
// update and install required packages
sudo apt-get update
sudo apt-get install \
    apt-transport-https \
    ca-certificates \
    curl \
    gnupg-agent \
    software-properties-common
// add official gpg key
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
// stable repo
sudo add-apt-repository \
   "deb [arch=amd64] https://download.docker.com/linux/ubuntu \
   $(lsb_release -cs) \
   stable"
// update and install
sudo apt-get update
sudo apt-get install docker-ce docker-ce-cli containerd.io
// if you want specific versions
sudo apt-get install docker-ce=<VERSION_STRING> docker-ce-cli=<VERSION_STRING> containerd.io

134. What are the recommended storage drivers on different distributions?
Centos: overlay2
Ubuntu supports overlay2, aufs and btrfs storage drivers. Overlay2 is the default one

135. What are all the release channels that Docker CE supports?
Stable gives you latest releases for general availability.
Test gives pre-releases that are ready for testing before general availability.
Nightly gives you latest builds of work in progress for the next major release.

136. Where are the Docker-CE binaries available?
Docker Engine - Community binaries for a release are available on download.docker.com as packages for the supported operating systems.

137. Where are the Docker-EE binaries available?
Docker Hub

138. What are logging drivers?
Docker has multiple mechanisms to get the logging information from running docker containers and services. These mechanisms are called logging drivers

139. How to configure a logging driver for the Docker daemon so that all the containers use it?
configure log-driver in /etc/docker/daemon.json
{
  "log-driver": "syslog"
}

140. Whats is the default logging driver?
json-file

141. If you have configurable options for your logging driver how do you specify?
use log-opts in the daemon.json file
{
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "10m",
    "max-file": "3",
    "labels": "production_status",
    "env": "os,customer"
  }
}

142. How to find the logging driver for the Docker daemon?
docker info --format '{{.LoggingDriver}}'

143. How to configure a logging driver for a container?
docker run -it --log-driver json-file --log-opt max-size=10m alpine ash 

144. What are the available logging drivers for the Docker CE edition?
json-file
local
journald

145. If the swarm loses the quorum of managers it loses the ability to perform management tasks. Is this statement correct?
Yes.

146. We should use a fixed IP address for the advertise address to prevent the swarm from becoming unstable on machine reboot. Is this statement correct?
Yes. If the whole swarm restarts and every manager node subsequently gets a new IP address, there is no way for any node to contact an existing manager. Therefore the swarm is hung while nodes try to contact one another at their old IP addresses.

147.  If the swarm loses quorum all the existing tasks and services are all deleted. Is this statement correct?
No, All the existing tasks will continue to run. But, new nodes cannot be added and new tasks can't be created.

148. You should maintain an odd number of managers in the swarm to support manager node failures. Is this statement correct?
yes.

149. I have manager nodes 3, 5, 7, 9. How do you distribute these manager nodes on availability zones so that If you suffer a failure in any of those zones, the swarm should maintain the quorum of manager nodes
Manager Nodes		Availability Zones
3				1-1-1
5				2-2-1
7				3-2-2
9				3-3-3

150. How to drain the node
docker node update --availability drain <NODE>

151. How to cleanly rejoin a manager node in the cluster?
1. To demote the node to a worker, run docker node demote <NODE>
2. To remove the node from the swarm, run docker node rm <NODE>
3. Re-join the node to the swarm with a fresh state using docker swarm jo

152. How to forcibly remove a node?
docker node rm --force <NODE>

153. If you want to remove a manager node you need to demote it to a worker role first. Is this statement correct?
Yes. You must ensure that there is a quorum

154. What is the location where swarm managers save the swarm state?
/var/lib/docker/swarm

155. How to backup the swarm?
1. If autolock is enabled. You must unlock the swarm
2. stop the docker on the manager node so that you don't have unpredictable results
3. save the entire contents of /var/lib/docker/swarm
4. start the manager

156. How to restore swarm from the backup?
1. shut down the docker on the targeted machine
2. Remove the contents of /var/lib/docker/swarm
3. Restore the /var/lib/docker/swarm directory from the backup
4. Start the docker on the node so that it doesn't connect to old ones
docker swarm init --force-new-cluster
5. Verify the state of the swarm docker service ls
6. rotate the autolock key
7. Add manager and worker nodes for the required capacity
8. backup this swarm

157. What is a team in the DTR?
A team defines the permissions a set of users have for a set of repositories.

158. What are all the permission levels that teams could have?
Read Only: View repository and pull images.
Read Write: View repository, pull and push images.
Admin: Manage repository and change its settings, pull and push images.

159. Where is the Docker daemon directory?
/var/lib/docker on Linux
C:\ProgramData\docker on Windows

160.  How to enable the debugging on Docker daemon
	1. add this flag in /etc/docker/daemon.json
{
  "debug": true
}
	2. Send a HUP signal to the daemon to cause it to reload its configuration.
	sudo kill -SIGHUP $(pidof dockerd)

161. How to check whether Docker is running?
/ all these can be used depending on the operating system
docker info
sudo systemctl is-active docker
sudo status docker
sudo service docker status

162. What are the hardware and software requirements for UCP?
Minimum
1. 8GB of RAM for manager nodes or nodes running DTR
2. 4GB of RAM for worker nodes
3. 3GB of free disk space
Recommended
1. 16GB of RAM for manager nodes or nodes running DTR
2. 4 vCPUs for manager nodes or nodes running DTR
3. 25-100GB of free disk space

163. What products that Docker EE contains?
UCP
DTR
Docker Engine with enterprise-grade support,

164. Where is the location of the custom certificates?
/etc/docker/certs.d

165. What are the ports that DTR uses?
80/tcp     -     Web app and API client access to DTR.
443/tcp    -     Web app and API client access to DTR

166.  DTR needs to be installed on a worker node that is being managed by UCP. You can't install DTR on a standalone Docker engine. Is this statement correct?
yes

167. How to backup the UCP
To create a UCP backup, run the docker/ucp:2.2.22 backup command on a single UCP manager
docker container run \
  --log-driver none --rm \
  --interactive \
  --name ucp \
  -v /var/run/docker.sock:/var/run/docker.sock \
  docker/ucp:2.2.22 backup \
  --id <ucp-instance-id> \
  --passphrase "secret" > /tmp/backup.tar

168. How to restore the UCP
docker/ucp:2.2.22 restore --passphrase "secret"
docker container run --rm -i --name ucp \
  -v /var/run/docker.sock:/var/run/docker.sock  \
  docker/ucp:2.2.22 restore --passphrase "secret" < /tmp/backup.tar

169. You need to backup the UCP on the single manager node since Docker maintains the same UCP state in all the manager nodes. Is this statement correct?
yes

170. How to backup the DTR?
To perform a backup of a DTR node, run the docker/dtr backup command.

171. DTR requires that a majority (n/2 + 1) of its replicas are healthy at all times for it to work. So if a majority of replicas are unhealthy or lost, the only way to restore DTR to a working state is by recovering from a backup. Is this statement correct?
yes.

172. How to configure Docker to start on boot?
sudo systemctl enable docker

173. What is the default network that the docker creates automatically?
Bridge

174. How to list the networks on the Docker machine?
docker netwrok ls

175. How to connect to the default bridge network when you create a container?
// since no network is specified, it will be connected to default bridge network
docker run -dit --name alpine1 alpine ash

176. How to inspect the default network bridge?
docker network inspect bridge

177. The default bridge network is not recommended for production. Is this statement correct?
Yes

178. How to create a user-defined network?
docker network create --driver bridge my-network

179. How to inspect the user-defined network?
docker network inspect my-network

180. How to connect to the user-defined network while creating a container?
docker run -dit --name alpine1 --network my-network alpine ash

181. How to connect the existing container to the user-defined network?
docker netwrok connect my-network alpine2

182. How to troubleshoot a user-defined network?
// using  nicolaka/netshoot
docker run -it --rm --network container:<container_name> nicolaka/netshoot

List all the backend task IP addresses using ipvsadm: ipvsadm -l -f <fwmark>

183. How to publish a port so that it can be accessed externally?
docker run -p 127.0.0.1:$HOSTPORT:$CONTAINERPORT --name CONTAINER -t <image>

184. How to list port mappings or a specific mapping for the container?
// List the containers
docker ps
// use this command with container name
docker port <CONTAINER NAME>
// USE the specific port
docker port <CONTAINER NAME> <specific port>

185. What are all the different built-in network drivers?
Bridge Network Driver
Overlay Network Driver
MACVLAN Driver
Host
None

186. What are the Bridge network and its use case?
The bridge driver creates a private network internal to the host so containers on this network can communicate.
The bridge driver does the service discovery for us automatically if two containers are on the same network
The bridge driver is a local scope driver, which means it only provides service discovery, IPAM, and connectivity on a single host.

187. What is the scope of the bridge network?
Local

188. What are the Overlay network and their use case?
The built-in Docker overlay network driver radically simplifies many of the complexities in multi-host networking.
It is a swarm scope driver, which means that it operates across an entire Swarm or UCP cluster rather than individual hosts.

189. What is the scope of the overlay network?
swarm.

190. What are the MACVLAN network and their use case?
The macvlan driver is the newest built-in network driver and offers several unique characteristics. 
It’s a very lightweight driver, because rather than using any Linux bridging or port mapping, it connects container interfaces directly to host interfaces.

191. What is the scope of the macvlan network?
local

192. What are the Host network and its use case?
With the host driver, a container uses the networking stack of the host. 
There is no namespace separation, and all interfaces on the host can be used directly by the container.

193. What is the scope of the host network?
local

194. What are the None network and its use case?
The none driver gives a container its own networking stack and network namespace but does not configure interfaces inside the container. Without additional configuration, the container is completely isolated from the host networking stack.

195. What is the scope of the None network?
local

196. The Docker networking architecture is built on a set of interfaces called the Container Networking Model (CNM). Is this statement correct?
yes

197. What is a sandbox in the CNM model?
A Sandbox contains the configuration of a container's network stack. This includes the management of the container's interfaces, routing table, 
and DNS settings. An implementation of a Sandbox could be a Windows HNS or Linux Network Namespace, a FreeBSD Jail, or other similar concept. A Sandbox may contain many endpoints from multiple networks.

198. What is an endpoint in the CNM model?
An Endpoint joins a Sandbox to a Network. The Endpoint construct exists so the actual connection to the network can be abstracted away from the application. 
This helps maintain portability so that a service can use different types of network drivers without being concerned with how it's connected to that network.

199. What is a network in the CNM model?
The CNM does not specify a Network in terms of the OSI model. An implementation of a Network could be a Linux bridge, a VLAN, etc. A Network is a collection of endpoints that have connectivity between them.
 Endpoints that are not connected to a network do not have connectivity on a network.

200. What part of the Docker that provides the actual implementation that makes networks work?
Network Drivers

201. What is IPAM drivers?
Docker has a native IP Address Management Driver that provides default subnets or IP addresses for the networks and endpoints if they are not specified.

202. How to configure docker to use external DNS?
edit the /etc/docker/daemon.json
{    
   "dns": ["10.0.0.2", "8.8.8.8"]
}
restart the docker
sudo systemctl docker restart

203. Which network should handles control and data traffic related to swarm services?
Ingress

204. Which network which connects the individual Docker daemon to the other daemons participating in the swarm?
docker_gwbridge

205. What is the default network created when you create a swarm cluster?
ingress

206. How to create a user-defined overlay network for communication among services?
docker network create -d overlay my-overlay

207. How to create an overlay network which can be used by swarm services or standalone containers to communicate with other standalone containers running on other Docker daemons?
create with --attachable flag
docker network create -d overlay --attachable my-attachable-overlay

208. All the swarm management data is encrypted by default. Is this statement correct?
Yes.

209. is application data on the swarm encrypted by default?
No.

210. How to encrypt application data as well on the swarm?
// use --opt=encrypted
docker network create --opt encrypted --driver overlay --attachable my-attachable-multi-host-network

211. What is the host port publishing mode?
To publish a service’s port directly on the node where it is running, use the mode=host option to the --publish flag.

212. What is DCT?
Through DCT, image publishers can sign their images and image consumers can ensure that the images they use are signed.

213. What is DCT stand for?
Docker Content Trust

214. What is the command to generate delegation keys?
docker trust generate key

215. How to load if you have any existing keys?
docker trust key load

216. How to sign a particular tag and push it up to the registry?
docker trust sign dtr.example.com/admin/demo:1

217. How to enable docker content trust so that you can sign images automatically when you use docker push?
export DOKCER_CONTENT_TRUST=1

218. How to inspect remote trusted data for a tag?
docker trust inspect

219. How to remove remote trusted data for a tag?
docker trust revoke

220. What is a grant?
A grant defines who has how much access to set of resources

221. What is the subject?
A subject can be user, team, organization and is granted a role for set of resources

222. What is the role?
A role is a set of permitted API operations that you can assign to a specific subject and collection by using a grant

223. What is a Client Bundle?
A client bundle is a group of certificates downloadable directly from the Docker Universal Control Plane (UCP) user interface within the admin section for “My Profile”. This allows you to authorize a remote Docker engine to a specific user account managed in Docker EE, absorbing all associated RBAC controls in the process. 
You can now execute docker swarm commands from your remote machine that take effect on the remote cluster.

224. What is the easiest way to access or control the UCP?
Client Bundle

225. What is the kernel feature that isolates the processes running in different containers?
Namespaces

226. Which kernel feature limits the resources used by docker containers?
Control Groups

227. What is the kernel feature that needed extra configuration?
user.

228. Docker swarm should be secure by default?
Yes.

229. What is the pluggable architecture that Docker supports for the container writable layer storage?
Storage Drivers

230. What is the preferred storage driver for all Linux distributions which need no extra configuration?
Overlay2

231. Which device-mapper driver is used for production environments?
direct-lvm

232. Which device-mapper driver is used for testing environments?
loopback-lvm

233. How do you check the current storage driver information?
docker info

234. How do you configure device-mapper?
// stop docker
sudo systemctl stop docker
// set the device-mapper in /etc/docker/daemon.json file
{
  "storage-driver": "devicemapper"
}
//start docker
sudo systemctl start docker

235. what is the option that sets the direct-lvm for production device-mapper?
dm.directlvm_device

236. What do you set the device-mapper and all configurable options in the daemon.json?
{
  "storage-driver": "devicemapper",
  "storage-opts": [
    "dm.directlvm_device=/dev/xdf",
    "dm.thinp_percent=95",
    "dm.thinp_metapercent=1",
    "dm.thinp_autoextend_threshold=80",
    "dm.thinp_autoextend_percent=20",
    "dm.directlvm_device_force=false"
  ]
}

237. What are the different available storage options available for containers?
Block Storage
FiLE System Storage
Object Storage

238. Do containers create a writable layer on top of Image read-only layers?
yes.

239. Where is the Docker’s local storage area?
/var/lib/docker/<storage-driver>

240. What is the difference between bind mounts and volumes?
Volumes are completely managed by docker
Bind Mounts are dependent on the host directory structure

241. Volumes don’t increase the size of the containers. Is this statement correct and why?
Yes. Because volumes live outside of containers

242. What should we use if we want to persist the data beyond the lifecycle of the containers?
Volumes

243. How to create a Volume? 
docker volume create my-volume

244. How to list docker volumes?
docker volume ls

245. How to inspect docker volume?
docker volume inspect my-vol

246. How to remove docker volumes?
docker volume rm my-vol

247. If you start a container with a volume that does not yet exist, Docker creates the volume for you. Is this statement correct?
Yes.

248. How to create a volume myvol2 with a docker run?
docker run -d \
  --name devtest \
  -v myvol2:/app \
  nginx:latest

249. How to verify the volume is created with the container?
// Look for the mounts section
docker inspect devtest

250. How to create a volume with the --mount flag?
docker run -d \
  --name devtest \
  --mount source=myvol2,target=/app \
  nginx:latest

251. How to remove all unused images not just dangling images?
docker system prune --all

252. The commands that operate with DCT are:

push
build
create
pull
run

===========================================================================================================================================================================
1. You can use local file system if you’re deploying DTR with high-availability.
False.

2.The devicemapper can only use a single block device when using direct-lvm mode.
false

3. When you're starting the docker daemon manually and see an error like the one below, what should you do?
unable to configure the Docker daemon with file /etc/docker/daemon.json:
the following directives are specified both as a flag and in the configurationfile: hosts: (from flag: [unix:///var/run/docker.sock], from file: [tcp://127.0.0.1:2376])
Adjsut the flags your using to start daemon or modify the daemon.json to remove the conflicts.

4. Single sign-on (SSO) with UCP can only be configured at installation
False.

5. Which type of mount is a good choice when you need to back up, restore, or migrate data from one Docker host to another?
volumes.

6. You can specify multiple labels for an object, but each key-value pair must be unique within an object.
True.

7. Which Docker feature allows operations with a remote Docker registry to enforce client-side signing and verification of image tags?
Docker content trust.

8. Which command can you use to clean up multiple types of objects (images, containers, and networks) at once?
docker system prune.

9. Docker's default container format is called?
Libecontainer.

10. To bypass the routing mesh, you can start a service using DNS Round Robin (DNSRR) mode
True.

11. To bypass the routing mesh, you must use the long --publish service and set mode to host
True.

12. Restart policies only apply to containers, not swarm services.
True.

13. Which format is the output returned, when the --pretty flag is not specified for the docker service inspect command?
Json

14. When running network in 'host' mode you can override the hostname of the container using which option of 'docker run'?
--hostname

15. Which of the following commands can be used to create a readonly volume?
docker run -d --name test --mount source=testvol, target=/testvol,readonly ubuntu:latest
docker run -d --name test -v testvol:/testvol:ro ubuntu:latest


16. Which command is used to join a swarm as a worker?
docker swarm join

17. Which instruction specifies the Base Image from which you are building?
FROM

18. If the manager in a single-manager swarm fails, your services continue to run, but you need to create a new cluster to recover.
True.

19. The docker daemon is called?
dockerd

20. Which instruction copies new files, directories or remote file URLs from <src> and adds them to the filesystem of the image at the path <dest>?
ADD

21. Which option allows you to override the default Docker directory (/var/lib/docker) on Linux?
data-root

22. Which availability mode needs to be set on a node when you need to perform maintenance on a node?
DRAIN

23. Which of the following make up container format? 
Namespaces, control groups, unionFS

24. Which flag of 'docker service create' and 'docker service update' configures the time delay between updates to a service task or sets of tasks?
--update-delay

25. Docker EE is available on multiple platforms. Which of the following are not valid hardware platforms to run Docker EE?
SUN/oracle SPARC

26. Which command can you run on a manager node to retrieve the join command for a worker?
docker swarm join-token worker

27. Which instruction is required if no executable is specified in the CMD instruction?
ENTRYPOINT

28. Which command should you run to show all the nodes in a swarm?
docker node ls

29. Docker Engine uses which file system to provide the building blocks for containers?
UnionFS

30. Which flag should be specified for 'docker/dtr install' to configure DTR to have single sign-on (SSO) with UCP?
--dtr-external-url

31. Docker regularly cleans up unused objects, such as images, containers, volumes, and networks
False

32. Which docker feature allows you to control whether your containers start automatically when they exit, or when Docker restarts, and that they are started in the correct order
Restart policy.

33. Which products are included with a Docker EE software subscription
Docker content trust
docker universal control plane
docekr engine with enterprise-grade support

34. ____ is an instance of the Docker engine participating in the swarm.
Node.

35. Build-time variable values are not visible to anyone so it is OK to pass secret keys, user credentials, etc
False.

36. You can create a volume dynamically using which command?
docker volume create.

37. Which 'docker service update' flags are used to update the published ports of existing services?
--publish-rm
--publish-add

38. Tasks are initialized in which state?
NEW

39. Which command is used to create a swarm
docker swarm init

40. DTR only allows deleting images if that image hasn’t been signed.
true.

41. The first instruction in a dockerfile must be?
FROM

42. The core component of UCP is a globally-scheduled service called?
ucp-agent

43. Which flag of 'docker service create' determines how many tasks are started for the service?
--replicas

44. Which command is used to create an overlay network that can be used with swarm services
docker network create -d overlay 'network name'

45. Without the quorum, administrative tasks are not possible, including scaling or updating services and joining or removing nodes from the swarm.
true.

46. If a container has files or directories in the directory to be mounted, the directory’s contents are copied into the volume and the volume is mounted
True.

47. To decrease the image pull time for geographically dispersed users you can configure which DTR option?
DTR cache

48. After joining a swarm, machines that are running Docker, are referred to as?
nodes

49. Which command is used to remove a user-defined bridge network?
docker network rm 'network name'

50. Which technology allows Docker Engine to share available hardware resources to containers and optionally enforce limits and constraints?
control groups

51. What is the default network driver used when one is not specified?
bridge

52. Which option of dockerd is useful for troubleshooting
--debug

53. Which storage driver uses block devices dedicated to Docker and operates at the block level, rather than the file level?
devicemapper

54. By default manager nodes also run services as worker nodes, but you can configure them to run manager tasks exclusively and be manager-only nodes.
True.

55. By default the docker/dtr backup will perform an online backup
true

56. In an environment where DOCKER_CONTENT_TRUST is set, which docker build flag allows individual operations on tagged images without DCT?
--disable-content-trust

57. Docker is written in which programming language?
Go

58. Instead of enabling DCT through the system-wide configuration, DCT can be enabled or disabled on a per-shell or per-invocation basis
true

59. Changing the storage driver makes any containers you have already created inaccessible on the local system.
true.

60. t is possible to take a backup while the manager is running (a “hot” backup), but this is not recommended and your results are less predictable when restoring.
true

61. Command line arguments to docker run <image> will be appended after all elements in an exec form ENTRYPOINT, and will override all elements specified using CMD.
true

62. Which two commands can get an image from a registry?
docker pull
docker run

63. Which command is used to force a full stack trace to be logged?
sudo kill -SIGUSR1 $(pidof dockerd)

64. Docker uses which technology to provide the isolated workspace that make up the container?
Namesapces

65. Which of the following are valid states of the MANAGER STATUS column as shown when running 'docker node ls'? (select three)
Leader, reachable, unavailable

66. Which instruction allows you to configure a container that will run as an executable?
ENTRYPOINT

67. Which command is used to see the list of running services?
docker service ls

68. If you bind-mount into a non-empty directory on the container, the directory’s existing contents are obscured by the bind mount.
True.

69. Which instruction copies new files, directories from <src> and adds them to the filesystem of the container at the path <dest>?
copy


70. Which command is used to delete a service from the swarm?
docker service rm

71. Which flag of 'docker service create' and 'docker service update' configures the maximum number of service tasks that the scheduler updates simultaneously?
--update-parallelism

72. There are two ways to configure the Docker daemon:
a json file congiguration
with flags when starting docker.

73. If a tasks fails during initialization it reverts to its previous state
false.

74. What mechanism allows you to organize your images, record licensing information, annotate relationships between containers, volumes, and networks, etc?
labels

75. Which command is used to scale the number of containers / tasks in a service?
docker service scale

76. Which instruction will execute any commands in a new layer on top of the current image and commit the results?
RUN

77. Which network driver connects multiple Docker daemons together and enables swarm services to communicate with each other, thereby removing the need to do OS-level routing between containers?
overlay

78. Swarm services connected to the same overlay network effectively expose all ports to each other.
true.

79. If you manually stop a container, its restart policy is ignored until the Docker daemon restarts or the container is manually restarted
true.

80. Which environment variable needs to be set in order to enable DCT on a per-shell basis?
DOCKER_CONTENT_TRUST.

81. Which flag of 'docker swarm init' enables autolocking of swarm manager nodes when Docker restarts?
--autolock

82. When running network in 'host' mode you can override the dns related settings of the host by using which options of 'docker run'?
--dns
--dns-search
--dns-opt
--hostname

83. When you receive the error below, what should you do?
docker login dtr.example.org
x509: certificate signed by unknown authority
Get the DTR CA certificate and configure your OS to trust that certifcate.

84. What is used to define what goes on in the environment inside a container?
Dockerfile

85. Which command is used to create an overlay network that can be used with swarm services or standalone containers to communicate with other standalone containers running on other Docker daemons?
docker network create -d overlay --attachable 'network name'

86. For a port to be accessible outside of the service, that port must be published. Which of the following options to 'docker service create' are valid methods of publishing port 80 on the service to port 8080 on the routing mesh?
-p 8080:80
-p published=8080,target=80

87. Which of the following are valid states of the AVAILABILITY column as shown when running 'docker node ls'?
active
Drain
Pause

88. Which ports have to be opened to allow traffic to, and from, docker hosts that will be part of an overlay network? 
TCP port 2377 for cluster management communications
TCP and UDP port 7946 for communication among nodes
UDP port 4789 for overlay network traffic

89. Which Docker Tool/Features is used to provision and manage multiple remote Docker hosts or Swarm Clusters?
Docker machine

90. Which network driver allows you to assign a MAC address to a container, making it appear as a physical device on your network?
macvlan

91. Which command is used to promote a worker node to be a manager?
docker node promote

92. Which instruction defines a variable that users can pass at build-time?
ARG

93. With which type of mount may the data be stored anywhere on the host system?
bind mounts

94. Docker manager nodes store the swarm state and manager logs in which location?
/var/lib/docker/swarm/

95. Docker Hub and Docker Cloud are examples of?
docker registries

96. Which command should you run to connect a running container to an existing user-defined bridge
docker network connect 'network name' 'container name'

97. Which command is used to get the state of a task?
docker service ps

98. Which file is used to configure the default logging driver?
/etc/docker/daemon.json

99. When DCT is enabled in the Docker client, docker CLI commands that operate on tagged images must either have content signatures or explicit content hashes. Which docker command does not operate with DCT?
Load.

100. What is the only instruction that may precede FROM in the Dockerfile?
ARG

101. Which type of mount is a good choice when you want to store your container’s data on a remote host or a cloud provider, rather than locally?
volumes

102. Which of the following manager node failure scenarios will be handled without downtime? 
three-mabnger swarm tolerates a maximum loss of one manager
a five manager swarm tolerates maximum simulataneous loss of two manager nodes.

103. It recommended to write instructions in a dockerfile in UPPERCASE and arguments in lowercase
True

104. To verify that the daemon is using the devicemapper storage driver you should?
use the docker info and verify storage driver is shown as device maper

105. Which of the following are valid Docker Editions?
Docker EE and docker CE

106. Which two file persistence options are available in Docker? (File persistence allows containers to store files in the host machine, so that they are available even after the container stops)
bind mounts and volumes

107. Which command is used to create an overlay network capable of encrypting application data?
docker network create -d overlay --opt encrypted 'network name'

108. Which instruction creates a mount point with the specified name and marks it as holding externally mounted volumes from native host or other containers?
volume

109. If you use --mount to bind-mount a file or directory that does not yet exist on the Docker host, Docker does not automatically create it for you, but generates an error.
true

110. Besides the readonly image and the writable container layer, which other ways can containers take up disk space? 
voulmes n bind mounts used by containers.
disk spaces used for container's configuration file which are typpically small
disk spaces used for log files if you use the json-file logging driver

111. Which command is used to create a service in a swarm?
docker service create

112. Swarm management commands like docker node ls only work on manager nodes.
true

113. Which command is used on a node to remove it from the swarm?
docker swarm leave

114. Which command should you run to disconnect a running container from an existing user-defined bridge
docker netwrok disconnect 'network name' 'conatiner name'

115. Which mechanism is used exclude files and directories from the context that docker CLI uses to create an IMAGE
.dockerigonore file

116. Before signing and pushing images to DTR you should perform which actions?
configure the notary CLI client
import your UCP private keys to the notary client.

117. Which instruction's main purpose is to provide defaults for an executing container?
CMD

118. All application data traffic is encrypted by default.
false

119. Which column of the 'docker node ls' command shows whether or not the scheduler can assign tasks to the node?
AVAILABLITY

Active means that the scheduler can assign tasks to the node.
Pause means the scheduler doesn’t assign new tasks to the node, but existing tasks remain running.
Drain means the scheduler doesn’t assign new tasks to the node. The scheduler shuts down any existing tasks and schedules them on an available node.

120. To enable high-availability for Docker Trusted Registry (DTR) you can deploy multiple DTR replicas on different UCP workers.
true.

121. Which command can upload an image to a registry?
docker push

122. A group of machines that are running Docker and joined into a cluster is called a?
swarm

123. FROM can only appear once in a single dockerfile
False

124. Which cleanup command should be run on the manager node after another node has left the swarm?
docker node rm

125. Which command is used to live stream a container’s runtime metrics?
docker stats

126. Which dockerd flag allows you to keep your containers running during a Docker upgrade, though networking and user input are interrupted?
--live-restore

127. Which Docker feature allows you to protect the mutual TLS encryption key and the key used to encrypt and decrypt Raft logs at rest, by allowing you to take ownership of these keys and to require manual unlocking of your managers?
autolock

128. The only way to recover from losing the quorum is to use which command from a manager node?
docker swarm init --force-new-cluster

129. Running 'docker/dtr backup' will backup your DTR Metadata and Image Content
false.

130. a container’s hostname defaults to be the container’s ID in Docker
true

131. You can modify a service’s configuration, including the networks and volumes it is connected to, without the need to manually restart the service
true.

132. Docker runs instructions in a dockerfile in sequential order
true.

133. Which of the following are valid options for the '--update-failure-action' of the 'docker service create' and 'docker service update' commands?
continue
rollback
pause

134. What is the enterprise-grade cluster management solution from Docker called?
docker UCP

135. Which column of the 'docker node ls' command shows shows node participation in the Raft consensus?
MANAGER STATUS

136. You can set the logging driver for a specific container by using the --log-driver flag to which commands? 
docker container create
docker run

137. You can use the 'docker swarm join-token' command to generate which types of tokens? 
manager and worker

138. A container is launched by running which of the following?
images

139. Each layer is stored in its own directory inside the Docker host’s local storage area.
true

140. All swarm service management traffic is encrypted by default.
true

141. Docker Trusted Registry (DTR) has to be installed on UCP
true/

142. What is the maximum recommended number of managers for a swarm?
7

143. Which command is used to see details about the service running in the swarm AND provide the output in easily readable format?
docker service inspect --pretty

144. Which command is used to demote a manager node to be a worker?
docker node demote

145. Which command removes all images which are not used by existing containers?
docker image prune -a

146. If you use -v or --volume to bind-mount a file or directory that does not yet exist on the Docker host, Docker does not automatically create it for you, but generates an error.
false

147. Dockerfile should specify at least one of CMD or ENTRYPOINT commands.
true

148.____ is a running container which is part of a swarm service and managed by a swarm manager, as opposed to a standalone container
task

149. Which instruction sets the user name (or UID) and optionally the user group (or GID) to use when running the image and for any RUN, CMD and ENTRYPOINT instructions that follow it in the Dockerfile?
USER

150. With which type of mount is the data stored in a part of the host filesystem which is managed by Docker (/var/lib/docker/volumes/ on Linux)?
volumes

151. _______ is the definition of the tasks to execute on the manager or worker nodes.
service

152. Which flag of docker run allows you to use a different container logging driver than the Docker daemon’s default?
--log-driver

153. Which network driver removes network isolation between the container and the Docker host, and uses the host’s networking directly?
host

154. The host networking driver is supported on which of the following?
Linux host


156. What happens when Docker can't find an image locally?
It looks for the image on the Docker Hub website

157. What does "docker run" do?
You supply an image as another parameter and it will create a container from it

158. How do you run an nginx web server in a container?
docker run -d -P --name [containerName] nginx

159. Which of the Dockerfile options initializes a new build stage and sets the base image for subsequent instructions?
FROM

160. Which of the following is NOT how to create an efficient image via a Dockerfile?
Start with an appropriate base image
Use multi-stage builds
Avoid installing unnecessary packages
Combine multiple applications into a single container   // correct answer

161. Volume mapping maps the host server's directory into the Docker container. The data will remain in a safe and accessible place if you do which of the following?
backup the container  //correct answer
re-create the container
delete the container
migrate the container

162. ________is a tool for defining and running multi-container Docker applications.
Docker compose.

163. <p>What will the following command do?</p><p> docker run -t -i ubuntu:14.04 /bin/bash</p>
Starts ubuntu container and puts us at a command prompt inside of it

164. <p>What does the "--name" parameter do in the following command?</p><p> docker run --name mywordpress --link wordpressmysql:mysql -P -d wordpress</p>
Provides a custom name for the container that will be run

165. Which​ ​command​ ​is​ ​used​ ​to​ ​place​ ​an​ ​image​ ​into​ ​a​ ​registry? 
docker​ ​push 

166. Which​ ​network​ ​allows​ ​Docker​ ​Trusted​ ​Registry​ ​components​ ​running​ ​on​ ​different​ ​nodes​ ​to communicate​ ​and​ ​replicate​ ​Docker​ ​Trusted​ ​Registry​ ​data? 
 dtr-ol 

167. Which​ ​of​ ​the​ ​following​ ​is​ ​not​ ​an​ ​endpoint​ ​exposed​ ​by​ ​Docker​ ​Trusted​ ​Registry​ ​that​ ​can​ ​be used​ ​to​ ​assess​ ​the​ ​health​ ​of​ ​a​ ​Docker​ ​Trusted​ ​Registry​ ​replica? 
 /replica_status 


168.  ​Which​ ​of​ ​the​ ​following​ ​endpoints​ ​exposed​ ​by​ ​Docker​ ​Trusted​ ​Registry​ ​can​ ​be​ ​used​ ​to assess​ ​the​ ​health​ ​of​ ​a​ ​Docker​ ​Trusted​ ​Registry​ ​replica? 
 /health 


169.  ​One​ ​of​ ​your​ ​developers​ ​is​ ​trying​ ​to​ ​push​ ​an​ ​image​ ​to​ ​the​ ​registry​ ​(dtr.example.com).​ ​The push​ ​fails​ ​with​ ​the​ ​error​ ​“denied:​ ​requested​ ​access​ ​to​ ​the​ ​resource​ ​is​ ​denied”.​ ​What​ ​should​ ​you verify​ ​the​ ​user​ ​has​ ​completed? 
 docker​ ​login​ ​-u​ ​<username>​ ​-p​ ​<password> dtr.example.com 


170. You​ ​have​ ​been​ ​asked​ ​to​ ​backup​ ​the​ ​swarm​ ​state​ ​on​ ​a​ ​Linux​ ​installation.​ ​By​ ​default,​ ​where do​ ​Docker​ ​manager​ ​nodes​ ​store​ ​the​ ​swarm​ ​state​ ​and​ ​manager​ ​logs? 
 /var/lib/docker/swarm 

171. Which​ ​of​ ​the​ ​following​ ​will​ ​put​ ​the​ ​Docker​ ​engine​ ​into​ ​debug​ ​mode? 
 echo  '{"debug":​ ​true}'​ ​>​ ​/etc/docker/daemon.json​ ​;​ ​sudo​ ​kill​ ​-HUP​ ​<pid​ ​of dockerd>  


172.  ​How​ ​do​ ​you​ ​deploy​ ​4​ ​new​ ​instances​ ​of​ ​nginx​ ​with​ ​a​ ​single​ ​command? 
docker​ ​service​ ​create​ ​--replicas​ ​4​ ​--name​ ​myservice​ ​nginx 

173. You​ ​are​ ​using​ ​self-signed​ ​UCP​ ​certs​ ​and​ ​have​ ​a​ ​second​ ​DNS​ ​name​ ​that​ ​points​ ​to​ ​your internal​ ​controllers.​ ​When​ ​installing​ ​UCP,​ ​which​ ​flag​ ​should​ ​you​ ​use​ ​to​ ​add​ ​this​ ​additional name? 
 --san 

174. following are valid values of the --restart flag for docker run
always
on-failure
no
unless stopped.

175. To configure the restart policy for a container, use the --restart flag when using the docker run command. 


176. If you want to create a services but prevent it from being deployed you should?
scale the service to 0

177. Which instruction informs Docker that a container listens on specified network ports at runtime?
EXPOSE.

178. Which type of file defines how Docker containers should behave in production?
docker-compose.yml

179. Docker can manage the block device for you, simplifying configuration of direct-lvm mode, when using the devicemapper storage driver
true

180. On Linux systems the Docker daemon is configured using a JSON file, create a file at?
/etc/docker/daemon.json

181. When the container starts, it can be connected to multiple networks using '--network'
false.

182. Which instruction sets the working directory for any RUN, CMD, ENTRYPOINT, COPY and ADD instructions that follow it in the Dockerfile?
WORKDIR

183. Which type of mount has the side effect that you can change the host filesystem via processes running in a container, includ
bind mounts

184. Which 'docker create' option allows you to join a container to a network?
--network

185. Which command is used to enable swarm mode and make the machine, where the command is run, the current machine a swarm manager?
docker swarm init

186. Macvlan networks can be created in which of the following modes?
In bridge mode, macvlan traffic goes through a physical device on the host.
In 802.1q trunk bridge mode, traffic goes through an 802.1q sub-interface which Docker creates on the fly. This allows you to control routing and filtering at a more granular level.

187. By default, a container has no resource constraints and can use as much of a given resource as the host’s kernel scheduler allows.
true.

188. You can remove unused volumes using which command?
docker volume prune

189. In order to scale your app with docker stack deploy, it is required to tear down the existing stack first
false.

190. Which command is used to create a user-defined bridge network?
docker network create 'netwrok name'

191. Which two new networks are created on a Docker host when you initialize a swarm or join a Docker host to an existing swarm?
an overlay network called ingress, which handles control and data traffic related to swarm services. 
a bridge network called docker_gwbridge, which connects the individual Docker daemon to the other daemons participating in the swarm.

192. When Docker is running in swarm mode, you cant run standalone containers on any of the Docker hosts participating in the swarm.
false.

193. The major difference between a container and an image is?
the top writable layer. 

194. Docker CE is available on multiple platforms. Which of the following are not valid operating systems to run Docker CE Server?
windows server 2012.

195. Docker has an enterprise-grade image storage solution that you can install behind your firewall so that you can securely store and manage the Docker images you use in your applications. What is it called?
Docker trust registry

196. Which docker components can be described as "an executable package that includes everything needed to run an application--the code, a runtime, libraries, environment variables, and configuration files"?
image

197. How long after a container successfully starts does a docker restart policy start monitoring the container?
10 seconds

199. Restart policies only apply to containers. Restart policies for swarm services are configured differently. 

200. Which command builds a docker image from a dockerfile?
docker build

201. Which flags of 'docker swarm update' enable and/or disable autolock on an existing swarm?
--autolock=true
--autolock=false

202. To have high-availability on UCP and DTR, you need a minimum of?
3 dedicated nodes to install UCP with HA and 3 dedicated nodes to install DTR with HA

203. Which instruction adds metadata to an image?
label

204. In the output below, what do the IDs next to the "Pull complete" message indicate?
$ docker pull ubuntu:15.0415.04:

Pulling from library/ubuntu

1ba8ac955b97: Pull complete

f157c4e5ede7: Pull complete

0b7e98f84c4c: Pull complete

a3ed95caeb02: Pull complete
Digest: sha256:5e279a9df07990286cce22e1b0f5b0490629ca6d187698746ae5e28e604a640e

Status: Downloaded newer image for ubuntu:15.04

LAYERS OF THE IMAGES.

205. Which docker component can be described as "a runtime instance of an image"?
Container.

206. Which command is used to see which nodes are running the service?
docker service ps

207. Which command shows information logged by a running container?
docker logs

208. In which model, does the swarm manager distribute a specific number of replica tasks among the nodes based upon the scale you set in the desired state?
replicated.

209. Which actions allow you to scale an app? 
execute the docker stack deploy command
change the replica value in docker-compose.yml file

210. Which docker feature enables each node in the swarm to accept connections on published ports for any service running in the swarm, even if there’s no task running on the node? 
Hint: The features also routes all incoming requests to published ports on available nodes to an active container.

routing mesh.


211. Docker CE is available on multiple platforms. Which of the following are valid operating systems to run Docker CE Desktop?
windows 10 and Macos

212. The ucp-agent runs on both manager and worker nodes
True 

213. With which type of mount is the data stored in the host system’s memory only?
tempfs

214. Which of the following is not a supported logging driver?
azurelogs

215. Which two modes does Docker provide for delivering messages from the container to the log driver?
blocking and non blocking

216. Services are really just “containers in production.”
true.

217. Which docker run flag should you use to configure the restart policy for a container?
--restart

218. Which type of mount is a good choice when you do not want the data to persist either on the host machine or within the container?
tempfs

219. Which command will drain a node that had a task assigned to it?
docker node update --availability drain.

220.Each storage driver handles the implementation differently, but all drivers use stackable image layers and the copy-on-write (CoW) strategy.
True

221. You can configure DTR to use your own certificates, so that it is automatically trusted by your users’ browser and client tools
True

222. The EXPOSE instruction does not actually publish the port.
true

223. By default Docker Swarm uses which default address pool?
10.0.0.0/8

224. There are two ways to run the Engine in swarm mode:
Create a new swarm
Join an existing swarm.

225. After restoring a swarm backup you should run 'docker swarm init' with which flag to ensure that the node does not attempt to connect to nodes that were part of the old swarm?
--force-new-cluster

226. Which instruction is used to set environmental variables?
ENV 

227.In which model, does the swarm manager run one task for the service on every available node in the cluster?
global


228. Setting a node to DRAIN also removes standalone containers from that node, freeing it up completely.
false




















      






 












































 

















     



































































































   



	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 


























