ORCHESTRATION:
---------------------------------------------------------------------------------------------------------------------------------------------------------
1. Swarm services use a declarative model
2. To confirm that the service was created and started successfully, use the docker service ls
3. To provide a name for your service, use the --name flag: docker service create --name my_web nginx
4. gMSA credential spec - a requirement for Active Directory-authenticated applications. 
   This reduces the burden of distributing credential specs to the nodes they’re used on.
5. To use a Config as a credential spec, first create the Docker Config containing the credential spec: docker config create credspec credspec.json
6. you can create a service using this credential spec. To do so, use the --credential-spec flag with the config name, like this: docker service create --credential-spec="config://credspec" <your image>
7. If your image is available on a private registry which requires login, use the --with-registry-auth flag with docker service create
8. If your image is stored on registry.example.com, which is a private registry, use a command like the docker login registry.example.com
9. Credential spec files are applied at runtime.
10. no gMSA credentials are written to disk on worker nodes.
11. When deploying a service using a gMSA-based config, the credential spec is passed directly to the runtime of containers in that service.
12. To use a Config as a credential spec, create a Docker Config in a credential spec file named credpspec.json.
13. You can change almost everything about an existing service using the docker service update command. 
14. Since Nginx is a web service, it works much better if you publish port 80 to clients outside the swarm. You can specify this when you create the service, using the -p or --publish flag.
15. When updating an existing service, the flag is --publish-add. Assuming that the my_web service from the previous section still exists, use the following command to update it to publish port 80.
       docker service update --publish-add 80 my_web
16. here is also a --publish-rm flag to remove a port that was previously published.
17. To remove a service, use the docker service remove command.
18. If you specify a tag, the manager resolves that tag to a digest.
19. When the request to create a container task is received on a worker node, the worker node only sees the digest, not the tag.
20. When you create a service, the image’s tag is resolved to the specific digest the tag points to at the time of service creation.
21. If the manager can’t resolve the tag to a digest, each worker node is responsible for resolving the tag to a digest, and different nodes may use different versions of the image.
22. To see an image’s current digest, issue the command docker inspect <IMAGE>:<TAG> and look for the RepoDigests line.
23. If the swarm manager can resolve the image tag to a digest, it instructs the worker nodes to redeploy the tasks and use the image at that digest.
24. When you create a swarm service, you can publish that service’s ports to hosts outside the swarm in two ways: 
       i. You can rely on the routing mesh.  ii. You can publish a service task’s port directly on the swarm node .
25. Routing mesh: When you publish a service port, the swarm makes the service accessible at the target port on every node, regardless of whether there is a task for the service running on that node or not. This is less complex and is the right choice for many types of services.
26. PUBLISH A SERVICE’S PORTS USING THE ROUTING MESH: To publish a service’s ports externally to the swarm, use the --publish <PUBLISHED-PORT>:<SERVICE-PORT> flag
27. If an external host connects to that port on any swarm node, the routing mesh routes it to a task.
28. The external host does not need to know the IP addresses or internally-used ports of the service tasks to interact with the service. When a user or process connects to a service, any worker node running a service task may respond.
29. PUBLISH A SERVICE’S PORTS DIRECTLY ON THE SWARM NODE: You can publish a service task’s port directly on the swarm node where that service is running.
30. PUBLISH A SERVICE’S PORTS DIRECTLY ON THE SWARM NODE: provides the maximum flexibility, including the ability for you to develop your own routing framework.
31. To publish a service’s port directly on the node where it is running, use the mode=host option to the --publish flag.
32. create overlay network on a manager node using the docker network create command with the --driver overlay flag.
33. You can create a new service and pass the --network flag to attach the service to the overlay network: 
    docker service create \
  --replicas 3 \
  --network my-network \
  --name my-web \
    nginx
34. You can also connect an existing service to an overlay network using the --network-add flag. docker service update --network-add my-network my-web.
35. To disconnect a running service from a network, use the --network-rm flag.: docker service update --network-rm my-network my-web.
36. Placement constraints let you configure the service to run only on nodes with specific (arbitrary) metadata set, and cause the deployment to fail if appropriate nodes do not exist. 
37. you can specify that your service should only run on nodes where an arbitrary label pci_compliant is set to true.
38. Placement preferences let you apply an arbitrary label with a range of values to each node, and spread your service’s tasks across those nodes using an algorithm.
39. Swarm mode has two types of services: replicated and global.
40. For replicated services, you specify the number of replica tasks for the swarm manager to schedule onto available nodes.
41. For global services, the scheduler places one task on each available node that meets the service’s placement constraints and resource requirements.
42. I created a deployment that runs several identical tasks on nodes. which type of service deployment is this?: Replicated.
43. I created a deployment that runs exactly one task on every node. which type of service deployment is this? Global.
44. You control the type of service using the --mode flag. If you don’t specify a mode, the service defaults to replicated.
44. For replicated services, you specify the number of replica tasks you want to start using the --replicas flag. For example, to start a replicated nginx service with 3 replica tasks:
     docker service create \
  --name my_web \
  --replicas 3 \
  nginx
45. To start a global service on each available node, pass --mode global to docker service create.
46. The global service on the new node.
      docker service create \
  --name myservice \
  --mode global \
    alpine top
47. To reserve a given amount of memory or number of CPUs for a service, use the --reserve-memory or --reserve-cpu flags.
48. PLACEMENT CONSTRAINTS: service only runs on nodes with the label region set to east. If no appropriately-labelled nodes are available, tasks will wait in Pending  until they become available.
49. PLACEMENT CONSTRAINTS: The --constraint flag uses an equality operator (== or !=)
     docker service create \
  --name my-nginx \
  --replicas 5 \
  --constraint node.labels.region==east \
  nginx
50. PLACEMENT PREFERENCES: While placement constraints limit the nodes a service can run on, placement preferences try to place tasks on appropriate nodes in an algorithmic way (currently, only spread evenly). 
      docker service create \
  --replicas 9 \
  --name redis_2 \
  --placement-pref 'spread=node.labels.datacenter' \
  redis:3.0.6
  
51. When updating a service with docker service update, --placement-pref-add appends a new placement preference after all existing placement preferences.
52. --placement-pref-rm removes an existing placement preference that matches the argument.
53. The --update-delay flag configures the time delay between updates to a service task or sets of tasks.
54. By default the scheduler updates 1 task at a time. You can pass the --update-parallelism flag to configure the maximum number of service tasks that the scheduler updates simultaneously.
55. When an update to an individual task returns a state of RUNNING, the scheduler continues the update by continuing to another task until all tasks are updated. If, at any time during an update a task returns FAILED, the scheduler pauses the update.
    You can control the behavior using the --update-failure-action flag for docker service create or docker service update.
56. When an updated task returns either RUNNING or FAILED, the scheduler waits 10 seconds before stopping the next task to update:
      docker service create \
  --replicas 10 \
  --name my_web \
  --update-delay 10s \
  --update-parallelism 2 \
  --update-failure-action continue \
  alpine
57. The --update-max-failure-ratio flag controls what fraction of tasks can fail during an update before the update as a whole is considered to have failed. 
58. An individual task update is considered to have failed if the task doesn’t start up, or if it stops running within the monitoring period specified with the --update-monitor flag.
59. The default value for --update-monitor is 30 seconds.
60. In case the updated version of a service doesn’t function as expected, it’s possible to manually roll back to the previous version of the service using docker service update’s --rollback flag.
61. You can create two types of mounts for services in a swarm, volume mounts or bind mounts.
62. using the --mount flag when you create a service. the --mount-add or --mount-rm flag when updating an existing service. 
63. Data volumes are storage that exist independently of a container.
64. Volumes can be created before deploying a service, or if they don’t exist on a particular host when a task is scheduled there, they are created automatically according to the volume specification on the service. 
65. The default volume driver is local.
66.  To use a different volume driver with this create-on-demand pattern, specify the driver and its options with the --mount flag
67. Bind mounts are file system paths from the host where the scheduler deploys the container for the task. 
     docker service create \
  --mount type=bind,src=<HOST-PATH>,dst=<CONTAINER-PATH> \     /// READ-WRITE BIND.
  --name myservice \
  <IMAGE>
68. docker service create \
  --mount type=bind,src=<HOST-PATH>,dst=<CONTAINER-PATH>,readonly \   ///READ-ONLY
  --name myservice \
  <IMAGE>
69. Bind mounts can be useful but they can also cause problems. The main risks include the following:
      i. If you bind mount a host path into your service’s containers, the path must exist on every swarm node.The Docker swarm mode scheduler can schedule containers on any machine that meets resource availability requirements and satisfies all constraints and placement preferences you specify
      ii. The Docker swarm mode scheduler may reschedule your running service containers at any time if they become unhealthy or unreachable.
	  iii. Host bind mounts are non-portable. When you use bind mounts, there is no guarantee that your application runs the same way in development as it does in production.
70. The following flags are supported for template:
      --hostname
      --mount
      --env	  
71. docker node update [OPTIONS] NODE: 
     --availability ->Availability of the node (“active”|”pause”|”drain”)
     --label-add->Add or update a node label (key=value) 
     --label-rm->Remove a node label if exists	 
     --role->Role of the node (“worker”|”manager”)
72. Add metadata to a swarm node using node labels.  $ docker node update --label-add foo worker1
73. To add multiple labels to a node, pass the --label-add flag for each label: $ docker node update --label-add foo --label-add bar worker1
74. When you create a service, you can use node labels as a constraint. A constraint limits the nodes where the scheduler deploys tasks for a service.
         $ docker node update --label-add type=queue worker1    ///where type is a label and queue is service.
75. Open a terminal and ssh into the machine where you want to run your manager node. you can connect to it via SSH using the  command:
        $ docker-machine ssh manager1
76. Run the command to create a new swarm: $ docker swarm init --advertise-addr <MANAGER-IP>
77. The --advertise-addr flag configures the manager node to publish its address as 192.168.99.100. The other nodes in the swarm must be able to access the manager at the IP address.
78. Run docker info to view the current state of the swarm:
79. Run the docker node ls command to view information about nodes:
80. you can run the following command on a manager node to retrieve the join command for a worker: docker swarm join-token worker.
81. STACK must be executed on a swarm manager node.
82. The Compose file is a YAML file defining services, networks and volumes. The default path for a Compose file is ./docker-compose.yml.
83. restart: no is the default restart policy, and it does not restart a container under any circumstance. When always is specified, the container always restarts.
     The on-failure policy restarts a container if the exit code indicates an on-failure error and restart: unless-stopped
            restart: "no"
            restart: always
            restart: on-failure
            restart: unless-stopped
84. The restart option is ignored when deploying a stack in swarm mode.
85. The Raft logs used by swarm managers are encrypted on disk by default.
86. When Docker restarts, both the TLS key used to encrypt communication among swarm nodes, and the key used to encrypt and decrypt Raft logs on disk, are loaded into each manager node’s memory.
87. Ability to protect the mutual TLS encryption key and the key used to encrypt and decrypt Raft logs at rest, by allowing you to take ownership of these keys and to require manual unlocking of your managers. This feature is called autolock.
88. When Docker restarts, you must unlock the swarm first, using a key encryption key generated by Docker when the swarm was locked. You can rotate this key encryption key at any time.
89. You don’t need to unlock the swarm when a new node joins the swarm, because the key is propagated to it over mutual TLS.
90. When you initialize a new swarm, you can use the --autolock flag to enable autolocking of swarm manager nodes when Docker restarts.
                $ docker swarm init --autolock.
91. To enable autolock on an existing swarm, set the autolock flag to true: $ docker swarm update --autolock=true
92. To disable autolock, set --autolock to false. $ docker swarm update --autolock=false.
93. To unlock a locked swarm, use docker swarm unlock. : $ docker swarm unlock
94. If the key has not been rotated since the node left the swarm, and you have a quorum of functional manager nodes in the swarm, you can view the current unlock key using
         $ docker swarm unlock-key
95. docker swarm unlock-key --rotate
96. $ docker service ls

	ID            NAME        SCALE  IMAGE   COMMAND
	9uk4639qpg7n  helloworld  1/1    alpine  ping docker.com
97. A task is the atomic unit of scheduling within a swarm.
98. The container is the instantiation of the task.
99. API: accepts command and  create service object.
100. orchestrator: reconsillation loop that creates tasks for service object.
111. allocater:  allocates IP address to task.
112. dispatcher: assign task to nodes.
113. scheduler: instructs a worker to run a task. 
114. If your only intention is to prevent a service from being deployed, scale the service to 0 instead of trying to configure it in such a way that it remains in pending.
115. If all nodes are paused or drained, and you create a service, it is pending until a node becomes available.
116. The first node to become available gets all of the tasks, so this is not a good thing to do in a production environment.   
117. You can reserve a specific amount of memory for a service. If no node in the swarm has the required amount of memory, the service remains in a pending state until a node is available which can run its tasks.
118. For a replicated service, you specify the number of identical tasks you want to run. For example, you decide to deploy an HTTP service with three replicas, each serving the same content.
119. A global service is a service that runs one task on every node. There is no pre-specified number of tasks.
120: Running Services on a Docker Swarm can be scaled in different ways: docker service scale SERVICE=NUMBER. This works only for replicated services. 
       Another way is using docker service update --replicas=NUMBER SERVICE to achieve the same goal.
121. multiple services can be scaled at once, using the docker service scale SERVICE1=NUMBER1 SERVICE2=NUMBER2
122. docker inspect: Return low-level information on Docker objects. Docker inspect provides detailed information on constructs controlled by Docker.
123. By default, docker inspect will render results in a JSON array.
124. Run docker service inspect --pretty <SERVICE-ID> to display the details about a service in an easily readable format.
125. Run docker service ps <SERVICE-ID> to see which nodes are running the service.
126. By default, manager nodes in a swarm can execute tasks just like worker nodes.
127. docker stack ls shows an overview of all existing stacks.
128. docker stack ps STACK lists all tasks in a stack.
129. docker stack services STACK gives an overview over the running services.
130. docker stack rm STACK removes one or more stack.
131. Swarm manager nodes use the Raft Consensus Algorithm to manage the swarm state.
132. manager nodes are the key components for managing the swarm and storing the swarm state.
133. More Manager nodes reduce write performance because more nodes must acknowledge proposals to update the swarm state. This means more network round-trip traffic.
134. Raft requires a majority of managers, also called the quorum.
135. If the swarm loses the quorum of managers, the swarm cannot perform management tasks.
136. Even if a swarm loses the quorum of managers, swarm tasks on existing worker nodes continue to run. However, swarm nodes cannot be added, updated, or removed, and new or existing tasks cannot be started, stopped, moved, or updated.
137. Scaling down to a single manager is an unsafe operation and is not recommended.
138. If the last node leaves the swarm unexpectedly during the demote operation, the swarm becomes unavailable until you reboot the node or restart with --force-new-cluster.
139. By default manager nodes also act as a worker nodes. This means the scheduler can assign tasks to a manager node.
140. To avoid interference with manager node operation, you can drain manager nodes to make them unavailable as worker nodes:
           docker node update --availability drain <NODE>
141. When you drain a node, the scheduler reassigns any tasks running on the node to other available worker nodes in the swarm. It also prevents the scheduler from assigning tasks to the node.
142. You can monitor the health of manager nodes by querying the docker nodes API in JSON format through the /nodes HTTP endpoint
      docker node inspect manager1 --format "{{ .ManagerStatus.Reachability }}"
      reachable
       
      docker node inspect manager1 --format "{{ .Status.State }}"
      ready

142. An unreachable health status means that this particular manager node is unreachable from other manager nodes. In this case you need to take action to restore the unreachable manager:

     Restart the daemon and see if the manager comes back as reachable.     
     Reboot the machine.
     If neither restarting or rebooting work, you should add another manager node or promote a worker to be a manager node. You also need to cleanly remove the failed node entry from the manager set with docker node demote <NODE> and docker node rm <id-node>.
143. You should never restart a manager node by copying the raft directory from another node. The data directory is unique to a node ID. 
     A node can only use a node ID once to join the swarm. The node ID space should be globally unique.
145. To cleanly re-join a manager node to a cluster:

	To demote the node to a worker, run docker node demote <NODE>.
	To remove the node from the swarm, run docker node rm <NODE>.
	Re-join the node to the swarm with a fresh state using docker swarm join.
146. remove a node: docker node rm
147. you can forcefully remove the node without shutting it down by passing the --force flag. docker node rm --force node9
148. Before you forcefully remove a manager node, you must first demote it to the worker role. Make sure that you always have an odd number of manager nodes if you demote or remove a manager.
149. Docker manager nodes store the swarm state and manager logs in the /var/lib/docker/swarm/ directory.
150. You can back up the swarm using any manager. Use the following procedure. 
	1. If the swarm has auto-lock enabled, you need the unlock key to restore the swarm from backup. 
        Retrieve the unlock key if necessary and store it in a safe location. If you are unsure, read Lock your swarm to protect its encryption key.
        2. Stop Docker on the manager before backing up the data, so that no data is being changed during the backup. 
        It is possible to take a backup while the manager is running (a “hot” backup), but this is not recommended and your results are less predictable when restoring. While the manager is down, other nodes continue generating swarm data that is not part of this backup.
	3. Back up the entire /var/lib/docker/swarm directory.
        4. Restart the manager.
151. Recover from disaster: Restore from a backup.
        1. Shut down Docker on the target host machine for the restored swarm.
        2. Remove the contents of the /var/lib/docker/swarm directory on the new swarm.
        3. Restore the /var/lib/docker/swarm directory with the contents of the backup.
	4. Start Docker on the new node. Unlock the swarm if necessary. Re-initialize the swarm using the following command, 	
           so that this node does not attempt to connect to nodes that were part of the old swarm, and presumably no longer exist. $ docker swarm init --force-new-cluster
	5. Verify that the state of the swarm is as expected. This may include application-specific tests or simply checking the output of docker service ls to be sure that all expected services are present.
	6. If you use auto-lock, rotate the unlock key.
	7. Add manager and worker nodes to bring your new swarm up to operating capacity.
	8. Reinstate your previous backup regimen on the new swarm.
152. swarm cannot automatically recover if it loses a quorum.
153. The swarm can tolerate up to (N-1)/2 permanent failures beyond which requests involving swarm management cannot be processed.
154. The best way to recover from losing the quorum is to bring the failed nodes back online. If you can’t do that, the only way to recover from this state is to use the --force-new-cluster action from a manager node.
155.  you can use the --force or -f flag with the docker service update command to force the service to redistribute its tasks across the available worker nodes. 
156. When the container starts, it can only be connected to a single network, using --network
157. you can connect a running container to multiple networks using docker network connect.
158. container’s hostname defaults to be the container’s ID in Docker.
159. can override the hostname using --hostname.
-------------------------------------------------------------------------------------------------------------------------------------------------------------
IMAGE MANAGEMENT AND REGISTRY.

1. Build an image from a Dockerfile
2. The results of the health checks are available at the /debug/health endpoint on the debug HTTP server if the debug HTTP server is enabled.
3. You can delete an image from the Docker Hub by deleting individual tags in your repository in the tags area.
4. An image may be deleted from the registry via its name and reference. A delete may be issued with the following request format: DELETE /v2/<name>/manifests/<reference>
5. Tag Image: docker image tag SOURCE_IMAGE[:TAG] TARGET_IMAGE[:TAG]
6. docker image build: Build an image from a Dockerfile, docker image import: Import the contents from a tarball to create a filesystem image. docker image load: Load an image from a tar archive or STDIN. docker image save: Save one or more images to a tar archive (streamed to STDOUT by default).
7. The Registry is a stateless, highly scalable server side application that stores and lets you distribute Docker images. The Registry is open-source, under the permissive Apache license.
8. Start your registry: docker run -d -p 5000:5000 --name registry registry:2
9. Tag the image so that it points to your registry: docker image tag ubuntu localhost:5000/myfirstimage
10. Push it: Docker push localhost:5000/myfirstimage
11. Pull it back: docker pull localhost:5000/myfirstimage
12. Now stop your registry and remove all data: docker container stop registry && docker container rm -v registry
13. 

    



















   



	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 
	 


























